@article{barzilai1988Twopoint,
  title    = {Two-Point Step Size Gradient Methods},
  author   = {Barzilai, Jonathan and Borwein, Jonathan M.},
  year     = {1988},
  journal  = {IMA Journal of Numerical Analysis},
  volume   = {8},
  number   = {1},
  doi      = {10.1093/imanum/8.1.141},
  abstract = {We derive two-point step sizes for the steepest-descent method by approximating the secant equation. At the cost of storage of an extra iterate and gradient, these algorithms achieve better performance and cheaper computation than the classical steepest-descent method. We indicate a convergence analysis of the method in the two-dimensional quadratic case. The behaviour is highly remarkable and the analysis entirely nonstandard. © 1988 Oxford University Press.},
  file     = {/Users/vivak/Zotero/storage/D8YZUZH6/Barzilai and Borwein - 1988 - Two-Point Step Size Gradient Methods.pdf}
}

@book{bertsekas2016Nonlinear,
  title     = {Nonlinear {{Optimization}}},
  author    = {Bertsekas, Dimitri},
  year      = {2016},
  publisher = {Athena Scientific},
  isbn      = {978-1-886529-05-2}
}

@article{lanteri2023Designing,
  title    = {Designing to Detect Heteroscedasticity in a Regression Model},
  author   = {Lanteri, Alessandro and Leorato, Samantha and Lopez-Fidalgo, Jesus and Tommasi, Chiara},
  year     = {2023-02-24},
  journal  = {Journal of the Royal Statistical Society},
  series   = {B},
  volume   = {85},
  number   = {2},
  pages    = {315--326},
  doi      = {10.1093/jrsssb/qkad004},
  abstract = {We consider the problem of designing experiments to detect the presence of a specified heteroscedastity in Gaussian regression models. We study the relationship of the Ds- and KL-criteria with the noncentrality parameter of the asymptotic chi-squared distribution of a likelihood-based test, for local alternatives. We found that, when the heteroscedastity depends on one parameter, the two criteria coincide asymptotically and that the D1-criterion is proportional to the noncentrality parameter. Differently, when it depends on several parameters, the KL-optimum design converges to the design that maximizes the noncentrality parameter. Our theoretical findings are confirmed through a simulation study.},
  file     = {/Users/vivak/Zotero/storage/U96ZUTBJ/Lanteri et al. - 2023 - Designing to detect heteroscedasticity in a regression model.pdf}
}

@article{li2023Convex,
  title    = {Convex and {{Non-convex Optimization Under Generalized Smoothness}}},
  author   = {Li, Haochuan and Qian, Jian and Tian, Yi and Rakhlin, Alexander and Jadbabaie, Ali},
  year     = {2023-06},
  journal  = {Advances in Neural Information Processing Systems},
  volume   = {36},
  doi      = {10.48550/arxiv.2306.01264},
  abstract = {Classical analysis of convex and non-convex optimization methods often requires the Lipshitzness of the gradient, which limits the analysis to functions bounded by quadratics. Recent work relaxed this requirement to a non-uniform smoothness condition with the Hessian norm bounded by an affine function of the gradient norm, and proved convergence in the non-convex setting via gradient clipping, assuming bounded noise. In this paper, we further generalize this non-uniform smoothness condition and develop a simple, yet powerful analysis technique that bounds the gradients along the trajectory, thereby leading to stronger results for both convex and non-convex optimization problems. In particular, we obtain the classical convergence rates for (stochastic) gradient descent and Nesterov's accelerated gradient method in the convex and/or non-convex setting under this general smoothness condition. The new analysis approach does not require gradient clipping and allows heavy-tailed noise with bounded variance in the stochastic setting.}
}

@inproceedings{malitsky2020Adaptive,
  title      = {Adaptive {{Gradient Descent}} without {{Descent}}},
  booktitle  = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author     = {Malitsky, Yura and Mishchenko, Konstantin},
  year       = {2020-11-21},
  pages      = {6702--6712},
  publisher  = {PMLR},
  issn       = {2640-3498},
  doi        = {10.48550/arxiv.1910.09529},
  urlyear    = {2024-11-09},
  abstract   = {We present a strikingly simple proof that two rules are sufficient to automate gradient descent: 1) don’t increase the stepsize too fast and 2) don’t overstep the local curvature. No need for functional values, no line search, no information about the function except for the gradients. By following these rules, you get a method adaptive to the local geometry, with convergence guarantees depending only on the smoothness in a neighborhood of a solution. Given that the problem is convex, our method converges even if the global smoothness constant is infinity. As an illustration, it can minimize arbitrary continuously twice-differentiable convex function. We examine its performance on a range of convex and nonconvex problems, including logistic regression and matrix factorization.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid     = {english},
  keywords   = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file       = {/Users/vivak/Zotero/storage/F4V9JIVL/Malitsky and Mishchenko - 2020 - Adaptive Gradient Descent without Descent.pdf;/Users/vivak/Zotero/storage/RSSLLAIN/Malitsky and Mishchenko - 2020 - Adaptive Gradient Descent without Descent.pdf;/Users/vivak/Zotero/storage/V5PV87L6/Malitsky and Mishchenko - 2020 - Adaptive Gradient Descent without Descent.pdf;/Users/vivak/Zotero/storage/CIKUXJD6/1910.html}
}

@book{mccullagh1998Generalized,
  title     = {Generalized {{Linear Models}}},
  author    = {McCullagh, Peter and Nelder, John},
  year      = {1998},
  edition   = {Second},
  publisher = {{Chapman and Hall}}
}

@article{nesterov1983Method,
  title    = {A Method for Solving the Convex Programming Problem with Convergence Rate {{O}}(1/K2)},
  author   = {Nesterov, Yurii},
  year     = {1983},
  journal  = {Proceedings of the USSR Academy of Sciences},
  volume   = {269},
  pages    = {543--547},
  keywords = {⛔ No DOI found},
  file     = {/Users/vivak/Zotero/storage/L8ITZFNL/dan46009.pdf;/Users/vivak/Zotero/storage/Y62WCE5F/nesterov83.pdf;/Users/vivak/Zotero/storage/H3CGHXIX/1370862715914709505.html}
}

@article{patel2024Gradient,
  title   = {Gradient {{Descent}} in the {{Absence}} of {{Global Lipschitz Continuity}} of the {{Gradients}}},
  author  = {Patel, Vivak and Berahas, Albert},
  year    = {2024},
  journal = {SIAM},
  volume  = {6},
  number  = {3},
  pages   = {579--846},
  doi     = {10.1137/22M1527210}
}

@article{wedderburn1974Quasilikelihood,
  title     = {Quasi-Likelihood Functions, Generalized Linear Models, and the {{Gauss}}—{{Newton}} Method},
  author    = {Wedderburn, R. W. M.},
  year      = {1974},
  journal   = {Biometrika},
  volume    = {61},
  number    = {3},
  pages     = {439--447},
  publisher = {Oxford University Press},
  doi       = {10.1093/biomet/61.3.439}
}

@article{wu2020WNGrad,
  title   = {{{WNGrad}}: {{Learn}} the {{Learning Rate}} in {{Gradient Descent}}},
  author  = {Wu, Xiaoxia and Ward, Rachel and Bottou, Leon},
  year    = {2020-11-19},
  journal = {arxiv},
  doi     = {10.48550/arXiv.1803.02865}
}
