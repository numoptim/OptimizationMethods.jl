@article{barzilai1988Twopoint,
  title    = {Two-Point Step Size Gradient Methods},
  author   = {Barzilai, Jonathan and Borwein, Jonathan M.},
  year     = {1988},
  journal  = {IMA Journal of Numerical Analysis},
  volume   = {8},
  number   = {1},
  doi      = {10.1093/imanum/8.1.141},
}

@book{bertsekas2016Nonlinear,
  title     = {Nonlinear {{Optimization}}},
  author    = {Bertsekas, Dimitri},
  year      = {2016},
  publisher = {Athena Scientific},
  isbn      = {978-1-886529-05-2}
}

@article{lanteri2023Designing,
  title    = {Designing to Detect Heteroscedasticity in a Regression Model},
  author   = {Lanteri, Alessandro and Leorato, Samantha and Lopez-Fidalgo, Jesus and Tommasi, Chiara},
  year     = {2023-02-24},
  journal  = {Journal of the Royal Statistical Society},
  series   = {B},
  volume   = {85},
  number   = {2},
  pages    = {315--326},
  doi      = {10.1093/jrsssb/qkad004},
}

@article{li2023Convex,
  title    = {Convex and {{Non-convex Optimization Under Generalized Smoothness}}},
  author   = {Li, Haochuan and Qian, Jian and Tian, Yi and Rakhlin, Alexander and Jadbabaie, Ali},
  year     = {2023-06},
  journal  = {Advances in Neural Information Processing Systems},
  volume   = {36},
  doi      = {10.48550/arxiv.2306.01264},
}

@inproceedings{malitsky2020Adaptive,
  title      = {Adaptive {{Gradient Descent}} without {{Descent}}},
  booktitle  = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author     = {Malitsky, Yura and Mishchenko, Konstantin},
  year       = {2020-11-21},
  pages      = {6702--6712},
  publisher  = {PMLR},
  issn       = {2640-3498},
  doi        = {10.48550/arxiv.1910.09529},
  urlyear    = {2024-11-09},
  abstract   = {We present a strikingly simple proof that two rules are sufficient to automate gradient descent: 1) don’t increase the stepsize too fast and 2) don’t overstep the local curvature. No need for functional values, no line search, no information about the function except for the gradients. By following these rules, you get a method adaptive to the local geometry, with convergence guarantees depending only on the smoothness in a neighborhood of a solution. Given that the problem is convex, our method converges even if the global smoothness constant is infinity. As an illustration, it can minimize arbitrary continuously twice-differentiable convex function. We examine its performance on a range of convex and nonconvex problems, including logistic regression and matrix factorization.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid     = {english},
}

@book{mccullagh1998Generalized,
  title     = {Generalized {{Linear Models}}},
  author    = {McCullagh, Peter and Nelder, John},
  year      = {1998},
  edition   = {Second},
  publisher = {{Chapman and Hall}}
}

@article{nesterov1983Method,
  title    = {A Method for Solving the Convex Programming Problem with Convergence Rate {{O}}(1/K2)},
  author   = {Nesterov, Yurii},
  year     = {1983},
  journal  = {Proceedings of the USSR Academy of Sciences},
  volume   = {269},
  pages    = {543--547},
}

@article{patel2024Gradient,
  title   = {Gradient {{Descent}} in the {{Absence}} of {{Global Lipschitz Continuity}} of the {{Gradients}}},
  author  = {Patel, Vivak and Berahas, Albert},
  year    = {2024},
  journal = {SIAM},
  volume  = {6},
  number  = {3},
  pages   = {579--846},
  doi     = {10.1137/22M1527210}
}

@article{wedderburn1974Quasilikelihood,
  title     = {Quasi-Likelihood Functions, Generalized Linear Models, and the {{Gauss}}—{{Newton}} Method},
  author    = {Wedderburn, R. W. M.},
  year      = {1974},
  journal   = {Biometrika},
  volume    = {61},
  number    = {3},
  pages     = {439--447},
  publisher = {Oxford University Press},
  doi       = {10.1093/biomet/61.3.439}
}

@article{wu2020WNGrad,
  title   = {{{WNGrad}}: {{Learn}} the {{Learning Rate}} in {{Gradient Descent}}},
  author  = {Wu, Xiaoxia and Ward, Rachel and Bottou, Leon},
  year    = {2020-11-19},
  journal = {arxiv},
  doi     = {10.48550/arXiv.1803.02865}
}

@article{gratton2022First-order,
  title = {First-{Order} {Objective}-{Function}-{Free} {Optimization} {Algorithms} and {Their} {Complexity}},
  url = {https://arxiv.org/abs/2203.01757v2},
  doi = {10.48550/arXiv.2203.01757},
  journal = {arxiv},
  author = {Gratton, Serge and Jerad, Sadok and Toint, Philippe},
  month = mar,
  year = {2022},
}
