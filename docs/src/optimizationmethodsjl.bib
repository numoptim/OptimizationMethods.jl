@article{barzilai1988Twopoint,
  title = {Two-Point Step Size Gradient Methods},
  author = {Barzilai, Jonathan and Borwein, Jonathan M.},
  year = {1988},
  journal = {IMA Journal of Numerical Analysis},
  volume = {8},
  number = {1},
  doi = {10.1093/imanum/8.1.141},
  abstract = {We derive two-point step sizes for the steepest-descent method by approximating the secant equation. At the cost of storage of an extra iterate and gradient, these algorithms achieve better performance and cheaper computation than the classical steepest-descent method. We indicate a convergence analysis of the method in the two-dimensional quadratic case. The behaviour is highly remarkable and the analysis entirely nonstandard. © 1988 Oxford University Press.},
  file = {/Users/vivak/Zotero/storage/D8YZUZH6/Barzilai and Borwein - 1988 - Two-Point Step Size Gradient Methods.pdf}
}

@inproceedings{malitsky2020Adaptive,
  title = {Adaptive {{Gradient Descent}} without {{Descent}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Malitsky, Yura and Mishchenko, Konstantin},
  year = {2020-11-21},
  pages = {6702--6712},
  publisher = {PMLR},
  issn = {2640-3498},
  doi = {10.48550/arxiv.1910.09529},
  abstract = {We present a strikingly simple proof that two rules are sufficient to automate gradient descent: 1) don’t increase the stepsize too fast and 2) don’t overstep the local curvature. No need for functional values, no line search, no information about the function except for the gradients. By following these rules, you get a method adaptive to the local geometry, with convergence guarantees depending only on the smoothness in a neighborhood of a solution. Given that the problem is convex, our method converges even if the global smoothness constant is infinity. As an illustration, it can minimize arbitrary continuously twice-differentiable convex function. We examine its performance on a range of convex and nonconvex problems, including logistic regression and matrix factorization.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/vivak/Zotero/storage/F4V9JIVL/Malitsky and Mishchenko - 2020 - Adaptive Gradient Descent without Descent.pdf;/Users/vivak/Zotero/storage/RSSLLAIN/Malitsky and Mishchenko - 2020 - Adaptive Gradient Descent without Descent.pdf;/Users/vivak/Zotero/storage/V5PV87L6/Malitsky and Mishchenko - 2020 - Adaptive Gradient Descent without Descent.pdf;/Users/vivak/Zotero/storage/CIKUXJD6/1910.html}
}

@article{wedderburn1974Quasilikelihood,
  title = {Quasi-Likelihood Functions, Generalized Linear Models, and the {{Gauss}}—{{Newton}} Method},
  author = {Wedderburn, R. W. M.},
  year = {1974},
  journal = {Biometrika},
  volume = {61},
  number = {3},
  pages = {439--447},
  publisher = {Oxford University Press},
  doi = {10.1093/biomet/61.3.439}
}

@article{lanteri2023designing,
  series = {B},
  title = {Designing to detect heteroscedasticity in a regression model},
  volume = {85},
  doi = {10.1093/jrsssb/qkad004},
  abstract = {We consider the problem of designing experiments to detect the presence of a specified heteroscedastity in Gaussian regression models. We study the relationship of the Ds- and KL-criteria with the noncentrality parameter of the asymptotic chi-squared distribution of a likelihood-based test, for local alternatives. We found that, when the heteroscedastity depends on one parameter, the two criteria coincide asymptotically and that the D1-criterion is proportional to the noncentrality parameter. Differently, when it depends on several parameters, the KL-optimum design converges to the design that maximizes the noncentrality parameter. Our theoretical findings are confirmed through a simulation study.},
  number = {2},
  journal = {Journal of the Royal Statistical Society},
  author = {Lanteri, Alessandro and Leorato, Samantha and Lopez-Fidalgo, Jesus and Tommasi, Chiara},
  month = {02},
  year = {2023},
  pages = {315--326},
}


@book{mccullagh1998generalized,
  edition = {Second},
  title = {Generalized {Linear} {Models}},
  url = {https://archive.org/details/generalizedlinea0000mccu},
  publisher = {Chapman and Hall},
  author = {McCullagh, Peter and Nelder, John},
  year = {1998},
}
