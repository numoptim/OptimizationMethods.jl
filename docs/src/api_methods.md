# Contents

```@contents
Pages=["api_methods.md"]
```

# Barzilai Borwein Method

```@docs
barzilai_borwein_gd
BarzilaiBorweinGD
```

# Gradient Descent with Fixed Step Size

```@docs
fixed_step_gd
FixedStepGD
```

# Lipschitz Approximation (Malitsky & Mishchenko)

```@docs
lipschitz_approximation_gd
LipschitzApproxGD
```

# Weighted Norm Damping Gradient Method (WNGrad)

```@docs
weighted_norm_damping_gd
WeightedNormDampingGD
```

# Nesterov's Accelerated Gradient Descent

```@docs
nesterov_accelerated_gd

NesterovAcceleratedGD
```

# Gradient Descent with Diminishing Step Size
```@docs
DiminishingStepGD

diminishing_step_gd
```

Below is a list of step size functions that are in the library.
The step size sequence generated by these functions, ``\lbrace \alpha_k \rbrace`` 
satisfies ``\alpha_k > 0``, ``\lim_{k \to\infty} \alpha_k = 0`` and 
``\sum_{k=0}^\infty \alpha_k = \infty``.


```@docs
OptimizationMethods.inverse_k_step_size

OptimizationMethods.inverse_log2k_step_size

OptimizationMethods.stepdown_100_step_size
```

# Gradient Descent with Backtracking
```@docs
backtracking_gd

BacktrackingGD
```

# Gradient Descent with Non-monotone Line Search

## Fixed Step
```@docs
fixed_step_nls_maxval_gd

FixedStepNLSMaxValGD
```

# Gradient Descent with Non-sequential Armijo Line Search
```@docs
NonsequentialArmijoGD

nonsequential_armijo_gd
```

The method above requires several utility functions. These are listed
below.

```@docs
OptimizationMethods.update_local_lipschitz_approximation

OptimizationMethods.compute_step_size

OptimizationMethods.inner_loop!

OptimizationMethods.update_algorithm_parameters!
```

# Safeguarded and Globalized Barzilai-Borwein Method

```@docs
safe_barzilai_borwein_nls_maxval_gd

SafeBarzilaiBorweinNLSMaxValGD
```

# Line search Helper Functions

## Backtracking
```@docs
OptimizationMethods.backtracking!
```

## Non-sequential Armijo
```@docs
OptimizationMethods.non_sequential_armijo_condition
```

# Index 
```@index
```
