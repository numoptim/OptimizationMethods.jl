# Date: 12/19/2024
# Author: Christian Varner
# Purpose: Implement a quasi-likelihood examples
# with a linear mean and variance function that is 1 + mean + sin(2pi*mean)

"""
    QLLogisticSin{T, S} <: AbstractNLPModel{T, S}

Implements a Quasi-likelihood objective with a logistic link function and
    `linear_plus_sin` variance funtion. If the design matrix and the
    responses are not supplied, they are randomly generated.

# Objective Function

Let ``A`` be the design matrix, and ``b`` be the responses. Each row of ``A``
    and corresponding entry in ``b`` are the predictor and observations from one
    unit. The statistical model for this objective function assums ``b``
    are between ``0`` and ``1``.

Let ``A_i`` be row ``i`` of ``A`` and ``b_i`` entry ``i`` of ``b``. Let
```math
    \\mu_i(x) = \\mathrm{logistic}(A_i^\\intercal x)
```
and
```math
    v_i(\\mu) = 1 + \\mu(x) + \\sin(2 * \\pi * \\mu(x)). 
```

Let ``n`` be the number of rows in ``A``, then the quasi-likelihood objective is
```math
    F(x) = -\\sum_{i=1}^n \\int_0^{\\mu_i(x)} (b_i - \\mu)/v_i(\\mu) d\\mu.
```

!!! note
    ``F(x)`` does not have an easily expressible closed form, so a numerical
    integration scheme is used to evaluate the objective. The gradient
    and hessian have closed form solutions however.

# Fields

- `meta::NLPModelMeta{T, S}`, NLPModel struct for storing meta information for 
    the problem
- `counters::Counters`, NLPModel Counter struct that provides evaluations 
    tracking.
- `design::Matrix{T}`, covariate matrix for the problem/experiment (``A``).
- `response::Vector{T}`, observations for the problem/experiment (``b``).
- `mean::Function`, function of the linear predictor ``A * x`` to predict 
    the response.
- `mean_first_derivative::Function`, function for the first derivative of the 
    mean function.
- `mean_second_derivative::Function`, function for the second derivative of the
    mean funciton.
- `variance::Function`, variance function that estimates the variance of each
    response.
- `variance_first_derivative::Function`, function that returns the first 
    derivative of the variance function.
- `residual::Function`, computes the weighted residual of the model. 

# Constructors

## Inner Constructors

    QLLogisticSin{T, S}(meta::NLPModelMeta{T, S}, counters::Counters,
        design::Matrix{T}, response::Vector{T})
    
Constructs the `struct`. We purposely do not allow specification of the mean
    and variance function so that the struct is initialized with
    the correct mean and variance function.

## Outer Constructors

    function QLLogisticSin(::Type{T}; nobs::Int64 = 1000,
        nvar::Int64 = 50) where {T}

Construct the `struct` using simulated data. The `design` matrix has 
a column of all ones and the rest generated from a normal distribution with
its entries divided by `sqrt(nvar - 1)`. The responses are generated by 
`mean.(design * x) + variance.(mean.(design * x)) * ϵ`, where `ϵ` is a noise
vector generated from the Arcsine distribution with default parameters.
 
    function QLLogisticSin(design::Matrix{T}, response::Vector{T}, 
        x0::Vector{T} = zeros(T, size(design)[2])) where {T}

Constructs the `struct` using the data the is provided by the user.
"""
mutable struct QLLogisticSin{T, S} <: AbstractNLPModel{T, S}
    meta::NLPModelMeta{T, S}
    counters::Counters
    design::Matrix{T}
    response::Vector{T}
    mean::Function
    mean_first_derivative::Function
    mean_second_derivative::Function
    variance::Function
    variance_first_derivative::Function
    weighted_residual::Function

    QLLogisticSin{T, S}(meta::NLPModelMeta{T, S}, counters::Counters, 
        design::Matrix{T}, response::Vector{T}) where {T, S} = 
    begin
        residual(μ, y) = (y - μ)/OptimizationMethods.linear_plus_sin(μ) 
        new(meta, counters, design, response, 
            OptimizationMethods.logistic,           # force the correct mean function
            OptimizationMethods.dlogistic,          # force the correct derivative function
            OptimizationMethods.ddlogistic,         # force the correct derivative function
            OptimizationMethods.linear_plus_sin,    # force the correct variance function
            OptimizationMethods.dlinear_plus_sin,   # force the correct derivative function
            residual                                # residual function
        )
    end
end
function QLLogisticSin(
    ::Type{T};
    nobs::Int64 = 1000,
    nvar::Int64 = 50
) where {T}

    @assert nobs > 0 "Number of observations is $(nobs) which is not"*
    "greater than 0."
    
    @assert nvar > 0 "Number of variables is $(nvar) which is not"*
    "greater than 0."

    # initialize the meta data and counters
    meta = NLPModelMeta(
        nvar,
        name = "Quasi-likelihood with logistic link function and sine variance",
        x0 = zeros(T, nvar)
    )
    counters = Counters()

    # simulate the design matrix
    design = hcat(ones(T, nobs), randn(T, nobs, nvar-1) ./ T(sqrt(nvar - 1)))

    # get reponses
    β_true = randn(T, nvar)
    η = design * β_true
    μ_obs = OptimizationMethods.logistic.(η)
    ϵ = T.((rand(Distributions.Arcsine()) .- .5)./(1/8))

    # generate responses
    response = μ_obs + OptimizationMethods.linear_plus_sin.(μ_obs) * ϵ

    return QLLogisticSin{T, Vector{T}}(
        meta,
        counters,
        design,
        response
    )
end
function QLLogisticSin(
    design::Matrix{T},
    response::Vector{T},
    x0::Vector{T} = zeros(T, size(design)[2])
) where {T}

    @assert size(design, 1) == size(response, 1) "Number rows in design matrix is not"*
    "equal to the number of observations."

    @assert size(design, 2) == size(x0, 1) "Number of columns in design matrix is not"*
    "equal to the number of elements in x0."
    
    # initialize meta
    meta = NLPModelMeta(
            size(design, 2),
            name = "Quasi-likelihood with logistic link function and sine variance",
            x0 = x0
           )

    # initialize counters
    counters = Counters()

    # return the struct
    return QLLogisticSin{T, Vector{T}}(
        meta,
        counters,
        design,
        response,
    )
end

# precompute struct
"""
    PrecomputeQLLogisticSin{T} <: AbstractPrecompute{T}

Structure that holds precomputed values for the quasi-likelihood problem.
    These values are precomputed to save on time, and they remain unchanged
    throughout the algorithm.

# Fields

- `obs_obs_t`, 3d array where `obs_obs_t[i, :, :]` contains the outer produce
    between the ith covariate vector and itself.

# Constructor

    PrecomputeQLLogisticSin(progData::QLLogisticSin{T, S}) where {T, S}

Initializes the field values for the precompute data structure and returns 
    a `struct`.
"""
struct PrecomputeQLLogisticSin{T} <: AbstractPrecompute{T}
    obs_obs_t::Array{T, 3}
end
function PrecomputeQLLogisticSin(progData::QLLogisticSin{T, S}) where {T, S}

    # get the size of the matrix
    nobs, nvar = size(progData.design)

    # create the space
    obs_obs_t = zeros(T, nobs, nvar, nvar)
    
    for i in 1:nobs
        obs_obs_t[i, :, :] .= view(progData.design, i, :) *
            view(progData, i, :)'
    end

    return PrecomputeQLLogisticSin{T}(obs_obs_t)
end

# allocate struct
"""
    AllocateQLLogisticSin{T} <: AbstractProblemAllocate{T}

Mutable struct that contains buffer arrays for various computations used for
    this objective function and for optimization algorithms.

# Fields

- `linear_effect::Vector{T}`, buffer array for `progData.response * x`.   
- `μ::Vector{T}`, buffer array for response prediction.
- `∇μ_η::Vector{T}`, buffer array for first derivatives for the mean function
    evaluated at each point in `linear_effect`.
- `∇∇μ_η::Vector{T}`, buffer arry for second derivatives for the mean function
    evaluated at each point in `linear_effect`.
- `variance::Vector{T}`, buffer array for the variance function evaluated at
    each point `μ`
- `∇variance::Vector{T}`, buffer array for the first derivatives for the 
    variance function evaluated at each point in `μ`
- `weighted_residual::Vector{T}`, buffer array for the weighted residuals.
- `grad::Vector{T}`, buffer array for the gradient vector.
- `hess::Matrix{T}`, buffer matrix for the hessian.

# Constructors

    AllocateQLLogisticSin(progData::QLLogisticSin{T,S}) where {T,S}

Allocates memory for each of the field values and returns the struct.
"""
struct AllocateQLLogisticSin{T} <: AbstractProblemAllocate{T}
    linear_effect::Vector{T}   
    μ::Vector{T}
    ∇μ_η::Vector{T}
    ∇∇μ_η::Vector{T}
    variance::Vector{T}
    ∇variance::Vector{T}
    weighted_residual::Vector{T}
    grad::Vector{T}
    hess::Matrix{T}
end
function AllocateQLLogisticSin(progData::QLLogisticSin{T, S}) where {T, S}

    # get dimensions
    nobs = size(progData.design, 1)
    nvar = size(progData.design, 2)

    # initialize memory
    linear_effect = zeros(T, nobs)
    μ = zeros(T, nobs)
    ∇μ_∇η = zeros(T, nobs)
    variance = zeros(T, nobs)
    residual = zeros(T, nobs)
    grad = zeros(T, nvar)
    hess = zeros(T, nvar, nvar)

    return AllocateQLLogisticSin(
        linear_effect,
        μ, ∇μ_∇η,
        variance, 
        residual,
        grad, 
        hess
    )
end

"""
    initialize(progData::QLLogisticSin{T,S}) where {T,S}

Creates a `PrecomputeQLLogisticSin` and `AllocateQLLogisticSin` struct, returning
    them in that order.
"""
function initialize(progData::QLLogisticSin{T, S}) where {T, S}
    precomp = PrecomputeQLLogisticSin(progData)
    store = AllocateQLLogisticSin(progData)

    return precomp, store
end

###############################################################################
# Operations that are not in-place. Does not make use of precomputed values.
###############################################################################

args = [:(progData::QLLogisticSin{T, S}), 
        :(x::Vector{T})
       ]

@eval begin

    @doc """
        obj(
            $(join(string.(args),",\n\t    "))
        ) where {T,S}
    
    Computes the objective function at the value `x`.
    """
    function NLPModels.obj($(args...)) where {T, S}
        increment!(progData, :neval_obj)
        η = progData.design * x
        μ_hat = progData.mean.(η)
        obj = 0
        for i in 1:length(progData.response)
            ## create numerical integration problem
            prob = IntegralProblem(
                progData.weighted_residual, 
                (0, μ_hat[i]), 
                progData.response[i]
                )

            ## solve the numerical integration problem
            ## TODO: this is just with some default parameters
            ## TODO: what happens when the code is false
            obj -= solve(prob, HCubatureJL(); reltol = 1e-3, abstol = 1e-3).u
        end

        return T(obj)
    end

    @doc """
        grad(
            $(join(string.(args),",\n\t    "))
        ) where {T,S}

    Computes the gradient function value at `x`.
    """
    function NLPModels.grad($(args...)) where {T, S}
        increment!(progData, :neval_grad)

        # compute values required for gradient
        η = progData.design * x
        μ_hat = progData.mean.(η)
        var = progData.variance.(μ_hat)
        d = progData.mean_first_derivative.(η)
        residual = progData.weighted_residual.(μ_hat, progData.response) 

        # compute and return gradient
        g = copy(x)
        fill!(g, 0)
        for i in 1:length(progData.response)
            g .-= residual[i] * d[i] .* view(progData.design, i, :)
        end
        return g
    end

    @doc """
         objgrad(
            $(join(string.(args),",\n\t    "))
        ) where {T,S}

    Computes the objective function and gradient function value at `x`. The
        values returned are the objective function value followed by the 
        gradient function value. 
    """
    function NLPModels.objgrad($(args...)) where {T, S}
        o = obj(progData, x)
        g = grad(progData, x)
        return o, g 
    end
    
    @doc """
        hess(
            $(join(string.(args),",\n\t    "))
        ) where {T,S}
    
    Computes the Hessian function value at `x`.
    """
    function hess($(args...)) where {T, S}
        increment!(progData, :neval_hess)

        # compoute required quantities
        η = progData.design * x
        μ_hat = progData.mean.(η)
        ∇μ_η = progData.mean_first_derivative.(η)
        ∇∇μ_η = progData.mean_second_derivative.(η)
        var = progData.variance.(μ_hat)
        ∇var = progData.variance_first_derivative.(μ_hat)
        r = progData.weighted_residual.(μ_hat, progData.response)

        # compute hessian
        nobs, nvar = size(progData.design)
        H = zeros(T, nvar, nvar)
        for i in 1:nobs
            t1 = var[i]^(-1) * ∇var[i] * (∇μ_η[i]^2) * r[i]
            t2 = var[i]^(-1) * (∇μ_η[i]^2)
            t3 = r[i] * ∇∇μ_η[i] 
            oi = view(progData.design, i, :) * view(progData.design, i, :)'

            H .-= (t3 - t1 - t2) .* oi 
        end

        return H
    end

end

###############################################################################
# Operations that are not in-place. Makes use of precomputed values. 
###############################################################################

args_pre = [
    :(progData::QLLogisticSin{T, S}),
    :(precomp::PrecomputeQLLogisticSin{T}),
    :(x::Vector{T})
]

@eval begin

    @doc """
        obj(
            $(join(string.(args),",\n\t    "))
        ) where {T,S}
    
    Computes the objective function at the value `x`.
    """
    function NLPModels.obj($(args_pre...)) where {T, S}
        return NLPModels.obj(progData, x)
    end

    @doc """
        grad(
            $(join(string.(args),",\n\t    "))
        ) where {T,S}

    Computes the gradient function value at `x`.
    """
    function NLPModels.grad($(args_pre...)) where {T, S}
        return NLPModels.grad(progData, x)
    end

    @doc """
         objgrad(
            $(join(string.(args),",\n\t    "))
        ) where {T,S}

    Computes the objective function and gradient function value at `x`. The
        values returned are the objective function value followed by the 
        gradient function value. 
    """
    function NLPModels.objgrad($(args_pre...)) where {T, S}
        return NLPModels.objgrad(progData, x)
    end

    @doc """
        hess(
            $(join(string.(args),",\n\t    "))
        ) where {T,S}
    
    Computes the Hessian function value at `x`.
        Utilizes the precomputed values in `precomp`.
    """
    function NLPModels.hess($(args_pre...)) where {T, S}
        increment!(progData, :neval_hess)

        # compoute required quantities
        η = progData.design * x
        μ_hat = progData.mean.(η)
        ∇μ_η = progData.mean_first_derivative.(η)
        ∇∇μ_η = progData.mean_second_derivative.(η)
        var = progData.variance.(μ_hat)
        ∇var = progData.variance_first_derivative.(μ_hat)
        r = progData.weighted_residual.(μ_hat, progData.response)

        # compute hessian
        nobs, nvar = size(progData.design)
        H = zeros(T, nvar, nvar)
        for i in 1:nobs
            t1 = var[i]^(-1) * ∇var[i] * (∇μ_η[i]^2) * r[i]
            t2 = var[i]^(-1) * (∇μ_η[i]^2)
            t3 = r[i] * ∇∇μ_η[i] 

            H .-= (t3 - t1 - t2) .* view(precomp.obs_obs_t, i, :, :)
        end

        return H
    end
end

###############################################################################
# Operations that are in-place. Makes use of precomputed values. 
###############################################################################

args_store = [
    :(progData::QLLogisticSin{T, S}),
    :(precomp::PrecomputeQLLogisticSin{T}),
    :(store::AllocateQLLogisticSin{T}),
    :(x::Vector{T})
]

@eval begin

    @doc """
        obj(
            $(join(string.(args),",\n\t    "))
        ) where {T,S}
    
    Computes the objective function at the value `x`.
        If `recompute = false`, then values already stored in `store` are 
        used in the computation, otherwise the necessary values are recomputed
        and used. 
    """
    function NLPModels.obj($(args_store...); recompute = true) where {T, S}
        increment!(progData, :neval_obj)
        
        # recompute possible values
        if recompute
            store.linear_effect .= progData.design * x
            store.μ .= progData.mean.(store.linear_effect)
        end
        
        # recompute possible objective functions
        obj = 0
        for i in 1:length(progData.response)
            ## create numerical integration problem
            prob = IntegralProblem(
                progData.weighted_residual, 
                (0, store.μ[i]), 
                progData.response[i]
                )

            ## solve the numerical integration problem
            ## TODO: this is just with some default parameters
            ## TODO: what happens when the code is false
            obj -= solve(prob, HCubatureJL(); reltol = 1e-3, abstol = 1e-3).u
        end

        return obj
    end

    @doc """
        grad(
            $(join(string.(args),",\n\t    "))
        ) where {T,S}

    Computes the gradient function value at `x`.
        Stores the computed gradient vector into `store.grad`. If 
        `recompute = false` then values that are already in `store`` are used
        for computation. Otherwise, values are recomputed and used.
    """
    function NLPModels.grad!($(args_store...); recompute = true) where {T, S}
        increment!(progData, :neval_grad)
        if recompute
            store.linear_effect .= progData.design * x
            store.μ .= progData.mean.(store.linear_effect)
            store.∇μ_η .= progData.mean_first_derivative.(store.linear_effect)
            store.variance .= progData.variance.(store.μ)
            store.weighted_residual .= progData.weighted_residual.(store.μ, progData.response)
        end

        fill!(store.grad, 0)
        for i in 1:length(progData.response)
            store.grad .-= store.weighted_residual[i] * store.∇μ_∇η[i] .* 
                view(progData.design, i, :)
        end
    end

    @doc """
         objgrad(
            $(join(string.(args),",\n\t    "))
        ) where {T,S}

    Computes the objective function and gradient function value at `x`. The
        values returned are the objective function and the gradient is
        stored in `store.grad`. If `recompute = false`, then values already 
        in `store` are used for computation, otherwise values required in the
        computation are computed and used.
    """
    function NLPModels.objgrad($(args_store...); recompute = true) where {T, S}
        NLPModels.grad!(progData, precomp, store, x; recompute = recompute)
        o = NLPModels.obj(progData, precomp, store, x; recompute = false)
        return o
    end

    @doc """
        hess(
            $(join(string.(args),",\n\t    "))
        ) where {T,S}
    
    Computes the Hessian function value at `x`.
        Utilizes the precomputed values in `precomp` and stores the result in
        `store.hess`. If `recompute = false`, tries to compute the hessian
        with values already stored in `store`, otherwise recomputes the 
        necessary quantities and computes the hessian.
    """
    function hess!($(args_store...); recompute = true) where {T, S}
        increment!(progData, :neval_hess)

        # compoute required quantities
        if recompute
            store.linear_effect .= progData.design * x
            store.μ .= progData.mean.(store.linear_effect)
            store.∇μ_η .= progData.mean_first_derivative.(store.linear_effect)
            store.∇∇μ_η .= progData.mean_second_derivative.(store.linear_effect)
            store.variance .= progData.variance.(store.μ)
            store.∇variance .= progData.variance_first_derivative.(store.μ)
            store.weighted_residual .= 
                progData.weighted_residual.(store.μ, progData.response)
        end

        # compute hessian
        nobs = size(progData.design, 1)
        fill!(store.hess, 0)
        for i in 1:nobs
            t1 = store.variance[i]^(-1) * store.∇variance[i] * 
                (store.∇μ_η[i]^2) * store.weighted_residual[i]
            t2 = store.variance[i]^(-1) * (store.∇μ_η[i]^2)
            t3 = store.residual[i] * store.∇∇μ_η[i] 

            store.hess .-= (t3 - t1 - t2) .* view(precomp.obs_obs_t, i, :, :)
        end
    end

end