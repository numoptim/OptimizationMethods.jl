<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Methods · OptimizationMethods.jl Documentation</title><meta name="title" content="Methods · OptimizationMethods.jl Documentation"/><meta property="og:title" content="Methods · OptimizationMethods.jl Documentation"/><meta property="twitter:title" content="Methods · OptimizationMethods.jl Documentation"/><meta name="description" content="Documentation for OptimizationMethods.jl Documentation."/><meta property="og:description" content="Documentation for OptimizationMethods.jl Documentation."/><meta property="twitter:description" content="Documentation for OptimizationMethods.jl Documentation."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">OptimizationMethods.jl Documentation</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Manual</span><ul><li><input class="collapse-toggle" id="menuitem-2-1" type="checkbox"/><label class="tocitem" for="menuitem-2-1"><span class="docs-label">Problems</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../problems/quasilikelihood_estimation/">Quasi-likelihood Estimation</a></li></ul></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../api_problems/">Problems</a></li><li class="is-active"><a class="tocitem" href>Methods</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Barzilai-Borwein-Method"><span>Barzilai Borwein Method</span></a></li><li class="toplevel"><a class="tocitem" href="#Gradient-Descent-with-Fixed-Step-Size"><span>Gradient Descent with Fixed Step Size</span></a></li><li class="toplevel"><a class="tocitem" href="#Lipschitz-Approximation-(Malitsky-and-Mishchenko)"><span>Lipschitz Approximation (Malitsky &amp; Mishchenko)</span></a></li><li class="toplevel"><a class="tocitem" href="#Weighted-Norm-Damping-Gradient-Method-(WNGrad)"><span>Weighted Norm Damping Gradient Method (WNGrad)</span></a></li><li class="toplevel"><a class="tocitem" href="#Nesterov&#39;s-Accelerated-Gradient-Descent"><span>Nesterov&#39;s Accelerated Gradient Descent</span></a></li><li class="toplevel"><a class="tocitem" href="#Gradient-Descent-with-Diminishing-Step-Size"><span>Gradient Descent with Diminishing Step Size</span></a></li><li class="toplevel"><a class="tocitem" href="#Gradient-Descent-with-Backtracking"><span>Gradient Descent with Backtracking</span></a></li><li class="toplevel"><a class="tocitem" href="#Gradient-Descent-with-Non-sequential-Armijo-Line-Search"><span>Gradient Descent with Non-sequential Armijo Line Search</span></a></li><li class="toplevel"><a class="tocitem" href="#Line-search-Helper-Functions"><span>Line search Helper Functions</span></a></li><li><a class="tocitem" href="#Backtracking"><span>Backtracking</span></a></li><li><a class="tocitem" href="#Non-sequential-Armijo"><span>Non-sequential Armijo</span></a></li><li class="toplevel"><a class="tocitem" href="#Index"><span>Index</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API</a></li><li class="is-active"><a href>Methods</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Methods</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/numoptim/OptimizationMethods.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/numoptim/OptimizationMethods.jl/blob/main/docs/src/api_methods.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Contents"><a class="docs-heading-anchor" href="#Contents">Contents</a><a id="Contents-1"></a><a class="docs-heading-anchor-permalink" href="#Contents" title="Permalink"></a></h1><ul><li><a href="#Contents">Contents</a></li><li><a href="#Barzilai-Borwein-Method">Barzilai Borwein Method</a></li><li><a href="#Gradient-Descent-with-Fixed-Step-Size">Gradient Descent with Fixed Step Size</a></li><li><a href="#Lipschitz-Approximation-(Malitsky-and-Mishchenko)">Lipschitz Approximation (Malitsky &amp; Mishchenko)</a></li><li><a href="#Weighted-Norm-Damping-Gradient-Method-(WNGrad)">Weighted Norm Damping Gradient Method (WNGrad)</a></li><li><a href="#Nesterov&#39;s-Accelerated-Gradient-Descent">Nesterov&#39;s Accelerated Gradient Descent</a></li><li><a href="#Gradient-Descent-with-Diminishing-Step-Size">Gradient Descent with Diminishing Step Size</a></li><li><a href="#Gradient-Descent-with-Backtracking">Gradient Descent with Backtracking</a></li><li><a href="#Gradient-Descent-with-Non-sequential-Armijo-Line-Search">Gradient Descent with Non-sequential Armijo Line Search</a></li><li><a href="#Line-search-Helper-Functions">Line search Helper Functions</a></li><li class="no-marker"><ul><li><a href="#Backtracking">Backtracking</a></li><li><a href="#Non-sequential-Armijo">Non-sequential Armijo</a></li></ul></li><li><a href="#Index">Index</a></li></ul><h1 id="Barzilai-Borwein-Method"><a class="docs-heading-anchor" href="#Barzilai-Borwein-Method">Barzilai Borwein Method</a><a id="Barzilai-Borwein-Method-1"></a><a class="docs-heading-anchor-permalink" href="#Barzilai-Borwein-Method" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.barzilai_borwein_gd" href="#OptimizationMethods.barzilai_borwein_gd"><code>OptimizationMethods.barzilai_borwein_gd</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">barzilai_borwein_gd(optData::BarzilaiBorweinGD{T}, progData::P 
    where P &lt;: AbstractNLPModel{T, S}) where {T,S}</code></pre><p>Implements gradient descent with Barzilai-Borwein step size and applies the      method to the optimization problem specified by <code>progData</code>. </p><p><strong>Reference(s)</strong></p><p><a href="../references/#barzilai1988Twopoint">Barzilai and Borwein. &quot;Two-Point Step Size Gradient Methods&quot;. IMA Journal of      Numerical Analysis.</a></p><p><strong>Method</strong></p><p>Given iterates <span>$\lbrace x_0,\ldots,x_k\rbrace$</span>, the iterate <span>$x_{k+1}$</span>     is equal to <span>$x_k - \alpha_k \nabla f(x_k)$</span>, where <span>$\alpha_k$</span> is     one of two versions.</p><p><strong>Long Step Size Version (if <code>optData.long_stepsize==true</code>)</strong></p><p>If <span>$k=0$</span>, then <span>$\alpha_0$</span> is set to <code>optData.init_stepsize</code>. For <span>$k&gt;0$</span>,</p><p class="math-container">\[\alpha_k = \frac{ \Vert x_k - x_{k-1} \Vert_2^2}{(x_k - x_{k-1})^\intercal 
    (\nabla f(x_k) - \nabla f(x_{k-1}))}.\]</p><p><strong>Short Step Size Version (if <code>optData.long_stepsize==false</code>)</strong></p><p>If <span>$k=0$</span>, then <span>$\alpha_0$</span> is set to <code>optData.init_stepsize</code>. For <span>$k&gt;0$</span>,</p><p class="math-container">\[\alpha_k = \frac{(x_k - x_{k-1})^\intercal (\nabla f(x_k) - 
    \nabla f(x_{k-1}))}{\Vert \nabla f(x_k) - \nabla f(x_{k-1})\Vert_2^2}.\]</p><p><strong>Arguments</strong></p><ul><li><code>optData::BarzilaiBorweinGD{T}</code>, the specification for the optimization method.</li><li><code>progData&lt;:AbstractNLPModel{T,S}</code>, the specification for the optimization   problem. </li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>progData</code> must have an <code>initialize</code> function that returns subtypes of <code>AbstractPrecompute</code> and <code>AbstractProblemAllocate</code>, where the latter has a <code>grad</code> argument.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_barzilai_borwein.jl#L116-L163">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.BarzilaiBorweinGD" href="#OptimizationMethods.BarzilaiBorweinGD"><code>OptimizationMethods.BarzilaiBorweinGD</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">BarzilaiBorweinGD{T} &lt;: AbstractOptimizerData{T}</code></pre><p>A structure for storing data about gradient descent using the Barzilai-Borwein      step size, and the progress of its application on an optimization problem.</p><p><strong>Fields</strong></p><ul><li><code>name:String</code>, name of the solver for reference.</li><li><code>init_stepsize::T</code>, initial step size to start the method. </li><li><code>long_stepsize::Bool</code>, flag for step size; if true, use the long version of    Barzilai-Borwein. If false, use the short version. </li><li><code>threshold::T</code>, gradient threshold. If the norm gradient is below this, then    iteration stops.</li><li><code>max_iterations::Int64</code>, max number of iterations (gradient steps) taken by    the solver.</li><li><code>iter_diff::Vector{T}</code>, a buffer for storing differences between subsequent   iterate values that are used for computing the step size</li><li><code>grad_diff::Vector{T}</code>, a buffer for storing differences between gradient    values at adjacent iterates, which is used to compute the step size</li><li><code>iter_hist::Vector{Vector{T}}</code>, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration <code>0</code>). The <code>k+1</code> entry   corresponds to the iterate at iteration <code>k</code>.</li><li><code>grad_val_hist::Vector{T}</code>, a vector for storing <code>max_iterations+1</code> gradient   norm values. The first entry corresponds to iteration <code>0</code>. The <code>k+1</code> entry   correpsonds to the gradient norm at iteration <code>k</code>.</li><li><code>stop_iteration::Int64</code>, the iteration number that the solver stopped on.   The terminal iterate is saved at <code>iter_hist[stop_iteration+1]</code>.</li></ul><p><strong>Constructors</strong></p><pre><code class="nohighlight hljs">BarzilaiBorweinGD(::Type{T}; x0::Vector{T}, init_stepsize::T, 
    long_stepsize::Bool, threshold::T, max_iterations::Int) where {T}</code></pre><p>Constructs the <code>struct</code> for the Barzilai-Borwein optimization method</p><p><strong>Arguments</strong></p><ul><li><code>T::DataType</code>, specific data type used for calculations.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>x0::Vector{T}</code>, initial point to start the solver at.</li><li><code>init_stepsize::T</code>, initial step size used for the first iteration. </li><li><code>long_stepsize::Bool</code>, flag for step size; if true, use the long version of   Barzilai-Borwein, if false, use the short version. </li><li><code>threshold::T</code>, gradient threshold. If the norm gradient is below this,    then iteration is terminated. </li><li><code>max_iterations::Int</code>, max number of iterations (gradient steps) taken by    the solver.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_barzilai_borwein.jl#L3-L54">source</a></section></article><h1 id="Gradient-Descent-with-Fixed-Step-Size"><a class="docs-heading-anchor" href="#Gradient-Descent-with-Fixed-Step-Size">Gradient Descent with Fixed Step Size</a><a id="Gradient-Descent-with-Fixed-Step-Size-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-Descent-with-Fixed-Step-Size" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.fixed_step_gd" href="#OptimizationMethods.fixed_step_gd"><code>OptimizationMethods.fixed_step_gd</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">fixed_step_gd(optData::FixedStepGD{T}, progData&lt;:AbstractNLPModel{T,S})
    where {T,S}</code></pre><p>Implements fixed step-size gradient descent for the desired optimization problem     specified by <code>progData</code>.</p><p><strong>Method</strong></p><p>The iterates are updated according to the procedure</p><p class="math-container">\[x_{k+1} = x_k - \alpha ∇f(x_k),\]</p><p>where <span>$\alpha$</span> is the step size, <span>$f$</span> is the objective function, and <span>$∇f$</span> is the      gradient function of <span>$f$</span>. </p><p><strong>Arguments</strong></p><ul><li><code>optData::FixedStepGD{T}</code>, the specification for the optimization method.</li><li><code>progData&lt;:AbstractNLPModel{T,S}</code>, the specification for the optimization   problem. </li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>progData</code> must have an <code>initialize</code> function that returns subtypes of <code>AbstractPrecompute</code> and <code>AbstractProblemAllocate</code>, where the latter has a <code>grad</code> argument. </p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_fixed.jl#L72-L100">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.FixedStepGD" href="#OptimizationMethods.FixedStepGD"><code>OptimizationMethods.FixedStepGD</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FixedStepGD{T} &lt;: AbstractOptimizerData{T}</code></pre><p>A structure for storing data about fixed step-size gradient descent, and the     progress of its application on an optimization problem.</p><p><strong>Fields</strong></p><ul><li><code>name::String</code>, name of the solver for reference</li><li><code>step_size::T</code>, the step-size selection for the optimization procedure</li><li><code>threshold::T</code>, the threshold on the norm of the gradient to induce stopping</li><li><code>max_iterations::Int</code>, the maximum allowed iterations</li><li><code>iter_hist::Vector{Vector{T}}</code>, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration <code>0</code>). The <code>k+1</code> entry   corresponds to the iterate at iteration <code>k</code>.</li><li><code>grad_val_hist::Vector{T}</code>, a vector for storing <code>max_iterations+1</code> gradient   norm values. The first entry corresponds to iteration <code>0</code>. The <code>k+1</code> entry   correpsonds to the gradient norm at iteration <code>k</code></li><li><code>stop_iteration</code>, iteration number that the algorithm stopped at.   Iterate number <code>stop_iteration</code> is produced. </li></ul><p><strong>Constructors</strong></p><pre><code class="nohighlight hljs">FixedStepGD(::Type{T}; x0::Vector{T}, step_size::T, threshold::T, 
    max_iterations::Int) where {T}</code></pre><p>Constructs the <code>struct</code> for the optimizer.</p><p><strong>Arguments</strong></p><ul><li><code>T::DataType</code>, specific data type for the calculations</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>x0::Vector{T}</code>, the initial iterate for the optimizers</li><li><code>step_size::T</code>, the step size of the optimizer </li><li><code>threshold::T</code>, the threshold on the norm of the gradient to induce stopping</li><li><code>max_iterations::Int</code>, the maximum number of iterations allowed  </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_fixed.jl#L3-L41">source</a></section></article><h1 id="Lipschitz-Approximation-(Malitsky-and-Mishchenko)"><a class="docs-heading-anchor" href="#Lipschitz-Approximation-(Malitsky-and-Mishchenko)">Lipschitz Approximation (Malitsky &amp; Mishchenko)</a><a id="Lipschitz-Approximation-(Malitsky-and-Mishchenko)-1"></a><a class="docs-heading-anchor-permalink" href="#Lipschitz-Approximation-(Malitsky-and-Mishchenko)" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.lipschitz_approximation_gd" href="#OptimizationMethods.lipschitz_approximation_gd"><code>OptimizationMethods.lipschitz_approximation_gd</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">lipschitz_approximation_gd(optData::FixedStepGD{T}, progData::P where P 
    &lt;: AbstractNLPModel{T, S}) where {T, S}</code></pre><p>Implements gradient descent with adaptive step sizes formed through a lipschitz      approximation for the desired optimization problem specified by <code>progData</code>.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>This method is designed for convex optimization problems.</p></div></div><p><strong>References(s)</strong></p><p><a href="../references/#malitsky2020Adaptive">Malitsky, Y. and Mishchenko, K. (2020). &quot;Adaptive Gradient Descent without      Descent.&quot;      Proceedings of the 37th International Conference on Machine Learning,      in Proceedings of Machine Learning Research 119:6702-6712.     </a></p><p><strong>Method</strong></p><p>The iterates are updated according to the procedure,</p><p class="math-container">\[x_{k+1} = x_{k} - \alpha_k \nabla f(x_{k}),\]</p><p>where <span>$\alpha_k$</span> is the step size and <span>$\nabla f$</span> is the gradient function      of the objective function <span>$f$</span>.</p><p>The step size is computed depending on <span>$k$</span>.      When <span>$k = 0$</span>, the step size is set to <code>optData.init_stepsize</code>.      When <span>$k &gt; 0$</span>, </p><p class="math-container">\[\alpha_k = \min\left\lbrace \sqrt{1 + \theta_{k-1}}\alpha_{k-1}, 
    \frac{\Vert x_k - x_{k-1} \Vert}{\Vert \nabla f(x_k) - 
    \nabla f(x_{k-1})\Vert} \right\rbrace,\]</p><p>where <span>$\theta_0 = \inf$</span> and <span>$\theta_k = \alpha_k / \alpha_{k-1}$</span>.</p><p><strong>Arguments</strong></p><ul><li><code>optData::LipschitzApproxGD{T}</code>, the specification for the optimization method.</li><li><code>progData&lt;:AbstractNLPModel{T,S}</code>, the specification for the optimization   problem. </li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>progData</code> must have an <code>initialize</code> function that returns subtypes of <code>AbstractPrecompute</code> and <code>AbstractProblemAllocate</code>, where the latter has a <code>grad</code> argument. </p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_lipschitz_approximation.jl#L98-L150">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.LipschitzApproxGD" href="#OptimizationMethods.LipschitzApproxGD"><code>OptimizationMethods.LipschitzApproxGD</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LipschitzApproxGD{T} &lt;: AbstractOptimizerData{T}</code></pre><p>A structure for storing data about adaptive gradient descent     using a Lipschitz Approximation scheme (AdGD), and the progress      of its application on an optimization problem.</p><p><strong>Fields</strong></p><ul><li><code>name::String</code>, name of the solver for reference</li><li><code>init_stepsize::T</code>, the initial step size for the method</li><li><code>prev_stepsize::T</code>, step size used at <code>iter - 1</code> when <code>iter &gt; 1</code>.</li><li><code>theta::T</code>, element used in the computation of the step size. See the    referenced paper for more information.</li><li><code>lipschitz_approximation::T</code>, help the lipschitz approximation used in the   computation of the step size. See the referenced paper for more information.</li><li><code>threshold::T</code>, the threshold on the norm of the gradient to induce stopping</li><li><code>max_iterations::Int64</code>, the maximum allowed iterations</li><li><code>iter_diff::Vector{T}</code>, a buffer for storing differences between subsequent   iterate values that are used for computing the step size</li><li><code>grad_diff::Vector{T}</code>, a buffer for storing differences between gradient    values at adjacent iterates, which is used to compute the step size</li><li><code>iter_hist::Vector{Vector{T}}</code>, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration <code>0</code>). The <code>k+1</code> entry   corresponds to the iterate at iteration <code>k</code>.</li><li><code>grad_val_hist::Vector{T}</code>, a vector for storing <code>max_iterations+1</code> gradient   norm values. The first entry corresponds to iteration <code>0</code>. The <code>k+1</code> entry   corresponds to the gradient norm at iteration <code>k</code></li><li><code>stop_iteration::Int64</code>, iteration number that the algorithm stopped at.   Iterate number <code>stop_iteration</code> is produced. </li></ul><p><strong>Constructors</strong></p><pre><code class="nohighlight hljs">LipschitzApproxGD(::Type{T}; x0::Vector{T}, init_stepsize::T, threshold::T, 
    max_iterations::Int) where {T}</code></pre><p>Constructs the <code>struct</code> for the optimizer.</p><p><strong>Arguments</strong></p><ul><li><code>T::DataType</code>, specific data type for the calculations</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>x0::Vector{T}</code>, the initial iterate for the optimizers</li><li><code>init_stepsize::T</code>, the initial step size for the method</li><li><code>threshold::T</code>, the threshold on the norm of the gradient to induce stopping</li><li><code>max_iterations::Int</code>, the maximum number of iterations allowed  </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_lipschitz_approximation.jl#L5-L53">source</a></section></article><h1 id="Weighted-Norm-Damping-Gradient-Method-(WNGrad)"><a class="docs-heading-anchor" href="#Weighted-Norm-Damping-Gradient-Method-(WNGrad)">Weighted Norm Damping Gradient Method (WNGrad)</a><a id="Weighted-Norm-Damping-Gradient-Method-(WNGrad)-1"></a><a class="docs-heading-anchor-permalink" href="#Weighted-Norm-Damping-Gradient-Method-(WNGrad)" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.weighted_norm_damping_gd" href="#OptimizationMethods.weighted_norm_damping_gd"><code>OptimizationMethods.weighted_norm_damping_gd</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">weighted_norm_damping_gd(optData::WeightedNormDampingGD{T}, 
    progData::P where P &lt;: AbstractNLPModel{T, S}) where {T, S}</code></pre><p>Method that implements gradient descent with weighted norm damping step size      using the specifications in <code>optData</code> on the problem specified by <code>progData</code>.</p><p><strong>Reference</strong></p><p>Wu, Xiaoxia et. al. &quot;WNGrad: Learn the Learning Rate in Gradient Descent&quot;. arxiv,      https://arxiv.org/abs/1803.02865</p><p><strong>Method</strong></p><p>Let <span>$\theta_k$</span> be the <span>$k^{th}$</span> iterate, and <span>$\alpha_k$</span> be the <span>$k^{th}$</span>      step size. The optimization method generate iterates following</p><p class="math-container">\[\theta_{k + 1} = \theta_{k} - \alpha_k \nabla f(\theta_k),\]</p><p>where <span>$\nabla f$</span> is the gradient of the objective function <span>$f$</span>.</p><p>The step size depends on the iteration number <span>$k$</span>. For <span>$k = 0$</span>, the step      size <span>$\alpha_0$</span> is the reciprocal of <code>optData.init_norm_damping_factor</code>.      For <span>$k &gt; 0$</span>, the step size is iteratively updated as</p><p class="math-container">\[\alpha_k = \left[
\frac{1}{\alpha_{k-1}} + \Vert\dot F(\theta_k)\Vert_2^2 \alpha_{k-1}
\right]^{-1}.\]</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>When <span>$\alpha_0 &lt; \Vert\dot F(\theta_0)\Vert_2^{-1}$</span> and a globally Lipschitz smooth objective function is used, then the method is guaranteed to find an <span>$\epsilon$</span>-stationary point. It is recommended then that  <code>optData.init_norm_damping_factor</code> exceed <span>$\Vert\dot F(\theta_0)\Vert_2$</span>.</p></div></div><p><strong>Arguments</strong></p><ul><li><code>optData::WeightedNormDampingGD{T}</code>, specification for the optimization algorithm.</li><li><code>progData::P where P &lt;: AbstractNLPModel{T, S}</code>, specification for the problem.</li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>progData</code> must have an <code>initialize</code> function that returns subtypes of <code>AbstractPrecompute</code> and <code>AbstractProblemAllocate</code>, where the latter has a <code>grad</code> argument. </p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_weighted_norm_damping.jl#L96-L143">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.WeightedNormDampingGD" href="#OptimizationMethods.WeightedNormDampingGD"><code>OptimizationMethods.WeightedNormDampingGD</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WeightedNormDampingGD{T} &lt;: AbstractOptimizerData{T}</code></pre><p>A mutable struct that represents gradient descent using the weighted-norm      damping step size. It stores the specification for the method and records      values during iteration.</p><p><strong>Fields</strong></p><ul><li><code>name::String</code>, name of the optimizer for recording purposes</li><li><code>init_norm_damping_factor::T</code>, initial damping factor. This value&#39;s reciprocal    will be the initial step size.</li><li><code>threshold::T</code>, norm gradient tolerance condition. Induces stopping when norm    is at most <code>threshold</code>.</li><li><code>max_iterations::Int64</code>, max number of iterates that are produced, not    including the initial iterate.</li><li><code>iter_hist::Vector{Vector{T}}</code>, store the iterate sequence as the algorithm    progresses. The initial iterate is stored in the first position.</li><li><code>grad_val_hist::Vector{T}</code>, stores the norm gradient values at each iterate.    The norm of the gradient evaluated at the initial iterate is stored in the    first position.</li><li><code>stop_iteration::Int64</code>, the iteration number the algorithm stopped on. The    iterate that induced stopping is saved at <code>iter_hist[stop_iteration + 1]</code>.</li></ul><p><strong>Constructors</strong></p><pre><code class="nohighlight hljs">WeightedNormDampingGD(::Type{T}; x0::Vector{T}, init_norm_damping_factor::T, 
    threshold::T, max_iterations::Int64) where {T}</code></pre><p>Constructs an instance of type <code>WeightedNormDampingGD{T}</code>.</p><p><strong>Arguments</strong></p><ul><li><code>T::DataType</code>, type for data and computation</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>x0::Vector{T}</code>, initial point to start the optimization routine. Saved in   <code>iter_hist[1]</code>.</li><li><code>init_norm_damping_factor::T</code>, initial damping factor, which will correspond   to the reciprocoal of the initial step size. </li><li><code>threshold::T</code>, norm gradient tolerance condition. Induces stopping when norm    at most <code>threshold</code>.</li><li><code>max_iterations::Int64</code>, max number of iterates that are produced, not    including the initial iterate.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_weighted_norm_damping.jl#L5-L50">source</a></section></article><h1 id="Nesterov&#39;s-Accelerated-Gradient-Descent"><a class="docs-heading-anchor" href="#Nesterov&#39;s-Accelerated-Gradient-Descent">Nesterov&#39;s Accelerated Gradient Descent</a><a id="Nesterov&#39;s-Accelerated-Gradient-Descent-1"></a><a class="docs-heading-anchor-permalink" href="#Nesterov&#39;s-Accelerated-Gradient-Descent" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.nesterov_accelerated_gd" href="#OptimizationMethods.nesterov_accelerated_gd"><code>OptimizationMethods.nesterov_accelerated_gd</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">nesterov_accelerated_gd(optData::NesterovAcceleratedGD{T}, progData::P
    where P &lt;: AbstractNLPModel{T, S}) where {T, S}</code></pre><p>Implements Nesterov&#39;s Accelerated Gradient Descent as specified by <code>optData</code> on the problem specified by <code>progData</code>.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>This algorithm is designed for convex problems.</p></div></div><p><strong>Reference(s)</strong></p><ul><li>See Algorithm 1 of <a href="../references/#li2023Convex">Li et. al. &quot;Convex and Non-convex Optimization Under    Generalized Smoothness&quot;. arxiv, https://arxiv.org/abs/2306.01264.</a></li><li>See line-search based approach in <a href="../references/#nesterov1983Method">Nesterov, Yurii. 1983. “A Method for    Solving the Convex Programming Problem    with Convergence Rate O(1/K^2).” Proceedings of the USSR Academy of Sciences    269:543–47.</a></li></ul><p><strong>Method</strong></p><p>Let the objective function be denoted by <span>$F(\theta)$</span> and <span>$\nabla F(     \theta)$</span> denote its gradient function. Given <span>$\theta_0$</span> and  a step      size <span>$\alpha$</span> (equal to <code>optData.step_size</code>), the method produces five      sequences. At <span>$k=0$</span>,</p><p class="math-container">\[\begin{cases}
    B_0 &amp;= 0 \\
    z_0 &amp;= \theta_0 \\
    \Delta_0 &amp; = 1 \\
    y_0 &amp;= \theta_0;
\end{cases}\]</p><p>and, for <span>$k\in\mathbb{N}$</span>,</p><p class="math-container">\[\begin{cases}
    B_{k} &amp;= B_{k-1} + \Delta_{k-1} \\
    \theta_k &amp;= y_{k-1} - \alpha \nabla F(y_{k-1}) \\
    z_k &amp;= z_{k-1} - \alpha\Delta_{k-1}\nabla F(y_{k-1}) \\
    \Delta_k &amp;= \frac{1}{2}\left( 1 + \sqrt{4 B_{k} + 1}  \right) \\
    y_k &amp;= \theta_k + \frac{\Delta_{k}}{B_k + \Delta_k + \alpha^{-1}} 
    (z_k - \theta_k).
\end{cases}\]</p><p>The iterate sequence of interest is <span>$\lbrace \theta_k \rbrace$</span>.</p><p><strong>Arguments</strong></p><ul><li><code>optData::NesterovAcceleratedGD{T}</code>, the specification for the optimization    method.</li><li><code>progData&lt;:AbstractNLPModel{T,S}</code>, the specification for the optimization   problem.</li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>progData</code> must have an <code>initialize</code> function that returns subtypes of <code>AbstractPrecompute</code> and <code>AbstractProblemAllocate</code>, where the latter has a <code>grad</code> argument.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_nesterov_accelerated.jl#L116-L176">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.NesterovAcceleratedGD" href="#OptimizationMethods.NesterovAcceleratedGD"><code>OptimizationMethods.NesterovAcceleratedGD</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NesterovAcceleratedGD{T} &lt;: AbstractOptimizerData</code></pre><p>A structure that represents Nesterov Accelerated Gradient Descent. Stores variables related to the method, and tracks quantities as the algorithm progresses.</p><p><strong>Fields</strong></p><ul><li><code>name:String</code>, name of the solver for reference.</li><li><code>step_size::T</code>, step size used in the method. </li><li><code>z::Vector{T}</code>, buffer array for auxiliary iterate sequence</li><li><code>y::Vector{T}</code>, buffer array for convex combination of iterate and auxiliary   sequence </li><li><code>B::T</code>, auxiliary quadratic scaling term for computing acceleration weights    and step size.</li><li><code>threshold::T</code>, gradient threshold. If the norm gradient is below this, then    iteration stops.</li><li><code>max_iterations::Int64</code>, max number of iterations (gradient steps) taken by    the solver.</li><li><code>iter_hist::Vector{Vector{T}}</code>, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration <code>0</code>). The <code>k+1</code> entry   corresponds to the iterate at iteration <code>k</code>.</li><li><code>grad_val_hist::Vector{T}</code>, a vector for storing <code>max_iterations+1</code> gradient   norm values. The first entry corresponds to iteration <code>0</code>. The <code>k+1</code> entry   corresponds to the gradient norm at iteration <code>k</code>.</li><li><code>stop_iteration::Int64</code>, the iteration number that the solver stopped on.   The terminal iterate is saved at <code>iter_hist[stop_iteration+1]</code>.</li></ul><p><strong>Constructors</strong></p><pre><code class="nohighlight hljs">    NesterovAcceleratedGD(::Type{T}; x0::Vector{T}, step_size::T,
        threshold::T, max_iterations::Int64) where {T}</code></pre><p><strong>Arguments</strong></p><ul><li><code>T::DataType</code>, specific data type used for calculations.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>x0::Vector{T}</code>, initial point to start the solver at.</li><li><code>step_size::T</code>, step size used in the method. </li><li><code>threshold::T</code>, gradient threshold. If the norm gradient is below this,    then iteration is terminated. </li><li><code>max_iterations::Int</code>, max number of iterations (gradient steps) taken by    the solver.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_nesterov_accelerated.jl#L5-L51">source</a></section></article><h1 id="Gradient-Descent-with-Diminishing-Step-Size"><a class="docs-heading-anchor" href="#Gradient-Descent-with-Diminishing-Step-Size">Gradient Descent with Diminishing Step Size</a><a id="Gradient-Descent-with-Diminishing-Step-Size-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-Descent-with-Diminishing-Step-Size" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.DiminishingStepGD" href="#OptimizationMethods.DiminishingStepGD"><code>OptimizationMethods.DiminishingStepGD</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DiminishingStepGD{T} &lt;: AbstractOptimizerData{T}</code></pre><p>A structure for storing data about gradient descent using diminishing step sizes,     and recording the progress of its application on an optimization problem.</p><p><strong>Fields</strong></p><ul><li><code>name::String</code>, name of the solver for reference.</li><li><code>step_size_function::Function</code>, step size function. Should take the iteration    number and return the step size for that iteration.</li><li><code>step_size_scaling::T</code>, factor that is multipled with the amount of the step    size function.</li><li><code>threshold::T</code>, gradient threshold. If the norm gradient is below this, then    iteration stops.</li><li><code>max_iterations::Int64</code>, max number of iterations (gradient steps) taken by    the solver.</li><li><code>iter_hist::Vector{Vector{T}}</code>, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration <code>0</code>). The <code>k+1</code> entry   corresponds to the iterate at iteration <code>k</code>.</li><li><code>grad_val_hist::Vector{T}</code>, a vector for storing <code>max_iterations+1</code> gradient   norm values. The first entry corresponds to iteration <code>0</code>. The <code>k+1</code> entry   correpsonds to the gradient norm at iteration <code>k</code>.</li><li><code>stop_iteration::Int64</code>, the iteration number that the solver stopped on.   The terminal iterate is saved at <code>iter_hist[stop_iteration+1]</code>.</li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>step_size_function</code> should take in two arguments, the data type for computation <code>T</code> and the iteration number. For example, calling <code>step_size_function(Float64, 1)</code> should return the step size as a <code>Float64</code> for the iteration <code>1</code>.</p></div></div><p><strong>Constructors</strong></p><pre><code class="nohighlight hljs">DiminishingStepGD(::Type{T}; x0::Vector{T}, step_size_function::Function,
    step_size_scaling::T, threshold::T, max_iterations::Int)</code></pre><p>Constructs the <code>struct</code> for the diminishing step size gradient descent method.</p><p><strong>Arguments</strong></p><ul><li><code>T::DataType</code>, specific data type used for calculations.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>x0::Vector{T}</code>, initial point to start the solver at.</li><li><code>step_size_function::Function</code>, step size function. Should take the iteration    number and return the step size for that iteration.</li><li><code>step_size_scaling::T</code>, factor that is multipled with the amount of the step    size function.</li><li><code>threshold::T</code>, gradient threshold. If the norm gradient is below this,    then iteration is terminated. </li><li><code>max_iterations::Int</code>, max number of iterations (gradient steps) taken by    the solver.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_diminishing.jl#L6-L60">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.diminishing_step_gd" href="#OptimizationMethods.diminishing_step_gd"><code>OptimizationMethods.diminishing_step_gd</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">diminishing_step_gd(optData::DiminishingStepGD{T}, progData::P
    where P &lt;: AbstractNLPModel{T, S}) where {T, S}</code></pre><p>Implements gradient descent with diminishing step sizes and applies the method     to the optimization problem specified by <code>progData</code>.</p><p><strong>Reference(s)</strong></p><p><a href="../references/#patel2024Gradient">Patel, Vivak, and Albert Berahas. “Gradient Descent in the Absence of Global      Lipschitz Continuity of the Gradients.” SIAM 6 (3): 579–846.      https://doi.org/10.1137/22M1527210.</a></p><p><a href="../references/#bertsekas2016Nonlinear">Bertsekas, Dimitri. &quot;Nonlinear Programming&quot;. 3rd Edition, Athena Scientific,      Chapter 1.</a></p><p><strong>Method</strong></p><p>Given iterates <span>$\lbrace x_0,\ldots,x_k\rbrace$</span>, the iterate <span>$x_{k + 1}$</span>     is equal to <span>$x_k - \alpha_k \nabla f(x_k)$</span>, where <span>$\alpha_k$</span> is equal     to <code>optData.step_size_scaling * optData.step_size_function(T, k)</code>.</p><div class="admonition is-info"><header class="admonition-header">Step Size Function</header><div class="admonition-body"><p>The step size function should satisfy several conditions. First, <span>$\alpha_k &gt; 0$</span> for all <span>$k$</span>. Second, <span>$\lim_{k \to \infty} \alpha_k = 0.$</span> Finally, <span>$\sum_{k=0}^{\infty} \alpha_k = \infty.$</span> See <a href="../references/#patel2024Gradient">Patel and Berahas (2024)</a> for details.</p></div></div><p><strong>Arguments</strong></p><ul><li><code>optData::DiminishingStepGD{T}</code>, the specification for the optimization method.</li><li><code>progData&lt;:AbstractNLPModel{T,S}</code>, the specification for the optimization   problem. </li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>progData</code> must have an <code>initialize</code> function that returns subtypes of <code>AbstractPrecompute</code> and <code>AbstractProblemAllocate</code>, where the latter has a <code>grad</code> argument.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_diminishing.jl#L97-L136">source</a></section></article><p>Below is a list of step size functions that are in the library. The step size sequence generated by these functions, <span>$\lbrace \alpha_k \rbrace$</span>  satisfies <span>$\alpha_k &gt; 0$</span>, <span>$\lim_{k \to\infty} \alpha_k = 0$</span> and  <span>$\sum_{k=0}^\infty \alpha_k = \infty$</span>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.inverse_k_step_size" href="#OptimizationMethods.inverse_k_step_size"><code>OptimizationMethods.inverse_k_step_size</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">inverse_k_step_size(::Type{T}, k::Int64)</code></pre><p>Return the inverse of <code>k</code>.</p><p><strong>Method</strong></p><p>The step size sequence generated for <span>$k \in \mathbb{N}$</span> is</p><p class="math-container">\[    \alpha_k = \frac{1}{k+1},\]</p><p>when using this method.</p><p><strong>Arguments</strong></p><ul><li><code>T::Type</code>, data type that the computations are done in.</li><li><code>k::Int64</code>, index of the step size needed. </li></ul><p><strong>Returns</strong></p><p>Returns a number of type <code>T</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/stepsize_helpers/diminishing_stepsizes.jl#L6-L27">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.inverse_log2k_step_size" href="#OptimizationMethods.inverse_log2k_step_size"><code>OptimizationMethods.inverse_log2k_step_size</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">inverse_log2k_step_size(::Type{T}, k::Int64) where {T}</code></pre><p><strong>Method</strong></p><p>The step size sequence generated when using this method is</p><p class="math-container">\[    \alpha_k = \frac{1}{\lfloor \log_2(k+1) + 1 \rfloor}\]</p><p>for <span>$k \in \mathbb{N}$</span>.</p><p><strong>Arguments</strong></p><ul><li><code>T::Type</code>, data type that the computation are done in.</li><li><code>k::Int64</code>, index of the step size needed.</li></ul><p><strong>Returns</strong></p><p>Returns a number of type <code>T</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/stepsize_helpers/diminishing_stepsizes.jl#L32-L51">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.stepdown_100_step_size" href="#OptimizationMethods.stepdown_100_step_size"><code>OptimizationMethods.stepdown_100_step_size</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">stepdown_100_step_size(::Type{T}, k::Int64) where {T}</code></pre><p><strong>Method</strong></p><p>The step size sequence generated is</p><p class="math-container">\[    \alpha_k = \frac{1}{2^i}\]</p><p>for <span>$k \in [ (2^i-1)100, (2^{i+1}-1)100)$</span>.</p><p><strong>Arguments</strong></p><ul><li><code>T::Type</code>, data type that the computation are done in.</li><li><code>k::Int64</code>, index of the step size needed.</li></ul><p><strong>Returns</strong></p><p>Returns a number of type <code>T</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/stepsize_helpers/diminishing_stepsizes.jl#L56-L75">source</a></section></article><h1 id="Gradient-Descent-with-Backtracking"><a class="docs-heading-anchor" href="#Gradient-Descent-with-Backtracking">Gradient Descent with Backtracking</a><a id="Gradient-Descent-with-Backtracking-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-Descent-with-Backtracking" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.backtracking_gd" href="#OptimizationMethods.backtracking_gd"><code>OptimizationMethods.backtracking_gd</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">backtracking_gd(optData::BacktrackingGD{T},
    progData::P where P &lt;: AbstractNLPModel{T, S}) where {T, S}</code></pre><p><strong>Reference(s)</strong></p><p><a href="../references/#armijo1966Minimization">Armijo, Larry. “Minimization of Functions Having Lipschitz Continuous      First Partial Derivatives.” Pacific Journal of Mathematics 16.1      (1966): 1–3. Pacific Journal of Mathematics. Web.</a></p><p><a href="../references/#nocedal2006Numerical">Nocedal and Wright. &quot;Numerical Optimization&quot;.      Springer New York, NY.</a></p><p><strong>Method</strong></p><p>Let <span>$\theta_{k-1}$</span> be the current iterate, and let  <span>$\alpha \in \mathbb{R}_{&gt;0}$</span>, <span>$\delta \in (0, 1)$</span>, and <span>$\rho \in (0, 1)$</span>. The <span>$k^{th}$</span> iterate is generated as <span>$\theta_k = \theta_{k-1} - \delta^t\alpha \dot F(\theta_{k-1})$</span>  where <span>$t + 1 \in \mathbb{N}$</span> is the smallest such number satisfying</p><p class="math-container">\[    F(\theta_k) \leq F(\theta_{k-1}) - \rho\delta^t\alpha
    ||\dot F(\theta_{k-1})||_2^2,\]</p><p>where <span>$||\cdot||_2$</span> is the L2-norm. </p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Theoretically, there exists such a <span>$t$</span>, but it can be made arbitrarily large. Therefore, the line search procedure stops searching after <code>optData.line_search_max_iteration</code>. The current implementation terminates the procedure if the backtracking condition is not satisfied, and returns the previous iterate.</p></div></div><p><strong>Arguments</strong></p><ul><li><code>optData::BarzilaiBorweinGD{T}</code>, the specification for the optimization method.</li><li><code>progData&lt;:AbstractNLPModel{T,S}</code>, the specification for the optimization   problem. </li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>progData</code> must have an <code>initialize</code> function that returns subtypes of <code>AbstractPrecompute</code> and <code>AbstractProblemAllocate</code>, where the latter has a <code>grad</code> argument.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_backtracking.jl#L104-L149">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.BacktrackingGD" href="#OptimizationMethods.BacktrackingGD"><code>OptimizationMethods.BacktrackingGD</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">BacktrackingGD{T} &lt;: AbstractOptimizerData{T}</code></pre><p>Mutable sturcture representing gradient descent using backtracking.     It also stores and keeps track of values during the optimization     routine.</p><p><strong>Fields</strong></p><ul><li><code>name:String</code>, name of the solver for reference.</li><li><code>α::T</code>, initial step size used for backtracking.</li><li><code>δ::T</code>, backtracking decreasing factor applied to <code>α</code> when line   search criterion is not satisfied.</li><li><code>ρ::T</code>, factor involved in the acceptance criterion in the line search   procedure. Larger values correspond to stricter descent conditions, and   smaller values correspond to looser descent conditions.</li><li><code>line_search_max_iteration::Int64</code>, maximum allowable iterations for the   backtracking procedure.</li><li><code>threshold::T</code>, gradient threshold. If the norm gradient is below this, then    iteration stops.</li><li><code>max_iterations::Int64</code>, max number of iterations (gradient steps) taken by    the solver.</li><li><code>iter_hist::Vector{Vector{T}}</code>, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration <code>0</code>). The <code>k+1</code> entry   corresponds to the iterate at iteration <code>k</code>.</li><li><code>grad_val_hist::Vector{T}</code>, a vector for storing <code>max_iterations+1</code> gradient   norm values. The first entry corresponds to iteration <code>0</code>. The <code>k+1</code> entry   correpsonds to the gradient norm at iteration <code>k</code>.</li><li><code>stop_iteration::Int64</code>, the iteration number that the solver stopped on.   The terminal iterate is saved at <code>iter_hist[stop_iteration+1]</code>.</li></ul><p><strong>Constructors</strong></p><pre><code class="nohighlight hljs">BacktrackingGD(::Type{T}; x0::Vector{T}, α::T, δ::T, ρ::T,
    line_search_max_iteration::Int64, threshold::T, max_iteration::Int64)
    where {T}</code></pre><p>Returns a <code>struct</code> of type <code>BacktrackingGD{T}</code> with all the field initialized to     either initialized to the values given, or their default values.</p><p><strong>Arguments</strong></p><ul><li><code>T::DataType</code>, specific data type used for calculations.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>x0::Vector{T}</code>, initial point to start the solver at.</li><li><code>α::T</code>, initial step size used for backtracking.</li><li><code>δ::T</code>, backtracking decreasing factor applied to <code>α</code> when line   search criterion not satisfied.</li><li><code>ρ::T</code>, factor involved in the acceptance criterion in the line search   procedure. Larger values correspond to stricter descent conditions, and   smaller values correspond to looser descent conditions.</li><li><code>line_search_max_iteration::Int64</code>, maximum allowable iterations for the   backtracking procedure.</li><li><code>threshold::T</code>, gradient threshold. If the norm gradient is below this,    then iteration is terminated. </li><li><code>max_iterations::Int</code>, max number of iterations (gradient steps) taken by    the solver.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_backtracking.jl#L5-L64">source</a></section></article><h1 id="Gradient-Descent-with-Non-sequential-Armijo-Line-Search"><a class="docs-heading-anchor" href="#Gradient-Descent-with-Non-sequential-Armijo-Line-Search">Gradient Descent with Non-sequential Armijo Line Search</a><a id="Gradient-Descent-with-Non-sequential-Armijo-Line-Search-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-Descent-with-Non-sequential-Armijo-Line-Search" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.NonsequentialArmijoGD" href="#OptimizationMethods.NonsequentialArmijoGD"><code>OptimizationMethods.NonsequentialArmijoGD</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NonsequentialArmijoGD{T} &lt;: AbstractOptimizerData{T}</code></pre><p>A mutable struct that represents gradient descent with non-sequential armijo     line search and triggering events. It stores the specification for the     method and records values during iteration.</p><p><strong>Fields</strong></p><ul><li><code>name::String</code>, name of the optimizer for reference.</li><li><code>∇F_θk::Vector{T}</code>, buffer array for the gradient of the initial inner   loop iterate.</li><li><code>norm_∇F_ψ::T</code>, norm of the gradient of the current inner loop iterate.</li><li><code>prev_∇F_ψ::Vector{T}</code>, buffer array for the previous gradient in the inner    loop. Necessary for updating the local Lipschitz approximation.</li><li><code>prev_norm_step::T</code>, norm of the step between inner loop iterates.    Used for updating the local Lipschitz approximation.</li><li><code>α0k::T</code>, first step size used in the inner loop. </li><li><code>δk::T</code>, scaling factor used to condition the step size.</li><li><code>δ_upper::T</code>, upper limit imposed on the scaling factor when updating.</li><li><code>ρ::T</code>, parameter used in the non-sequential Armijo condition. Larger   numbers indicate stricter descent conditions. Smaller numbers indicate   less strict descent conditions.</li><li><code>τ_lower::T</code>, lower bound on the gradient interval triggering event.</li><li><code>τ_upper::T</code>, upper bound on the gradient interval triggering event.</li><li><code>local_lipschitz_estimate::T</code>, local Lipshitz approximation.</li><li><code>threshold::T</code>, norm gradient tolerance condition. Induces stopping when norm    is at most <code>threshold</code>.</li><li><code>max_iterations::Int64</code>, max number of iterates that are produced, not    including the initial iterate.</li><li><code>iter_hist::Vector{Vector{T}}</code>, store the iterate sequence as the algorithm    progresses. The initial iterate is stored in the first position.</li><li><code>grad_val_hist::Vector{T}</code>, stores the norm gradient values at each iterate.    The norm of the gradient evaluated at the initial iterate is stored in the    first position.</li><li><code>stop_iteration::Int64</code>, the iteration number the algorithm stopped on. The    iterate that induced stopping is saved at <code>iter_hist[stop_iteration + 1]</code>.</li></ul><p><strong>Constructors</strong></p><pre><code class="nohighlight hljs">NonsequentialArmijoGD(::Type{T}; x0::Vector{T}, δ0::T, δ_upper::T, ρ::T,
    threshold::T, max_iterations::Int64) where {T}</code></pre><p>Constructs an instance of type <code>NonsequentialArmijoGD{T}</code>.</p><p><strong>Arguments</strong></p><ul><li><code>T::DataType</code>, type for data and computation.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>x0::Vector{T}</code>, initial point to start the optimization routine. Saved in   <code>iter_hist[1]</code>.</li><li><code>δ0::T</code>, starting scaling factor.</li><li><code>δ_upper::T</code>, upper limit imposed on the scaling factor when updating.</li><li><code>ρ::T</code>, parameter used in the non-sequential Armijo condition. Larger   numbers indicate stricter descent conditions. Smaller numbers indicate   less strict descent conditions.</li><li><code>threshold::T</code>, norm gradient tolerance condition. Induces stopping when norm    at most <code>threshold</code>.</li><li><code>max_iterations::Int64</code>, max number of iterates that are produced, not    including the initial iterate.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_non_sequential_armijo.jl#L7-L70">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.nonsequential_armijo_gd" href="#OptimizationMethods.nonsequential_armijo_gd"><code>OptimizationMethods.nonsequential_armijo_gd</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">nonsequential_armijo_gd(optData::NonsequentialArmijoGD{T},
    progData::P where P &lt;: AbstractNLPModel{T, S}) where {T, S}</code></pre><p>Implementation of gradient descent with non-sequential armijo and triggering     events. The optimization algorithm is specified through <code>optData</code>, and     applied to the problem <code>progData</code>.</p><p><strong>Reference(s)</strong></p><p><strong>Method</strong></p><p>In what follows, we let <span>$||\cdot||_2$</span> denote the L2-norm.  Let <span>$\theta_{k}$</span> for <span>$k + 1 \in\mathbb{N}$</span> be the <span>$k^{th}$</span> iterate of the optimization algorithm. Let <span>$\delta_{k},  \tau_{\mathrm{grad},\mathrm{lower}}^k, \tau_{\mathrm{grad},\mathrm{upper}}^k$</span> be the <span>$k^{th}$</span> parameters for the optimization method.  The <span>$k+1^{th}$</span> iterate and parameters are produced by the following procedure. </p><p>Let <span>$\psi_0^k = \theta_k$</span>, and recursively define</p><p class="math-container">\[    \psi_{j}^k = \psi_0^k - \sum_{i = 0}^{j-1} \delta_k \alpha_i^k \dot F(\psi_i^k).\]</p><p>Let <span>$j_k \in \mathbb{N}$</span> be the smallest iteration for which at least one of the conditions are satisfied: </p><ol><li><span>$||\psi_{j_k}^k - \theta_k||_2 &gt; 10$</span>, </li><li><span>$||\dot F(\psi_{j_k}^k)||_2 \not\in (\tau_{\mathrm{grad},\mathrm{lower}}^k,  \tau_{\mathrm{grad},\mathrm{upper}}^k)$</span>, </li><li><span>$j_k == 100$</span>.</li></ol><p>The next iterate and algorithmic parameters in <code>optData</code> are updated based on  the result of the non-sequential Armijo condition</p><p class="math-container">\[    F(\psi_{j_k}^k) \leq 
    F(\theta_k) - \rho\delta_k\alpha_0^k||\dot F(\theta_k)||_2.\]</p><p>When this condition is not satisfied, the following quantities are updated.</p><ol><li>The iterate <span>$\psi_{j_k}^k$</span> is rejected, and <span>$\theta_{k+1} = \theta_k$</span></li><li>The scaling factor <span>$\delta_{k+1} = .5\delta_k$</span></li></ol><p>When this condition is satisfied, the following quantities are updated.</p><ol><li>The iterate <span>$\psi_{j_k}^k$</span> is accepted, and <span>$\theta_{k+1} = \psi_{j_k}^k$</span>.</li><li>The scaling factor is updated as <span>$\delta_{k+1} = \min(1.5*\delta_k, \bar\delta)$</span>  when <span>$||\dot F(\psi_{j_k}^k)||_2 &gt; \tau_{\mathrm{grad},\mathrm{lower}}^k$</span>,  otherwise <span>$\delta_{k+1} = \delta_k$</span>.</li><li>If <span>$||\dot F(\psi_{j_k}^k)||_2 \not\in (\tau_{\mathrm{grad},\mathrm{lower}}^k,  \tau_{\mathrm{grad},\mathrm{upper}}^k)$</span>, then   <span>$\tau_{\mathrm{grad},\mathrm{lower}}^{k+1} =   ||\dot F(\psi_{j_k}^k)||_2/\sqrt{2}$</span> and   <span>$\tau_{\mathrm{grad},\mathrm{upper}}^{k+1} =   \sqrt{10}||\dot F(\psi_{j_k}^k)||_2$</span>. Otherwise, this parameters are held  constant.</li></ol><p><strong>Arguments</strong></p><ul><li><code>optData::NesterovAcceleratedGD{T}</code>, the specification for the optimization    method.</li><li><code>progData&lt;:AbstractNLPModel{T,S}</code>, the specification for the optimization   problem.</li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>progData</code> must have an <code>initialize</code> function that returns subtypes of <code>AbstractPrecompute</code> and <code>AbstractProblemAllocate</code>, where the latter has a <code>grad</code> argument.</p></div></div><p><strong>Return</strong></p><ul><li><code>x::S</code>, final iterate of the optimization algorithm.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_non_sequential_armijo.jl#L423-L495">source</a></section></article><p>The method above requires several utility functions. These are listed below.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.update_local_lipschitz_approximation" href="#OptimizationMethods.update_local_lipschitz_approximation"><code>OptimizationMethods.update_local_lipschitz_approximation</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">update_local_lipschitz_approximation(j::Int64, k::Int64, djk::S, curr_grad::S,
    prev_grad::S, prev_approximation::T, prev_acceptance::Bool) where {T, S}</code></pre><p>Given the previous approximation of the local Lipschitz constant,     <code>prev_approximation::T</code>, update the current estimate.      That is, return the Lipschitz approximation for inner loop iteration <code>j</code>      and outer loop iteration <code>k</code>.</p><p><strong>Method</strong></p><p>The local Lipschitz approximation method is conducted as follows. Let <span>$j$</span>     be the current inner loop iteration counter, and let <span>$k$</span> be the current     outer loop iteration counter. Let <span>$\psi_j^k$</span> be the <span>$j^{th}$</span> iterate     of the <span>$k^{th}$</span> outer loop, and let <span>$\hat L_{j}^k$</span> be the <span>$j^th$</span> estimate     of the <span>$k^{th}$</span> outer loop of the local Lipschitz constant. The local     estimate is updated according the following five cases.</p><ol><li><p>When <span>$j == 1$</span> and <span>$k == 1$</span>, this is the first iteration of the first   inner loop, and as there is no information available we set it to <code>1.0</code>.</p></li><li><p>When <span>$j == 1$</span> and <span>$k &gt; 1$</span>, this is the first iteration of the <span>$k^{th}$</span>  inner loop, and we return <span>$L_{j_{k-1}}^{k-1}$</span> which is the local Lipschitz  estimates formed using information at the terminal iteration of the <span>$k-1^{th}$</span>  inner loop (i.e., this is the latest estimate).</p></li><li><p>When <span>$j &gt; 1$</span> and <span>$k == 1$</span>, this is an inner loop iteration where we have  possible taken multiple steps, so we return the most &#39;local&#39; estimate  of the local Lipschitz constant which is  <span>$\frac{      ||\dot F(\psi_{j}^k) - \dot F(\psi_{j-1}^k)||_2}{       ||\psi_{j}^k - \psi_{j-1}^k||_2}.$</span></p></li><li><p>When <span>$j &gt; 1$</span> and <span>$k &gt; 1$</span> and <span>$\psi_{j_{k-1}}^{k-1}$</span> satisfied the  descent condition, then we return   <span>$\frac{  ||\dot F(\psi_{j}^k) - \dot F(\psi_{j-1}^k)||_2}{      ||\psi_{j}^k - \psi_{j-1}^k||_2}.$</span></p></li><li><p>When <span>$j &gt; 1$</span> and <span>$k &gt; 1$</span> and <span>$\psi_{j_{k-1}}^{k-1}$</span> did not satisfy the  descent condition, then we return   <span>$\max  \left( \frac{||\dot F(\psi_{j}^k) - \dot F(\psi_{j-1}^k)||_2}{      ||\psi_{j}^k - \psi_{j-1}^k||_2}, \hat L_{j-1}^k \right).$</span></p></li></ol><p><strong>Arguments</strong></p><ul><li><code>j::Int64</code>, inner loop iteration.</li><li><code>k::Int6</code>, outer loop iteration.</li><li><code>norm_djk::T</code>, norm of difference between <code>\psi_j^k</code> and <span>$\psi_{j-1}^k$</span>.   On the first iteration of the inner loop this will not matter.</li><li><code>curr_grad::S</code>, gradient at <span>$\psi_j^k$</span> (i.e., the current iterate).</li><li><code>prev_grad::S</code>, gradient at <span>$\psi_{j-1}^k$</span> (i.e., the previous iterate).</li><li><code>prev_approximation::T</code>, the local Lipschitz approximation from the previous    iteration</li><li><code>prev_acceptance::Bool</code>, whether or not the previous inner loop resulted in   an accepted iterate.</li></ul><p><strong>Return</strong></p><ul><li><code>estimate::T</code>, estimate of the local Lipschitz constant.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_non_sequential_armijo.jl#L144-L206">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.compute_step_size" href="#OptimizationMethods.compute_step_size"><code>OptimizationMethods.compute_step_size</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">compute_step_size(τ_lower::T, norm_grad::T, local_lipschitz_estimate::T
    ) where {T}</code></pre><p>Computes the step size for the inner loop iterates.</p><p><strong>Method</strong></p><p>The inner loop iterates are generated according to the formula</p><p class="math-container">\[    \psi_{j+1}^k = \psi_j^k - \delta_k\alpha_j^k \dot F(\psi_j^k).\]</p><p>The step size, <span>$\alpha_j^k$</span> is computed as</p><p class="math-container">\[    \alpha_j^k = 
    \min \left( 
    (\tau_{\mathrm{grad}, \mathrm{lower}}^k)^2 / C_{j,1}^k, 
    1 / C_{j,2}^k 
    \right)\]</p><p>where </p><p class="math-container">\[    C_{j,1}^k = ||\dot F(\psi_j^k)||_2^3 + .5 \hat{L}_j^k ||\dot F(\psi_j^k)||_2^2 + 1e-16\]</p><p>and</p><p class="math-container">\[    C_{j,2}^k = ||\dot F(\psi_j^k)||_2 + .5 \hat{L}_j^k + 1e-16.\]</p><p><strong>Arguments</strong></p><ul><li><code>τ_lower::T</code>, lower bound on the gradient. </li><li><code>norm_grad::T</code>, norm of the gradient at <span>$\psi_j^k$</span>.</li><li><code>local_lipschitz_estimate::T</code>, local lipschitz approximation at inner loop   iteration <code>j</code> and outer loop iteration <code>k</code>.</li></ul><p><strong>Return</strong></p><ul><li><code>αjk::T</code>, the step size.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_non_sequential_armijo.jl#L226-L272">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.inner_loop!" href="#OptimizationMethods.inner_loop!"><code>OptimizationMethods.inner_loop!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">inner_loop!(ψjk::S, θk::S, optData::NonsequentialArmijoGD{T}, 
    progData::P1 where P1 &lt;: AbstractNLPModel{T, S}, 
    precomp::P2 where P2 &lt;: AbstractPrecompute{T}, 
    store::P3 where P3 &lt;: AbstractProblemAllocate{T}, 
    past_acceptance::Bool, k::Int64; max_iteration = 100) where {T, S}</code></pre><p>Conduct the inner loop iteration, modifying <code>ψjk</code>, <code>optData</code>, and <code>store</code> in place. <code>ψjk</code> gets updated to be the terminal iterate of the inner loop; the fields <code>local_lipschitz_estimate</code>, <code>norm_∇F_ψ</code>, and <code>prev_∇F_ψ</code> are updated in <code>optData</code>; the fields <code>grad</code> in <code>store</code> gets updated to be the gradient at <code>ψjk</code>.</p><p><strong>Arguments</strong></p><ul><li><code>ψjk::S</code>, buffer array for the inner loop iterates.</li><li><code>θk::S</code>, starting iterate.</li><li><code>optData::NonsequentialArmijoGD{T}</code>, <code>struct</code> that specifies the optimization   algorithm. Fields are modified during the inner loop.</li><li><code>progData::P1 where P1 &lt;: AbstractNLPModel{T, S}</code>, <code>struct</code> that specifies the   optimization problem. Fields are modified during the inner loop.</li><li><code>precomp::P2 where P2 &lt;: AbstractPrecompute{T}</code>, <code>struct</code> that has precomputed   values. Required to take advantage of this during the gradient computation.</li><li><code>store::P3 where P3 &lt;: AbstractProblemAllocate{T}</code>, <code>struct</code> that contains   buffer arrays for computation.</li><li><code>past_acceptance::Bool</code>, flag indicating if the previous inner loop resulted   in a success (i.e., <span>$F(\theta_k) &lt; F(\theta_{k-1})$</span>).</li><li><code>k::Int64</code>, outer loop iteration for computation of the local Lipschitz   approximation scheme.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>max_iteration = 100</code>, maximum number of allowable iteration of the inner loop.   Should be kept at <code>100</code> as that is what is specified in the paper, but   is useful to change for testing.</li></ul><p><strong>Returns</strong></p><ul><li><code>j::Int64</code>, the iteration for which a triggering event evaluated to true.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_non_sequential_armijo.jl#L283-L321">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.update_algorithm_parameters!" href="#OptimizationMethods.update_algorithm_parameters!"><code>OptimizationMethods.update_algorithm_parameters!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">update_algorithm_parameters!(θkp1::S, optData::NonsequentialArmijoGD{T},
    achieved_descent::Bool, iter::Int64) where {T, S}</code></pre><p>Given that the non-sequential Armijo condition is checked, update the parameters     the optimization method. The method updates the following variables in place.</p><ul><li><code>θkp1</code> is updated to be the next outer loop iterate.</li><li><code>optData</code> has (potentially) the following fields updated: <code>δk</code>, <code>τ_lower</code>,   <code>τ_upper</code>.</li></ul><p><strong>Arguments</strong></p><ul><li><code>θkp1::S</code>, buffer array for the storage of the next iterate.</li><li><code>optData::NonsequentialArmijoGD{T}</code>, <code>struct</code> that specifies the optimization   algorithm. </li><li><code>achieved_descent::Bool</code>, boolean flag indicating whether or not the   descent condition was achieved.</li><li><code>iter::Int64</code>, the current iteration of the method. The outer loop iteration.   This is requried as it is used to overwrite <code>θkp1</code> with the previous iterate.</li></ul><p><strong>Returns</strong></p><ul><li>A boolean flag equal to <code>achieved_descent</code> to indicate whether <code>θkp1</code> is    modified in-place.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/gd_non_sequential_armijo.jl#L376-L401">source</a></section></article><h1 id="Line-search-Helper-Functions"><a class="docs-heading-anchor" href="#Line-search-Helper-Functions">Line search Helper Functions</a><a id="Line-search-Helper-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Line-search-Helper-Functions" title="Permalink"></a></h1><h2 id="Backtracking"><a class="docs-heading-anchor" href="#Backtracking">Backtracking</a><a id="Backtracking-1"></a><a class="docs-heading-anchor-permalink" href="#Backtracking" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.backtracking!" href="#OptimizationMethods.backtracking!"><code>OptimizationMethods.backtracking!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">backtracking!(θk::S, θkm1::S, F::Function, gkm1::S, step_direction::S,
    reference_value::T, α::T, δ::T, ρ::T; max_iteration::Int64 = 100
    ) where {T, S}</code></pre><p>Implementation of backtracking which modifies <code>θk</code> in place. This method     should be used for general step directions. If <code>gkm1</code> is the step direction     use the other <code>backtracking!(...)</code> method.</p><p><strong>Reference(s)</strong></p><p><a href="../references/#nocedal2006Numerical">Nocedal and Wright. &quot;Numerical Optimization&quot;.      Springer New York, NY.</a></p><p><strong>Method</strong></p><p>Let <span>$\theta_{k-1}$</span> be the current iterate, and let  <span>$\alpha \in \mathbb{R}_{&gt;0}$</span>, <span>$\delta \in (0, 1)$</span>, and <span>$\rho \in (0, 1)$</span>. Let <span>$d_k$</span> be the step direction, then <span>$\theta_k = \theta_{k-1} - \delta^t\alpha d_k$</span> where  <span>$t + 1 \in \mathbb{N}$</span> is the smallest such number satisfying</p><p class="math-container">\[    F(\theta_k) \leq O_{k-1} - \rho\delta^t\alpha
    \dot F(\theta_{k-1})^\intercal d_k,\]</p><p>where <span>$O_{k-1}$</span> is some reference value. </p><p><strong>Arguments</strong></p><ul><li><code>θk::S</code>, buffer array for the next iterate.</li><li><code>θkm1::S</code>, current iterate the optimization algorithm.</li><li><code>F::Function</code>, objective function. Should take in   a single argument and return the value of the    objective at the input value.</li><li><code>gkm1::S</code>, gradient value at <code>θkm1</code>.</li><li><code>step_direction::S</code>, direction to move <code>θkm1</code> to form <code>θk</code>.</li><li><code>reference_value::T</code>, value to check the objective function   at <code>θk</code> against.</li><li><code>α::T</code>, initial step size.</li><li><code>δ::T</code>, backtracking decrease factor.</li><li><code>ρ::T</code>, factor involved in the acceptance criterion in <code>θk</code>.   Larger values correspond to stricter descent conditions, and   smaller values correspond to looser descent conditions on <code>θk</code>.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>max_iteration::Int64 = 100</code>, the maximum allowable iterations   for the line search procedure. In the language above, when   <span>$t$</span> is equal to <code>max_iteration</code> the algorithm will terminate.</li></ul><p><strong>Return</strong></p><ul><li><code>backtracking_condition_satisfied::Bool</code>, whether the backtracking condition   is satisfied before the max iteration limit.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/line_search_helpers/backtracking.jl#L5-L61">source</a></section><section><div><pre><code class="language-julia hljs">backtracking!(θk::S, θkm1::S, F::Function, gkm1::S, norm_gkm1_squared::T,
    reference_value::T, α::T, δ::T, ρ::T; max_iteration::Int64 = 100)
    where {T, S}</code></pre><p>Implementation of backtracking which modifies <code>θk</code> in place. This method      assumes that the step direction is <code>-gkm1</code>.</p><p><strong>Reference(s)</strong></p><p><a href="../references/#armijo1966Minimization">Armijo, Larry. “Minimization of Functions Having Lipschitz Continuous      First Partial Derivatives.” Pacific Journal of Mathematics 16.1      (1966): 1–3. Pacific Journal of Mathematics. Web.</a></p><p><a href="../references/#nocedal2006Numerical">Nocedal and Wright. &quot;Numerical Optimization&quot;.      Springer New York, NY.</a></p><p><strong>Method</strong></p><p>Let <span>$\theta_{k-1}$</span> be the current iterate, and let  <span>$\alpha \in \mathbb{R}_{&gt;0}$</span>, <span>$\delta \in (0, 1)$</span>, and <span>$\rho \in (0, 1)$</span>. Let <span>$\dot F(\theta_{k-1})$</span>  be the step direction, then  <span>$\theta_k = \theta_{k-1} - \delta^t\alpha \dot F(\theta_{k-1})$</span> where  <span>$t + 1 \in \mathbb{N}$</span> is the smallest such number satisfying</p><p class="math-container">\[    F(\theta_k) \leq O_{k-1} - \rho\delta^t\alpha
    ||\dot F(\theta_{k-1})||_2^2,\]</p><p>where <span>$O_{k-1}$</span> is some reference value, and <span>$||\cdot||_2$</span> is the L2-norm. </p><p><strong>Arguments</strong></p><ul><li><code>θk::S</code>, buffer array for the next iterate.</li><li><code>θkm1::S</code>, current iterate the optimization algorithm.</li><li><code>F::Function</code>, objective function. Should take in   a single argument and return the value of the    objective at the input value.</li><li><code>gkm1::S</code>, gradient value at <code>θkm1</code>.</li><li><code>norm_gkm1_squared::T</code>, norm squared for the value of <code>gkm1</code>.</li><li><code>reference_value::T</code>, value to check the objective function   at <code>θk</code> against.</li><li><code>α::T</code>, initial step size.</li><li><code>δ::T</code>, backtracking decrease factor.</li><li><code>ρ::T</code>, factor involved in the acceptance criterion in <code>θk</code>.   Larger values correspond to stricter descent conditions, and   smaller values correspond to looser descent conditions on <code>θk</code>.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>max_iteration::Int64 = 100</code>, the maximum allowable iterations   for the line search procedure. In the language above, when   <span>$t$</span> is equal to <code>max_iteration</code> the algorithm will terminate.</li></ul><p><strong>Return</strong></p><ul><li><code>backtracking_condition_satisfied::Bool</code>, whether the backtracking condition   is satisfied before the max iteration limit.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/line_search_helpers/backtracking.jl#L95-L155">source</a></section></article><h2 id="Non-sequential-Armijo"><a class="docs-heading-anchor" href="#Non-sequential-Armijo">Non-sequential Armijo</a><a id="Non-sequential-Armijo-1"></a><a class="docs-heading-anchor-permalink" href="#Non-sequential-Armijo" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.non_sequential_armijo_condition" href="#OptimizationMethods.non_sequential_armijo_condition"><code>OptimizationMethods.non_sequential_armijo_condition</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">non_sequential_armijo_condition(F_ψjk::T, reference_value::T,
    norm_grad_θk::T, ρ::T, δk::T, α0k::T) where {T}</code></pre><p>Check if <code>F_ψjk</code> satisfies the non-sequential armijo condition with respect to <code>reference_value</code> and the remaining parameters. Returns a boolean value indicating if the descent condition is satisfied or not.</p><p><strong>Method</strong></p><p>Let <span>$F(\theta) : \mathbb{R}^n \to \mathbb{R}$</span> be the objective function. Suppose that <span>$\theta_k \in \mathbb{R}^n$</span> is an iterate of an optimization routine. Let <span>$\psi_0^k = \theta_k$</span> and for <span>$j \in \lbrace 1,...,T \rbrace$</span> for some <span>$T \in \mathbb{N}$</span>, recursively define</p><p class="math-container">\[    \psi_j^k = \psi_0^k - \sum_{t = 0}^{j-1} \delta_k \alpha_t^k \dot F(\psi_t^k).\]</p><p>Then, the (monotone) non-sequential armijo condition requires that</p><p class="math-container">\[    F(\psi_j^k) &lt; F(\psi_0^k) - \rho \delta_k \alpha_0^k ||\dot F(\psi_0^k)||_2^2,\]</p><p>where <span>$||\cdot||_2$</span> is the L2-norm and <span>$\rho \in (0, 1)$</span>.</p><p>This function implements checking the inequality, where <code>F_ψjk</code> corresponds to     <span>$F(\psi_j^k)$</span>, <code>reference_value</code> corresponds to <span>$F(\psi_0^k)$</span>,     <code>norm_grad_θk</code> to <span>$||\dot F(\psi_0^k)||_2$</span>, <code>ρ</code> to <span>$\rho$</span>,      <code>δk</code> to <span>$\delta_k$</span>, and <code>α0k</code> to <span>$\alpha_0^k$</span>. To see more      about how this method is used read the documentation for      <a href="#OptimizationMethods.NonsequentialArmijoGD">gradient descent with non-sequential armijo</a></p><p><strong>Arguments</strong></p><ul><li><code>F_ψjk::T</code>, numeric value on the LHS of the inequality. In optimization context,   this is the objective value of a trial iterate to check if sufficient descent   is achieved.</li><li><code>reference_value::T</code>, numeric value on the RHS of the inequality. In the   optimization context, the value of the current iterate <code>F_ψjk</code> must be   smaller than this to guarantee a sufficient descent criterion.</li><li><code>norm_grad_θk::T</code>, numeric value forming the amount of descent that needs   to be achieved. This value is usually the norm of the gradient of a    previous iterate.</li><li><code>ρ::T</code>, parameter in the line search criterion dictating how much descent   should be required. Should be positive. Larger values indicate stricter   conditions, and lower value indicate looser conditions.</li><li><code>δk::T</code>, numeric value that corresponds to a scaling factor for the step size.</li><li><code>α0k::T</code>, numeric value. In the context of    <a href="#OptimizationMethods.NonsequentialArmijoGD">non-sequential armijo gradient descent</a>   this is the first step size used in an inner loop.</li></ul><p><strong>Return</strong></p><ul><li><code>flag::Bool</code>, <code>true</code> if the descent condition is satisfied, and <code>false</code>   if the descent condition is not satisfied.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/dd87bafd4b72635f0248f391fc537bfe844f809c/src/methods/line_search_helpers/non_sequential_armijo.jl#L5-L61">source</a></section></article><h1 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h1><ul><li><a href="#OptimizationMethods.BacktrackingGD"><code>OptimizationMethods.BacktrackingGD</code></a></li><li><a href="#OptimizationMethods.BarzilaiBorweinGD"><code>OptimizationMethods.BarzilaiBorweinGD</code></a></li><li><a href="#OptimizationMethods.DiminishingStepGD"><code>OptimizationMethods.DiminishingStepGD</code></a></li><li><a href="#OptimizationMethods.FixedStepGD"><code>OptimizationMethods.FixedStepGD</code></a></li><li><a href="../api_problems/#OptimizationMethods.LeastSquares"><code>OptimizationMethods.LeastSquares</code></a></li><li><a href="#OptimizationMethods.LipschitzApproxGD"><code>OptimizationMethods.LipschitzApproxGD</code></a></li><li><a href="../api_problems/#OptimizationMethods.LogisticRegression"><code>OptimizationMethods.LogisticRegression</code></a></li><li><a href="#OptimizationMethods.NesterovAcceleratedGD"><code>OptimizationMethods.NesterovAcceleratedGD</code></a></li><li><a href="#OptimizationMethods.NonsequentialArmijoGD"><code>OptimizationMethods.NonsequentialArmijoGD</code></a></li><li><a href="../api_problems/#OptimizationMethods.PoissonRegression"><code>OptimizationMethods.PoissonRegression</code></a></li><li><a href="../api_problems/#OptimizationMethods.QLLogisticCenteredExp"><code>OptimizationMethods.QLLogisticCenteredExp</code></a></li><li><a href="../api_problems/#OptimizationMethods.QLLogisticCenteredLog"><code>OptimizationMethods.QLLogisticCenteredLog</code></a></li><li><a href="../api_problems/#OptimizationMethods.QLLogisticMonomial"><code>OptimizationMethods.QLLogisticMonomial</code></a></li><li><a href="../api_problems/#OptimizationMethods.QLLogisticSin"><code>OptimizationMethods.QLLogisticSin</code></a></li><li><a href="#OptimizationMethods.WeightedNormDampingGD"><code>OptimizationMethods.WeightedNormDampingGD</code></a></li><li><a href="#OptimizationMethods.backtracking!"><code>OptimizationMethods.backtracking!</code></a></li><li><a href="#OptimizationMethods.backtracking_gd"><code>OptimizationMethods.backtracking_gd</code></a></li><li><a href="#OptimizationMethods.barzilai_borwein_gd"><code>OptimizationMethods.barzilai_borwein_gd</code></a></li><li><a href="../api_problems/#OptimizationMethods.centered_exp"><code>OptimizationMethods.centered_exp</code></a></li><li><a href="../api_problems/#OptimizationMethods.centered_shifted_log"><code>OptimizationMethods.centered_shifted_log</code></a></li><li><a href="#OptimizationMethods.compute_step_size"><code>OptimizationMethods.compute_step_size</code></a></li><li><a href="../api_problems/#OptimizationMethods.dcentered_exp"><code>OptimizationMethods.dcentered_exp</code></a></li><li><a href="../api_problems/#OptimizationMethods.dcentered_shifted_log"><code>OptimizationMethods.dcentered_shifted_log</code></a></li><li><a href="../api_problems/#OptimizationMethods.ddlogistic"><code>OptimizationMethods.ddlogistic</code></a></li><li><a href="#OptimizationMethods.diminishing_step_gd"><code>OptimizationMethods.diminishing_step_gd</code></a></li><li><a href="../api_problems/#OptimizationMethods.dlinear_plus_sin"><code>OptimizationMethods.dlinear_plus_sin</code></a></li><li><a href="../api_problems/#OptimizationMethods.dlogistic"><code>OptimizationMethods.dlogistic</code></a></li><li><a href="../api_problems/#OptimizationMethods.dmonomial_plus_constant"><code>OptimizationMethods.dmonomial_plus_constant</code></a></li><li><a href="#OptimizationMethods.fixed_step_gd"><code>OptimizationMethods.fixed_step_gd</code></a></li><li><a href="#OptimizationMethods.inner_loop!"><code>OptimizationMethods.inner_loop!</code></a></li><li><a href="../api_problems/#OptimizationMethods.inverse_complimentary_log_log"><code>OptimizationMethods.inverse_complimentary_log_log</code></a></li><li><a href="#OptimizationMethods.inverse_k_step_size"><code>OptimizationMethods.inverse_k_step_size</code></a></li><li><a href="#OptimizationMethods.inverse_log2k_step_size"><code>OptimizationMethods.inverse_log2k_step_size</code></a></li><li><a href="../api_problems/#OptimizationMethods.inverse_probit"><code>OptimizationMethods.inverse_probit</code></a></li><li><a href="../api_problems/#OptimizationMethods.linear_plus_sin"><code>OptimizationMethods.linear_plus_sin</code></a></li><li><a href="#OptimizationMethods.lipschitz_approximation_gd"><code>OptimizationMethods.lipschitz_approximation_gd</code></a></li><li><a href="../api_problems/#OptimizationMethods.logistic"><code>OptimizationMethods.logistic</code></a></li><li><a href="../api_problems/#OptimizationMethods.monomial_plus_constant"><code>OptimizationMethods.monomial_plus_constant</code></a></li><li><a href="#OptimizationMethods.nesterov_accelerated_gd"><code>OptimizationMethods.nesterov_accelerated_gd</code></a></li><li><a href="#OptimizationMethods.non_sequential_armijo_condition"><code>OptimizationMethods.non_sequential_armijo_condition</code></a></li><li><a href="#OptimizationMethods.nonsequential_armijo_gd"><code>OptimizationMethods.nonsequential_armijo_gd</code></a></li><li><a href="#OptimizationMethods.stepdown_100_step_size"><code>OptimizationMethods.stepdown_100_step_size</code></a></li><li><a href="#OptimizationMethods.update_algorithm_parameters!"><code>OptimizationMethods.update_algorithm_parameters!</code></a></li><li><a href="#OptimizationMethods.update_local_lipschitz_approximation"><code>OptimizationMethods.update_local_lipschitz_approximation</code></a></li><li><a href="#OptimizationMethods.weighted_norm_damping_gd"><code>OptimizationMethods.weighted_norm_damping_gd</code></a></li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../api_problems/">« Problems</a><a class="docs-footer-nextpage" href="../references/">References »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.10.2 on <span class="colophon-date" title="Tuesday 29 April 2025 19:32">Tuesday 29 April 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
