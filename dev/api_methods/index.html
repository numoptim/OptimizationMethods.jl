<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Methods · OptimizationMethods.jl Documentation</title><meta name="title" content="Methods · OptimizationMethods.jl Documentation"/><meta property="og:title" content="Methods · OptimizationMethods.jl Documentation"/><meta property="twitter:title" content="Methods · OptimizationMethods.jl Documentation"/><meta name="description" content="Documentation for OptimizationMethods.jl Documentation."/><meta property="og:description" content="Documentation for OptimizationMethods.jl Documentation."/><meta property="twitter:description" content="Documentation for OptimizationMethods.jl Documentation."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">OptimizationMethods.jl Documentation</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Manual</span><ul><li><input class="collapse-toggle" id="menuitem-2-1" type="checkbox"/><label class="tocitem" for="menuitem-2-1"><span class="docs-label">Problems</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../problems/quasilikelihood_estimation/">Quasi-likelihood Estimation</a></li></ul></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../api_problems/">Problems</a></li><li class="is-active"><a class="tocitem" href>Methods</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Barzilai-Borwein-Method"><span>Barzilai Borwein Method</span></a></li><li class="toplevel"><a class="tocitem" href="#Gradient-Descent-with-Fixed-Step-Size"><span>Gradient Descent with Fixed Step Size</span></a></li><li class="toplevel"><a class="tocitem" href="#Lipschitz-Approximation-(Malitsky-and-Mishchenko)"><span>Lipschitz Approximation (Malitsky &amp; Mishchenko)</span></a></li><li class="toplevel"><a class="tocitem" href="#Weighted-Norm-Damping-Gradient-Method-(WNGrad)"><span>Weighted Norm Damping Gradient Method (WNGrad)</span></a></li><li class="toplevel"><a class="tocitem" href="#Nesterov&#39;s-Accelerated-Gradient-Descent"><span>Nesterov&#39;s Accelerated Gradient Descent</span></a></li><li class="toplevel"><a class="tocitem" href="#Gradient-Descent-with-Diminishing-Step-Size"><span>Gradient Descent with Diminishing Step Size</span></a></li><li class="toplevel"><a class="tocitem" href="#Index"><span>Index</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API</a></li><li class="is-active"><a href>Methods</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Methods</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/numoptim/OptimizationMethods.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/numoptim/OptimizationMethods.jl/blob/main/docs/src/api_methods.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Contents"><a class="docs-heading-anchor" href="#Contents">Contents</a><a id="Contents-1"></a><a class="docs-heading-anchor-permalink" href="#Contents" title="Permalink"></a></h1><ul><li><a href="#Contents">Contents</a></li><li><a href="#Barzilai-Borwein-Method">Barzilai Borwein Method</a></li><li><a href="#Gradient-Descent-with-Fixed-Step-Size">Gradient Descent with Fixed Step Size</a></li><li><a href="#Lipschitz-Approximation-(Malitsky-and-Mishchenko)">Lipschitz Approximation (Malitsky &amp; Mishchenko)</a></li><li><a href="#Weighted-Norm-Damping-Gradient-Method-(WNGrad)">Weighted Norm Damping Gradient Method (WNGrad)</a></li><li><a href="#Nesterov&#39;s-Accelerated-Gradient-Descent">Nesterov&#39;s Accelerated Gradient Descent</a></li><li><a href="#Gradient-Descent-with-Diminishing-Step-Size">Gradient Descent with Diminishing Step Size</a></li><li><a href="#Index">Index</a></li></ul><h1 id="Barzilai-Borwein-Method"><a class="docs-heading-anchor" href="#Barzilai-Borwein-Method">Barzilai Borwein Method</a><a id="Barzilai-Borwein-Method-1"></a><a class="docs-heading-anchor-permalink" href="#Barzilai-Borwein-Method" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.barzilai_borwein_gd" href="#OptimizationMethods.barzilai_borwein_gd"><code>OptimizationMethods.barzilai_borwein_gd</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">barzilai_borwein_gd(optData::BarzilaiBorweinGD{T}, progData::P 
    where P &lt;: AbstractNLPModel{T, S}) where {T,S}</code></pre><p>Implements gradient descent with Barzilai-Borwein step size and applies the      method to the optimization problem specified by <code>progData</code>. </p><p><strong>Reference(s)</strong></p><p><a href="../references/#barzilai1988Twopoint">Barzilai and Borwein. &quot;Two-Point Step Size Gradient Methods&quot;. IMA Journal of      Numerical Analysis.</a></p><p><strong>Method</strong></p><p>Given iterates <span>$\lbrace x_0,\ldots,x_k\rbrace$</span>, the iterate <span>$x_{k+1}$</span>     is equal to <span>$x_k - \alpha_k \nabla f(x_k)$</span>, where <span>$\alpha_k$</span> is     one of two versions.</p><p><strong>Long Step Size Version (if <code>optData.long_stepsize==true</code>)</strong></p><p>If <span>$k=0$</span>, then <span>$\alpha_0$</span> is set to <code>optData.init_stepsize</code>. For <span>$k&gt;0$</span>,</p><p class="math-container">\[\alpha_k = \frac{ \Vert x_k - x_{k-1} \Vert_2^2}{(x_k - x_{k-1})^\intercal 
    (\nabla f(x_k) - \nabla f(x_{k-1}))}.\]</p><p><strong>Short Step Size Version (if <code>optData.long_stepsize==false</code>)</strong></p><p>If <span>$k=0$</span>, then <span>$\alpha_0$</span> is set to <code>optData.init_stepsize</code>. For <span>$k&gt;0$</span>,</p><p class="math-container">\[\alpha_k = \frac{(x_k - x_{k-1})^\intercal (\nabla f(x_k) - 
    \nabla f(x_{k-1}))}{\Vert \nabla f(x_k) - \nabla f(x_{k-1})\Vert_2^2}.\]</p><p><strong>Arguments</strong></p><ul><li><code>optData::BarzilaiBorweinGD{T}</code>, the specification for the optimization method.</li><li><code>progData&lt;:AbstractNLPModel{T,S}</code>, the specification for the optimization   problem. </li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>progData</code> must have an <code>initialize</code> function that returns subtypes of <code>AbstractPrecompute</code> and <code>AbstractProblemAllocate</code>, where the latter has a <code>grad</code> argument.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/0eadc1e7a54bf959b94f010bd15456fea616879d/src/methods/gd_barzilai_borwein.jl#L116-L163">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.BarzilaiBorweinGD" href="#OptimizationMethods.BarzilaiBorweinGD"><code>OptimizationMethods.BarzilaiBorweinGD</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">BarzilaiBorweinGD{T} &lt;: AbstractOptimizerData{T}</code></pre><p>A structure for storing data about gradient descent using the Barzilai-Borwein      step size, and the progress of its application on an optimization problem.</p><p><strong>Fields</strong></p><ul><li><code>name:String</code>, name of the solver for reference.</li><li><code>init_stepsize::T</code>, initial step size to start the method. </li><li><code>long_stepsize::Bool</code>, flag for step size; if true, use the long version of    Barzilai-Borwein. If false, use the short version. </li><li><code>threshold::T</code>, gradient threshold. If the norm gradient is below this, then    iteration stops.</li><li><code>max_iterations::Int64</code>, max number of iterations (gradient steps) taken by    the solver.</li><li><code>iter_diff::Vector{T}</code>, a buffer for storing differences between subsequent   iterate values that are used for computing the step size</li><li><code>grad_diff::Vector{T}</code>, a buffer for storing differences between gradient    values at adjacent iterates, which is used to compute the step size</li><li><code>iter_hist::Vector{Vector{T}}</code>, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration <code>0</code>). The <code>k+1</code> entry   corresponds to the iterate at iteration <code>k</code>.</li><li><code>grad_val_hist::Vector{T}</code>, a vector for storing <code>max_iterations+1</code> gradient   norm values. The first entry corresponds to iteration <code>0</code>. The <code>k+1</code> entry   correpsonds to the gradient norm at iteration <code>k</code>.</li><li><code>stop_iteration::Int64</code>, the iteration number that the solver stopped on.   The terminal iterate is saved at <code>iter_hist[stop_iteration+1]</code>.</li></ul><p><strong>Constructors</strong></p><pre><code class="nohighlight hljs">BarzilaiBorweinGD(::Type{T}; x0::Vector{T}, init_stepsize::T, 
    long_stepsize::Bool, threshold::T, max_iterations::Int) where {T}</code></pre><p>Constructs the <code>struct</code> for the Barzilai-Borwein optimization method</p><p><strong>Arguments</strong></p><ul><li><code>T::DataType</code>, specific data type used for calculations.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>x0::Vector{T}</code>, initial point to start the solver at.</li><li><code>init_stepsize::T</code>, initial step size used for the first iteration. </li><li><code>long_stepsize::Bool</code>, flag for step size; if true, use the long version of   Barzilai-Borwein, if false, use the short version. </li><li><code>threshold::T</code>, gradient threshold. If the norm gradient is below this,    then iteration is terminated. </li><li><code>max_iterations::Int</code>, max number of iterations (gradient steps) taken by    the solver.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/0eadc1e7a54bf959b94f010bd15456fea616879d/src/methods/gd_barzilai_borwein.jl#L3-L54">source</a></section></article><h1 id="Gradient-Descent-with-Fixed-Step-Size"><a class="docs-heading-anchor" href="#Gradient-Descent-with-Fixed-Step-Size">Gradient Descent with Fixed Step Size</a><a id="Gradient-Descent-with-Fixed-Step-Size-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-Descent-with-Fixed-Step-Size" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.fixed_step_gd" href="#OptimizationMethods.fixed_step_gd"><code>OptimizationMethods.fixed_step_gd</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">fixed_step_gd(optData::FixedStepGD{T}, progData&lt;:AbstractNLPModel{T,S})
    where {T,S}</code></pre><p>Implements fixed step-size gradient descent for the desired optimization problem     specified by <code>progData</code>.</p><p><strong>Method</strong></p><p>The iterates are updated according to the procedure</p><p class="math-container">\[x_{k+1} = x_k - \alpha ∇f(x_k),\]</p><p>where <span>$\alpha$</span> is the step size, <span>$f$</span> is the objective function, and <span>$∇f$</span> is the      gradient function of <span>$f$</span>. </p><p><strong>Arguments</strong></p><ul><li><code>optData::FixedStepGD{T}</code>, the specification for the optimization method.</li><li><code>progData&lt;:AbstractNLPModel{T,S}</code>, the specification for the optimization   problem. </li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>progData</code> must have an <code>initialize</code> function that returns subtypes of <code>AbstractPrecompute</code> and <code>AbstractProblemAllocate</code>, where the latter has a <code>grad</code> argument. </p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/0eadc1e7a54bf959b94f010bd15456fea616879d/src/methods/gd_fixed.jl#L72-L100">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.FixedStepGD" href="#OptimizationMethods.FixedStepGD"><code>OptimizationMethods.FixedStepGD</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FixedStepGD{T} &lt;: AbstractOptimizerData{T}</code></pre><p>A structure for storing data about fixed step-size gradient descent, and the     progress of its application on an optimization problem.</p><p><strong>Fields</strong></p><ul><li><code>name::String</code>, name of the solver for reference</li><li><code>step_size::T</code>, the step-size selection for the optimization procedure</li><li><code>threshold::T</code>, the threshold on the norm of the gradient to induce stopping</li><li><code>max_iterations::Int</code>, the maximum allowed iterations</li><li><code>iter_hist::Vector{Vector{T}}</code>, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration <code>0</code>). The <code>k+1</code> entry   corresponds to the iterate at iteration <code>k</code>.</li><li><code>grad_val_hist::Vector{T}</code>, a vector for storing <code>max_iterations+1</code> gradient   norm values. The first entry corresponds to iteration <code>0</code>. The <code>k+1</code> entry   correpsonds to the gradient norm at iteration <code>k</code></li><li><code>stop_iteration</code>, iteration number that the algorithm stopped at.   Iterate number <code>stop_iteration</code> is produced. </li></ul><p><strong>Constructors</strong></p><pre><code class="nohighlight hljs">FixedStepGD(::Type{T}; x0::Vector{T}, step_size::T, threshold::T, 
    max_iterations::Int) where {T}</code></pre><p>Constructs the <code>struct</code> for the optimizer.</p><p><strong>Arguments</strong></p><ul><li><code>T::DataType</code>, specific data type for the calculations</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>x0::Vector{T}</code>, the initial iterate for the optimizers</li><li><code>step_size::T</code>, the step size of the optimizer </li><li><code>threshold::T</code>, the threshold on the norm of the gradient to induce stopping</li><li><code>max_iterations::Int</code>, the maximum number of iterations allowed  </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/0eadc1e7a54bf959b94f010bd15456fea616879d/src/methods/gd_fixed.jl#L3-L41">source</a></section></article><h1 id="Lipschitz-Approximation-(Malitsky-and-Mishchenko)"><a class="docs-heading-anchor" href="#Lipschitz-Approximation-(Malitsky-and-Mishchenko)">Lipschitz Approximation (Malitsky &amp; Mishchenko)</a><a id="Lipschitz-Approximation-(Malitsky-and-Mishchenko)-1"></a><a class="docs-heading-anchor-permalink" href="#Lipschitz-Approximation-(Malitsky-and-Mishchenko)" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.lipschitz_approximation_gd" href="#OptimizationMethods.lipschitz_approximation_gd"><code>OptimizationMethods.lipschitz_approximation_gd</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">lipschitz_approximation_gd(optData::FixedStepGD{T}, progData::P where P 
    &lt;: AbstractNLPModel{T, S}) where {T, S}</code></pre><p>Implements gradient descent with adaptive step sizes formed through a lipschitz      approximation for the desired optimization problem specified by <code>progData</code>.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>This method is designed for convex optimization problems.</p></div></div><p><strong>References(s)</strong></p><p><a href="../references/#malitsky2020Adaptive">Malitsky, Y. and Mishchenko, K. (2020). &quot;Adaptive Gradient Descent without      Descent.&quot;      Proceedings of the 37th International Conference on Machine Learning,      in Proceedings of Machine Learning Research 119:6702-6712.     </a></p><p><strong>Method</strong></p><p>The iterates are updated according to the procedure,</p><p class="math-container">\[x_{k+1} = x_{k} - \alpha_k \nabla f(x_{k}),\]</p><p>where <span>$\alpha_k$</span> is the step size and <span>$\nabla f$</span> is the gradient function      of the objective function <span>$f$</span>.</p><p>The step size is computed depending on <span>$k$</span>.      When <span>$k = 0$</span>, the step size is set to <code>optData.init_stepsize</code>.      When <span>$k &gt; 0$</span>, </p><p class="math-container">\[\alpha_k = \min\left\lbrace \sqrt{1 + \theta_{k-1}}\alpha_{k-1}, 
    \frac{\Vert x_k - x_{k-1} \Vert}{\Vert \nabla f(x_k) - 
    \nabla f(x_{k-1})\Vert} \right\rbrace,\]</p><p>where <span>$\theta_0 = \inf$</span> and <span>$\theta_k = \alpha_k / \alpha_{k-1}$</span>.</p><p><strong>Arguments</strong></p><ul><li><code>optData::LipschitzApproxGD{T}</code>, the specification for the optimization method.</li><li><code>progData&lt;:AbstractNLPModel{T,S}</code>, the specification for the optimization   problem. </li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>progData</code> must have an <code>initialize</code> function that returns subtypes of <code>AbstractPrecompute</code> and <code>AbstractProblemAllocate</code>, where the latter has a <code>grad</code> argument. </p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/0eadc1e7a54bf959b94f010bd15456fea616879d/src/methods/gd_lipschitz_approximation.jl#L98-L150">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.LipschitzApproxGD" href="#OptimizationMethods.LipschitzApproxGD"><code>OptimizationMethods.LipschitzApproxGD</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LipschitzApproxGD{T} &lt;: AbstractOptimizerData{T}</code></pre><p>A structure for storing data about adaptive gradient descent     using a Lipschitz Approximation scheme (AdGD), and the progress      of its application on an optimization problem.</p><p><strong>Fields</strong></p><ul><li><code>name::String</code>, name of the solver for reference</li><li><code>init_stepsize::T</code>, the initial step size for the method</li><li><code>prev_stepsize::T</code>, step size used at <code>iter - 1</code> when <code>iter &gt; 1</code>.</li><li><code>theta::T</code>, element used in the computation of the step size. See the    referenced paper for more information.</li><li><code>lipschitz_approximation::T</code>, help the lipschitz approximation used in the   computation of the step size. See the referenced paper for more information.</li><li><code>threshold::T</code>, the threshold on the norm of the gradient to induce stopping</li><li><code>max_iterations::Int64</code>, the maximum allowed iterations</li><li><code>iter_diff::Vector{T}</code>, a buffer for storing differences between subsequent   iterate values that are used for computing the step size</li><li><code>grad_diff::Vector{T}</code>, a buffer for storing differences between gradient    values at adjacent iterates, which is used to compute the step size</li><li><code>iter_hist::Vector{Vector{T}}</code>, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration <code>0</code>). The <code>k+1</code> entry   corresponds to the iterate at iteration <code>k</code>.</li><li><code>grad_val_hist::Vector{T}</code>, a vector for storing <code>max_iterations+1</code> gradient   norm values. The first entry corresponds to iteration <code>0</code>. The <code>k+1</code> entry   corresponds to the gradient norm at iteration <code>k</code></li><li><code>stop_iteration::Int64</code>, iteration number that the algorithm stopped at.   Iterate number <code>stop_iteration</code> is produced. </li></ul><p><strong>Constructors</strong></p><pre><code class="nohighlight hljs">LipschitzApproxGD(::Type{T}; x0::Vector{T}, init_stepsize::T, threshold::T, 
    max_iterations::Int) where {T}</code></pre><p>Constructs the <code>struct</code> for the optimizer.</p><p><strong>Arguments</strong></p><ul><li><code>T::DataType</code>, specific data type for the calculations</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>x0::Vector{T}</code>, the initial iterate for the optimizers</li><li><code>init_stepsize::T</code>, the initial step size for the method</li><li><code>threshold::T</code>, the threshold on the norm of the gradient to induce stopping</li><li><code>max_iterations::Int</code>, the maximum number of iterations allowed  </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/0eadc1e7a54bf959b94f010bd15456fea616879d/src/methods/gd_lipschitz_approximation.jl#L5-L53">source</a></section></article><h1 id="Weighted-Norm-Damping-Gradient-Method-(WNGrad)"><a class="docs-heading-anchor" href="#Weighted-Norm-Damping-Gradient-Method-(WNGrad)">Weighted Norm Damping Gradient Method (WNGrad)</a><a id="Weighted-Norm-Damping-Gradient-Method-(WNGrad)-1"></a><a class="docs-heading-anchor-permalink" href="#Weighted-Norm-Damping-Gradient-Method-(WNGrad)" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.weighted_norm_damping_gd" href="#OptimizationMethods.weighted_norm_damping_gd"><code>OptimizationMethods.weighted_norm_damping_gd</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">weighted_norm_damping_gd(optData::WeightedNormDampingGD{T}, 
    progData::P where P &lt;: AbstractNLPModel{T, S}) where {T, S}</code></pre><p>Method that implements gradient descent with weighted norm damping step size      using the specifications in <code>optData</code> on the problem specified by <code>progData</code>.</p><p><strong>Reference</strong></p><p>Wu, Xiaoxia et. al. &quot;WNGrad: Learn the Learning Rate in Gradient Descent&quot;. arxiv,      https://arxiv.org/abs/1803.02865</p><p><strong>Method</strong></p><p>Let <span>$\theta_k$</span> be the <span>$k^{th}$</span> iterate, and <span>$\alpha_k$</span> be the <span>$k^{th}$</span>      step size. The optimization method generate iterates following</p><p class="math-container">\[\theta_{k + 1} = \theta_{k} - \alpha_k \nabla f(\theta_k),\]</p><p>where <span>$\nabla f$</span> is the gradient of the objective function <span>$f$</span>.</p><p>The step size depends on the iteration number <span>$k$</span>. For <span>$k = 0$</span>, the step      size <span>$\alpha_0$</span> is the reciprocal of <code>optData.init_norm_damping_factor</code>.      For <span>$k &gt; 0$</span>, the step size is iteratively updated as</p><p class="math-container">\[\alpha_k = \left[
\frac{1}{\alpha_{k-1}} + \Vert\dot F(\theta_k)\Vert_2^2 \alpha_{k-1}
\right]^{-1}.\]</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>When <span>$\alpha_0 &lt; \Vert\dot F(\theta_0)\Vert_2^{-1}$</span> and a globally Lipschitz smooth objective function is used, then the method is guaranteed to find an <span>$\epsilon$</span>-stationary point. It is recommended then that  <code>optData.init_norm_damping_factor</code> exceed <span>$\Vert\dot F(\theta_0)\Vert_2$</span>.</p></div></div><p><strong>Arguments</strong></p><ul><li><code>optData::WeightedNormDampingGD{T}</code>, specification for the optimization algorithm.</li><li><code>progData::P where P &lt;: AbstractNLPModel{T, S}</code>, specification for the problem.</li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>progData</code> must have an <code>initialize</code> function that returns subtypes of <code>AbstractPrecompute</code> and <code>AbstractProblemAllocate</code>, where the latter has a <code>grad</code> argument. </p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/0eadc1e7a54bf959b94f010bd15456fea616879d/src/methods/gd_weighted_norm_damping.jl#L96-L143">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.WeightedNormDampingGD" href="#OptimizationMethods.WeightedNormDampingGD"><code>OptimizationMethods.WeightedNormDampingGD</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WeightedNormDampingGD{T} &lt;: AbstractOptimizerData{T}</code></pre><p>A mutable struct that represents gradient descent using the weighted-norm      damping step size. It stores the specification for the method and records      values during iteration.</p><p><strong>Fields</strong></p><ul><li><code>name::String</code>, name of the optimizer for recording purposes</li><li><code>init_norm_damping_factor::T</code>, initial damping factor. This value&#39;s reciprocal    will be the initial step size.</li><li><code>threshold::T</code>, norm gradient tolerance condition. Induces stopping when norm    is at most <code>threshold</code>.</li><li><code>max_iterations::Int64</code>, max number of iterates that are produced, not    including the initial iterate.</li><li><code>iter_hist::Vector{Vector{T}}</code>, store the iterate sequence as the algorithm    progresses. The initial iterate is stored in the first position.</li><li><code>grad_val_hist::Vector{T}</code>, stores the norm gradient values at each iterate.    The norm of the gradient evaluated at the initial iterate is stored in the    first position.</li><li><code>stop_iteration::Int64</code>, the iteration number the algorithm stopped on. The    iterate that induced stopping is saved at <code>iter_hist[stop_iteration + 1]</code>.</li></ul><p><strong>Constructors</strong></p><pre><code class="nohighlight hljs">WeightedNormDampingGD(::Type{T}; x0::Vector{T}, init_norm_damping_factor::T, 
    threshold::T, max_iterations::Int64) where {T}</code></pre><p>Constructs an instance of type <code>WeightedNormDampingGD{T}</code>.</p><p><strong>Arguments</strong></p><ul><li><code>T::DataType</code>, type for data and computation</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>x0::Vector{T}</code>, initial point to start the optimization routine. Saved in   <code>iter_hist[1]</code>.</li><li><code>init_norm_damping_factor::T</code>, initial damping factor, which will correspond   to the reciprocoal of the initial step size. </li><li><code>threshold::T</code>, norm gradient tolerance condition. Induces stopping when norm    at most <code>threshold</code>.</li><li><code>max_iterations::Int64</code>, max number of iterates that are produced, not    including the initial iterate.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/0eadc1e7a54bf959b94f010bd15456fea616879d/src/methods/gd_weighted_norm_damping.jl#L5-L50">source</a></section></article><h1 id="Nesterov&#39;s-Accelerated-Gradient-Descent"><a class="docs-heading-anchor" href="#Nesterov&#39;s-Accelerated-Gradient-Descent">Nesterov&#39;s Accelerated Gradient Descent</a><a id="Nesterov&#39;s-Accelerated-Gradient-Descent-1"></a><a class="docs-heading-anchor-permalink" href="#Nesterov&#39;s-Accelerated-Gradient-Descent" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.nesterov_accelerated_gd" href="#OptimizationMethods.nesterov_accelerated_gd"><code>OptimizationMethods.nesterov_accelerated_gd</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">nesterov_accelerated_gd(optData::NesterovAcceleratedGD{T}, progData::P
    where P &lt;: AbstractNLPModel{T, S}) where {T, S}</code></pre><p>Implements Nesterov&#39;s Accelerated Gradient Descent as specified by <code>optData</code> on the problem specified by <code>progData</code>.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>This algorithm is designed for convex problems.</p></div></div><p><strong>Reference(s)</strong></p><ul><li>See Algorithm 1 of <a href="../references/#li2023Convex">Li et. al. &quot;Convex and Non-convex Optimization Under    Generalized Smoothness&quot;. arxiv, https://arxiv.org/abs/2306.01264.</a></li><li>See line-search based approach in <a href="../references/#nesterov1983Method">Nesterov, Yurii. 1983. “A Method for    Solving the Convex Programming Problem    with Convergence Rate O(1/K^2).” Proceedings of the USSR Academy of Sciences    269:543–47.</a></li></ul><p><strong>Method</strong></p><p>Let the objective function be denoted by <span>$F(\theta)$</span> and <span>$\nabla F(     \theta)$</span> denote its gradient function. Given <span>$\theta_0$</span> and  a step      size <span>$\alpha$</span> (equal to <code>optData.step_size</code>), the method produces five      sequences. At <span>$k=0$</span>,</p><p class="math-container">\[\begin{cases}
    B_0 &amp;= 0 \\
    z_0 &amp;= \theta_0 \\
    \Delta_0 &amp; = 1 \\
    y_0 &amp;= \theta_0;
\end{cases}\]</p><p>and, for <span>$k\in\mathbb{N}$</span>,</p><p class="math-container">\[\begin{cases}
    B_{k} &amp;= B_{k-1} + \Delta_{k-1} \\
    \theta_k &amp;= y_{k-1} - \alpha \nabla F(y_{k-1}) \\
    z_k &amp;= z_{k-1} - \alpha\Delta_{k-1}\nabla F(y_{k-1}) \\
    \Delta_k &amp;= \frac{1}{2}\left( 1 + \sqrt{4 B_{k} + 1}  \right) \\
    y_k &amp;= \theta_k + \frac{\Delta_{k}}{B_k + \Delta_k + \alpha^{-1}} 
    (z_k - \theta_k).
\end{cases}\]</p><p>The iterate sequence of interest is <span>$\lbrace \theta_k \rbrace$</span>.</p><p><strong>Arguments</strong></p><ul><li><code>optData::NesterovAcceleratedGD{T}</code>, the specification for the optimization    method.</li><li><code>progData&lt;:AbstractNLPModel{T,S}</code>, the specification for the optimization   problem.</li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>progData</code> must have an <code>initialize</code> function that returns subtypes of <code>AbstractPrecompute</code> and <code>AbstractProblemAllocate</code>, where the latter has a <code>grad</code> argument.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/0eadc1e7a54bf959b94f010bd15456fea616879d/src/methods/gd_nesterov_accelerated.jl#L116-L176">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.NesterovAcceleratedGD" href="#OptimizationMethods.NesterovAcceleratedGD"><code>OptimizationMethods.NesterovAcceleratedGD</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NesterovAcceleratedGD{T} &lt;: AbstractOptimizerData</code></pre><p>A structure that represents Nesterov Accelerated Gradient Descent. Stores variables related to the method, and tracks quantities as the algorithm progresses.</p><p><strong>Fields</strong></p><ul><li><code>name:String</code>, name of the solver for reference.</li><li><code>step_size::T</code>, step size used in the method. </li><li><code>z::Vector{T}</code>, buffer array for auxiliary iterate sequence</li><li><code>y::Vector{T}</code>, buffer array for convex combination of iterate and auxiliary   sequence </li><li><code>B::T</code>, auxiliary quadratic scaling term for computing acceleration weights    and step size.</li><li><code>threshold::T</code>, gradient threshold. If the norm gradient is below this, then    iteration stops.</li><li><code>max_iterations::Int64</code>, max number of iterations (gradient steps) taken by    the solver.</li><li><code>iter_hist::Vector{Vector{T}}</code>, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration <code>0</code>). The <code>k+1</code> entry   corresponds to the iterate at iteration <code>k</code>.</li><li><code>grad_val_hist::Vector{T}</code>, a vector for storing <code>max_iterations+1</code> gradient   norm values. The first entry corresponds to iteration <code>0</code>. The <code>k+1</code> entry   corresponds to the gradient norm at iteration <code>k</code>.</li><li><code>stop_iteration::Int64</code>, the iteration number that the solver stopped on.   The terminal iterate is saved at <code>iter_hist[stop_iteration+1]</code>.</li></ul><p><strong>Constructors</strong></p><pre><code class="nohighlight hljs">    NesterovAcceleratedGD(::Type{T}; x0::Vector{T}, step_size::T,
        threshold::T, max_iterations::Int64) where {T}</code></pre><p><strong>Arguments</strong></p><ul><li><code>T::DataType</code>, specific data type used for calculations.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>x0::Vector{T}</code>, initial point to start the solver at.</li><li><code>step_size::T</code>, step size used in the method. </li><li><code>threshold::T</code>, gradient threshold. If the norm gradient is below this,    then iteration is terminated. </li><li><code>max_iterations::Int</code>, max number of iterations (gradient steps) taken by    the solver.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/0eadc1e7a54bf959b94f010bd15456fea616879d/src/methods/gd_nesterov_accelerated.jl#L5-L51">source</a></section></article><h1 id="Gradient-Descent-with-Diminishing-Step-Size"><a class="docs-heading-anchor" href="#Gradient-Descent-with-Diminishing-Step-Size">Gradient Descent with Diminishing Step Size</a><a id="Gradient-Descent-with-Diminishing-Step-Size-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-Descent-with-Diminishing-Step-Size" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.DiminishingStepGD" href="#OptimizationMethods.DiminishingStepGD"><code>OptimizationMethods.DiminishingStepGD</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DiminishingStepGD{T} &lt;: AbstractOptimizerData{T}</code></pre><p>A structure for storing data about gradient descent using diminishing step sizes,     and recording the progress of its application on an optimization problem.</p><p><strong>Fields</strong></p><ul><li><code>name::String</code>, name of the solver for reference.</li><li><code>step_size_function::Function</code>, step size function. Should take the iteration    number and return the step size for that iteration.</li><li><code>step_size_scaling::T</code>, factor that is multipled with the amount of the step    size function.</li><li><code>threshold::T</code>, gradient threshold. If the norm gradient is below this, then    iteration stops.</li><li><code>max_iterations::Int64</code>, max number of iterations (gradient steps) taken by    the solver.</li><li><code>iter_hist::Vector{Vector{T}}</code>, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration <code>0</code>). The <code>k+1</code> entry   corresponds to the iterate at iteration <code>k</code>.</li><li><code>grad_val_hist::Vector{T}</code>, a vector for storing <code>max_iterations+1</code> gradient   norm values. The first entry corresponds to iteration <code>0</code>. The <code>k+1</code> entry   correpsonds to the gradient norm at iteration <code>k</code>.</li><li><code>stop_iteration::Int64</code>, the iteration number that the solver stopped on.   The terminal iterate is saved at <code>iter_hist[stop_iteration+1]</code>.</li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>step_size_function</code> should take in two arguments, the data type for computation <code>T</code> and the iteration number. For example, calling <code>step_size_function(Float64, 1)</code> should return the step size as a <code>Float64</code> for the iteration <code>1</code>.</p></div></div><p><strong>Constructors</strong></p><pre><code class="nohighlight hljs">DiminishingStepGD(::Type{T}; x0::Vector{T}, step_size_function::Function,
    step_size_scaling::T, threshold::T, max_iterations::Int)</code></pre><p>Constructs the <code>struct</code> for the diminishing step size gradient descent method.</p><p><strong>Arguments</strong></p><ul><li><code>T::DataType</code>, specific data type used for calculations.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>x0::Vector{T}</code>, initial point to start the solver at.</li><li><code>step_size_function::Function</code>, step size function. Should take the iteration    number and return the step size for that iteration.</li><li><code>step_size_scaling::T</code>, factor that is multipled with the amount of the step    size function.</li><li><code>threshold::T</code>, gradient threshold. If the norm gradient is below this,    then iteration is terminated. </li><li><code>max_iterations::Int</code>, max number of iterations (gradient steps) taken by    the solver.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/0eadc1e7a54bf959b94f010bd15456fea616879d/src/methods/gd_diminishing.jl#L6-L60">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.diminishing_step_gd" href="#OptimizationMethods.diminishing_step_gd"><code>OptimizationMethods.diminishing_step_gd</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">diminishing_step_gd(optData::DiminishingStepGD{T}, progData::P
    where P &lt;: AbstractNLPModel{T, S}) where {T, S}</code></pre><p>Implements gradient descent with diminishing step sizes and applies the method     to the optimization problem specified by <code>progData</code>.</p><p><strong>Reference(s)</strong></p><p><a href="../references/#patel2024Gradient">Patel, Vivak, and Albert Berahas. “Gradient Descent in the Absence of Global      Lipschitz Continuity of the Gradients.” SIAM 6 (3): 579–846.      https://doi.org/10.1137/22M1527210.</a></p><p><a href="../references/#bertsekas2016Nonlinear">Bertsekas, Dimitri. &quot;Nonlinear Programming&quot;. 3rd Edition, Athena Scientific,      Chapter 1.</a></p><p><strong>Method</strong></p><p>Given iterates <span>$\lbrace x_0,\ldots,x_k\rbrace$</span>, the iterate <span>$x_{k + 1}$</span>     is equal to <span>$x_k - \alpha_k \nabla f(x_k)$</span>, where <span>$\alpha_k$</span> is equal     to <code>optData.step_size_scaling * optData.step_size_function(T, k)</code>.</p><div class="admonition is-info"><header class="admonition-header">Step Size Function</header><div class="admonition-body"><p>The step size function should satisfy several conditions. First, <span>$\alpha_k &gt; 0$</span> for all <span>$k$</span>. Second, <span>$\lim_{k \to \infty} \alpha_k = 0.$</span> Finally, <span>$\sum_{k=0}^{\infty} \alpha_k = \infty.$</span> See <a href="../references/#patel2024Gradient">Patel and Berahas (2024)</a> for details.</p></div></div><p><strong>Arguments</strong></p><ul><li><code>optData::DiminishingStepGD{T}</code>, the specification for the optimization method.</li><li><code>progData&lt;:AbstractNLPModel{T,S}</code>, the specification for the optimization   problem. </li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>progData</code> must have an <code>initialize</code> function that returns subtypes of <code>AbstractPrecompute</code> and <code>AbstractProblemAllocate</code>, where the latter has a <code>grad</code> argument.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/0eadc1e7a54bf959b94f010bd15456fea616879d/src/methods/gd_diminishing.jl#L97-L136">source</a></section></article><p>Below is a list of step size functions that are in the library. The step size sequence generated by these functions, <span>$\lbrace \alpha_k \rbrace$</span>  satisfies <span>$\alpha_k &gt; 0$</span>, <span>$\lim_{k \to\infty} \alpha_k = 0$</span> and  <span>$\sum_{k=0}^\infty \alpha_k = \infty$</span>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.inverse_k_step_size" href="#OptimizationMethods.inverse_k_step_size"><code>OptimizationMethods.inverse_k_step_size</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">inverse_k_step_size(::Type{T}, k::Int64)</code></pre><p>Return the inverse of <code>k</code>.</p><p><strong>Method</strong></p><p>The step size sequence generated for <span>$k \in \mathbb{N}$</span> is</p><p class="math-container">\[    \alpha_k = \frac{1}{k+1},\]</p><p>when using this method.</p><p><strong>Arguments</strong></p><ul><li><code>T::Type</code>, data type that the computations are done in.</li><li><code>k::Int64</code>, index of the step size needed. </li></ul><p><strong>Returns</strong></p><p>Returns a number of type <code>T</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/0eadc1e7a54bf959b94f010bd15456fea616879d/src/methods/stepsize_helpers/diminishing_stepsizes.jl#L6-L27">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.inverse_log2k_step_size" href="#OptimizationMethods.inverse_log2k_step_size"><code>OptimizationMethods.inverse_log2k_step_size</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">inverse_log2k_step_size(::Type{T}, k::Int64) where {T}</code></pre><p><strong>Method</strong></p><p>The step size sequence generated when using this method is</p><p class="math-container">\[    \alpha_k = \frac{1}{\lfloor \log_2(k+1) + 1 \rfloor}\]</p><p>for <span>$k \in \mathbb{N}$</span>.</p><p><strong>Arguments</strong></p><ul><li><code>T::Type</code>, data type that the computation are done in.</li><li><code>k::Int64</code>, index of the step size needed.</li></ul><p><strong>Returns</strong></p><p>Returns a number of type <code>T</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/0eadc1e7a54bf959b94f010bd15456fea616879d/src/methods/stepsize_helpers/diminishing_stepsizes.jl#L32-L51">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OptimizationMethods.stepdown_100_step_size" href="#OptimizationMethods.stepdown_100_step_size"><code>OptimizationMethods.stepdown_100_step_size</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">stepdown_100_step_size(::Type{T}, k::Int64) where {T}</code></pre><p><strong>Method</strong></p><p>The step size sequence generated is</p><p class="math-container">\[    \alpha_k = \frac{1}{2^i}\]</p><p>for <span>$k \in [ (2^i-1)100, (2^{i+1}-1)100)$</span>.</p><p><strong>Arguments</strong></p><ul><li><code>T::Type</code>, data type that the computation are done in.</li><li><code>k::Int64</code>, index of the step size needed.</li></ul><p><strong>Returns</strong></p><p>Returns a number of type <code>T</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/numoptim/OptimizationMethods.jl/blob/0eadc1e7a54bf959b94f010bd15456fea616879d/src/methods/stepsize_helpers/diminishing_stepsizes.jl#L56-L75">source</a></section></article><h1 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h1><ul><li><a href="#OptimizationMethods.BarzilaiBorweinGD"><code>OptimizationMethods.BarzilaiBorweinGD</code></a></li><li><a href="#OptimizationMethods.DiminishingStepGD"><code>OptimizationMethods.DiminishingStepGD</code></a></li><li><a href="#OptimizationMethods.FixedStepGD"><code>OptimizationMethods.FixedStepGD</code></a></li><li><a href="../api_problems/#OptimizationMethods.LeastSquares"><code>OptimizationMethods.LeastSquares</code></a></li><li><a href="#OptimizationMethods.LipschitzApproxGD"><code>OptimizationMethods.LipschitzApproxGD</code></a></li><li><a href="../api_problems/#OptimizationMethods.LogisticRegression"><code>OptimizationMethods.LogisticRegression</code></a></li><li><a href="#OptimizationMethods.NesterovAcceleratedGD"><code>OptimizationMethods.NesterovAcceleratedGD</code></a></li><li><a href="../api_problems/#OptimizationMethods.PoissonRegression"><code>OptimizationMethods.PoissonRegression</code></a></li><li><a href="../api_problems/#OptimizationMethods.QLLogisticSin"><code>OptimizationMethods.QLLogisticSin</code></a></li><li><a href="#OptimizationMethods.WeightedNormDampingGD"><code>OptimizationMethods.WeightedNormDampingGD</code></a></li><li><a href="#OptimizationMethods.barzilai_borwein_gd"><code>OptimizationMethods.barzilai_borwein_gd</code></a></li><li><a href="../api_problems/#OptimizationMethods.centered_exp"><code>OptimizationMethods.centered_exp</code></a></li><li><a href="../api_problems/#OptimizationMethods.centered_shifted_log"><code>OptimizationMethods.centered_shifted_log</code></a></li><li><a href="../api_problems/#OptimizationMethods.ddlogistic"><code>OptimizationMethods.ddlogistic</code></a></li><li><a href="#OptimizationMethods.diminishing_step_gd"><code>OptimizationMethods.diminishing_step_gd</code></a></li><li><a href="../api_problems/#OptimizationMethods.dlinear_plus_sin"><code>OptimizationMethods.dlinear_plus_sin</code></a></li><li><a href="../api_problems/#OptimizationMethods.dlogistic"><code>OptimizationMethods.dlogistic</code></a></li><li><a href="#OptimizationMethods.fixed_step_gd"><code>OptimizationMethods.fixed_step_gd</code></a></li><li><a href="../api_problems/#OptimizationMethods.inverse_complimentary_log_log"><code>OptimizationMethods.inverse_complimentary_log_log</code></a></li><li><a href="#OptimizationMethods.inverse_k_step_size"><code>OptimizationMethods.inverse_k_step_size</code></a></li><li><a href="#OptimizationMethods.inverse_log2k_step_size"><code>OptimizationMethods.inverse_log2k_step_size</code></a></li><li><a href="../api_problems/#OptimizationMethods.inverse_probit"><code>OptimizationMethods.inverse_probit</code></a></li><li><a href="../api_problems/#OptimizationMethods.linear_plus_sin"><code>OptimizationMethods.linear_plus_sin</code></a></li><li><a href="#OptimizationMethods.lipschitz_approximation_gd"><code>OptimizationMethods.lipschitz_approximation_gd</code></a></li><li><a href="../api_problems/#OptimizationMethods.logistic"><code>OptimizationMethods.logistic</code></a></li><li><a href="../api_problems/#OptimizationMethods.monomial_plus_constant"><code>OptimizationMethods.monomial_plus_constant</code></a></li><li><a href="#OptimizationMethods.nesterov_accelerated_gd"><code>OptimizationMethods.nesterov_accelerated_gd</code></a></li><li><a href="#OptimizationMethods.stepdown_100_step_size"><code>OptimizationMethods.stepdown_100_step_size</code></a></li><li><a href="#OptimizationMethods.weighted_norm_damping_gd"><code>OptimizationMethods.weighted_norm_damping_gd</code></a></li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../api_problems/">« Problems</a><a class="docs-footer-nextpage" href="../references/">References »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Sunday 9 February 2025 18:05">Sunday 9 February 2025</span>. Using Julia version 1.11.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
