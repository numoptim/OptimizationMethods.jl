var documenterSearchIndex = {"docs":
[{"location":"api_problems/#Contents","page":"Problems","title":"Contents","text":"","category":"section"},{"location":"api_problems/","page":"Problems","title":"Problems","text":"Pages=[\"api_problems.md\"]","category":"page"},{"location":"api_problems/#Regression-Problems","page":"Problems","title":"Regression Problems","text":"","category":"section"},{"location":"api_problems/","page":"Problems","title":"Problems","text":"OptimizationMethods.LeastSquares\n\nOptimizationMethods.LogisticRegression\n\nOptimizationMethods.PoissonRegression","category":"page"},{"location":"api_problems/#OptimizationMethods.LeastSquares","page":"Problems","title":"OptimizationMethods.LeastSquares","text":"LeastSquares{T,S} <: AbstractNLSModel{T,S}\n\nImplements the data structure for defining a least squares problem.\n\nObjective Function\n\nmin_x 05Vert F(x) Vert_2^2\n\nwhere\n\nF(x) = A * x - b\n\nA is the coefficient matrix. b is the constant vector.\n\nFields\n\nmeta::NLPModelMeta{T, S}, data structure for nonlinear programming models\nnls_meta::NLSMeta{T, S}, data structure for nonlinear least squares models\ncounters::NLSCounters, counters for nonlinear least squares models\ncoef::Matrix{T}, coefficient matrix, A, for least squares problem \ncons::Vector{T}, constant vector, b, for least squares problem\n\nConstructors\n\nLeastSquares(::Type{T}; nequ=1000, nvar=50) where {T}\n\nConstructs a least squares problems with 1000 equations and 50 unknowns,     where the entries of the matrix and constant vector are independent     standard Gaussian variables.\n\nArguments\n\nT::DataType, specific data type of the optimization parameter\n\nOptional Keyword Arguments\n\nnequ::Int64=1000, the number of equations in the system \nnvar::Int64=50, the number of parameters in the system \nLeastSquares(design::Matrix{T}, response::Vector{T};        x0 = ones(T, size(design, 2))) where {T}    \n\nConstructs a least squares problem using the design as the coef matrix and the response as the cons vector. \n\nArguments\n\ndesign::Matrix{T}, coefficient matrix for least squares.\nresponse::Vector{T}, constant vector for least squares.\n\nOptional Keyword Arguments\n\nx0::Vector{T}=ones(T, size(design, 2)), default starting point for    optimization algorithms.\n\n\n\n\n\n","category":"type"},{"location":"api_problems/#OptimizationMethods.LogisticRegression","page":"Problems","title":"OptimizationMethods.LogisticRegression","text":"LogisticRegression{T,S} <: AbstractNLPModel{T,S}\n\nImplements logistic regression problem with canonical link function. If     the covariate (i.e., design) matrix and response vector are not supplied,     then these are simulated. \n\nObjective Function\n\nLet A be the covariate matrix and b denote the response vector. The     rows of A and corresponding entry of b correspond to      the predictors and response for the same experimental unit.      Let A_i be the vector that is row i of A,     and let b_i be the i entry of b. Note, b_i is either 0 or      1.\n\nLet mu(x) denote a vector-valued function whose ith entry is \n\nmu_i(x) = frac11 + exp(-A_i^intercal x)\n\nIf A has n rows (i.e., n is the number of observations),  then the objective function is negative log-likelihood function given by\n\nF(x) = -sum_i=1^n b_i log( mu_i(x) ) + (1 - b_i) log(1 - mu_i(x))\n\nFields\n\nmeta::NLPModelMeta{T,S}, data structure for nonlinear programming models\ncounters::Counters, counters for a nonlinear programming model\ndesign::Matrix{T}, the design matrix, A, of the logistic regression   problem\nresponse::Vector{Bool}, the response vector, b, of the logistic   regression problem\n\nConstructors\n\nLogisticRegression(::Type{T}; nobs::Int64 = 1000, nvar::Int64 = 50) where T\n\nConstructs a simulated problem where the number of observations is nobs and     the dimension of the parameter is nvar. The generated design matrix's     first column is all 1s. The remaining columns are independent random     normal entries such that each row (excluding the first entry) has unit     variance. The design matrix is stored as type Matrix{T}.\n\nLogisticRegression(design::Matrix{T}, response::Vector{Bool};\n    x0::Vector{T} = ones(T, size(design, 2)) ./ sqrt(size(design, 2))\n    ) where T\n\nConstructs a LogisticRegression problem with design matrix design and      response vector response. The default initial iterate, x0 is      a scaling of the vector of ones. x0 is optional. \n\n\n\n\n\n","category":"type"},{"location":"api_problems/#OptimizationMethods.PoissonRegression","page":"Problems","title":"OptimizationMethods.PoissonRegression","text":"PoissonRegression{T, S} <: AbstractNLPModel{T, S}\n\nImplements Poisson regression with the canonical link function. If the design     matrix (i.e., the covariates) and responses are not supplied, they are      randomly generated. \n\nObjective Function\n\nLet A be the design matrix, and b be the responses. Each row of A     and corresponding entry in b are the predictor and observation from one      unit. The entries in b must be integer valued and non-negative.\n\nLet A_i be row i of A and b_i entry i of b. Let\n\nmu_i(x) = exp(A_i^intercal x)\n\nLet n be the number of rows of A (i.e., number of observations), then     the negative log-likelihood of the model is \n\nsum_i=1^n mu_i(x) - b_i (A_i^intercal x) + C(b)\n\nwhere C(b) is a constant depending on the data. We implement the objective       function to be the negative log-likelihood up to the constant term C(b).      That is,\n\nF(x) = sum_i=1^n mu_i(x) - b_i (A_i^intercal x)\n\n!!! Remark     Because the additive term C(b) is not included in the objective function,     the objective function can take on negative values.\n\nFields\n\nmeta::NLPModelMeta{T, S}, NLPModel struct for storing meta information for    the problem\ncounters::Counters, NLPModel Counter struct that provides evaluations    tracking.\ndesign::Matrix{T}, covariate matrix for the problem/experiment (A).\nresponse::Vector{T}, observations for the problem/experiment (b).\n\nConstructors\n\nPoissonRegression(::Type{T}; nobs::Int64 = 1000, nvar::Int64 = 50) where {T}\n\nConstruct the struct for Poisson Regression when simulated data is needed.      The design matrix (A) and response vector b are randomly generated      as follows.      For the design matrix, the first column is all ones, and the rest are     generated according to a normal distribution where each row has been      scaled to have unit variance (excluding the first column).      For the response vector, let beta be the \"true\" relationship between      the covariates and response vector for the poisson regression model,      then the ith entry of the response vector is generated from a Poisson      Distribution with rate parameter exp(A_i^intercal beta).\n\nPoissonRegression(design::Matrix{T}, response::Vector{T}; \n    x0::Vector{T} = zeros(T, size(design)[2])) where {T}\n\nConstructs the struct for Poisson Regression when the design matrix and response      vector are known. The initial guess, x0 is a keyword argument that is set      to all zeros by default. \n\n!!! Remark     When using this constructor, the number of rows of design must be equal to      the size of response. When providing x0, the number of entries must be the      same as the number of columns in design.\n\n\n\n\n\n","category":"type"},{"location":"api_problems/#Index","page":"Problems","title":"Index","text":"","category":"section"},{"location":"api_problems/","page":"Problems","title":"Problems","text":"","category":"page"},{"location":"api_methods/#Contents","page":"Methods","title":"Contents","text":"","category":"section"},{"location":"api_methods/","page":"Methods","title":"Methods","text":"Pages=[\"api_methods.md\"]","category":"page"},{"location":"api_methods/#Barzilai-Borwein-Method","page":"Methods","title":"Barzilai Borwein Method","text":"","category":"section"},{"location":"api_methods/","page":"Methods","title":"Methods","text":"BarzilaiBorweinGD\n\nbarzilai_borwein_gd","category":"page"},{"location":"api_methods/#OptimizationMethods.BarzilaiBorweinGD","page":"Methods","title":"OptimizationMethods.BarzilaiBorweinGD","text":"BarzilaiBorweinGD{T} <: AbstractOptimizerData{T}\n\nA structure for storing data about gradient descent using the Barzilai-Borwein      step size, and the progress of its application on an optimization problem.\n\nFields\n\nname:String, name of the solver for reference.\ninit_stepsize::T, initial step size to start the method. \nlong_stepsize::Bool, flag for step size; if true, use the long version of    Barzilai-Borwein. If false, use the short version. \nthreshold::T, gradient threshold. If the norm gradient is below this, then    iteration stops.\nmax_iterations::Int64, max number of iterations (gradient steps) taken by    the solver.\niter_diff::Vector{T}, a buffer for storing differences between subsequent   iterate values that are used for computing the step size\ngrad_diff::Vector{T}, a buffer for storing differences between gradient    values at adjacent iterates, which is used to compute the step size\niter_hist::Vector{Vector{T}}, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration 0). The k+1 entry   corresponds to the iterate at iteration k.\ngrad_val_hist::Vector{T}, a vector for storing max_iterations+1 gradient   norm values. The first entry corresponds to iteration 0. The k+1 entry   correpsonds to the gradient norm at iteration k.\nstop_iteration::Int64, the iteration number that the solver stopped on.   The terminal iterate is saved at iter_hist[stop_iteration+1].\n\nConstructors\n\nBarzilaiBorweinGD(::Type{T}; x0::Vector{T}, init_stepsize::T, \n    long_stepsize::Bool, threshold::T, max_iterations::Int) where {T}\n\nConstructs the struct for the Barzilai-Borwein optimization method\n\nArguments\n\nT::DataType, specific data type used for calculations.\n\nKeyword Arguments\n\nx0::Vector{T}, initial point to start the solver at.\ninit_stepsize::T, initial step size used for the first iteration. \nlong_stepsize::Bool, flag for step size; if true, use the long version of   Barzilai-Borwein, if false, use the short version. \nthreshold::T, gradient threshold. If the norm gradient is below this,    then iteration is terminated. \nmax_iterations::Int, max number of iterations (gradient steps) taken by    the solver.\n\n\n\n\n\n","category":"type"},{"location":"api_methods/#OptimizationMethods.barzilai_borwein_gd","page":"Methods","title":"OptimizationMethods.barzilai_borwein_gd","text":"barzilai_borwein_gd(optData::BarzilaiBorweinGD{T}, progData::P \n    where P <: AbstractNLPModel{T, S}) where {T,S}\n\nImplements gradient descent with Barzilai-Borwein step size and applies the      method to the optimization problem specified by progData. \n\nReference(s)\n\nBarzilai and Borwein. \"Two-Point Step Size Gradient Methods\". IMA Journal of      Numerical Analysis.\n\nMethod\n\nGiven iterates lbrace x_0ldotsx_krbrace, the iterate x_k+1     is equal to x_k - alpha_k nabla f(x_k), where alpha_k is     one of two versions.\n\nLong Step Size Version (if optData.long_stepsize==true)\n\nIf k=0, then alpha_0 is set to optData.init_stepsize. For k0,\n\nalpha_k = frac Vert x_k - x_k-1 Vert_2^2(x_k - x_k-1^intercal \n    (nabla f(x_k) - nabla f(x_k-1)))\n\nShort Step Size Version (if optData.long_stepsize==false)\n\nIf k=0, then alpha_0 is set to optData.init_stepsize. For k0,\n\nalpha_k = frac(x_k - x_k-1^intercal (nabla f(x_k) - \n    nabla f(x_k-1)))Vert nabla f(x_k) - nabla f(x_k-1)Vert_2^2\n\nArguments\n\noptData::BarzilaiBorweinGD{T}, the specification for the optimization method.\nprogData<:AbstractNLPModel{T,S}, the specification for the optimization   problem. \n\nwarning: Warning\nprogData must have an initialize function that returns subtypes of AbstractPrecompute and AbstractProblemAllocate, where the latter has a grad argument.\n\n\n\n\n\n","category":"function"},{"location":"api_methods/#Gradient-Descent-with-Fixed-Step-Size","page":"Methods","title":"Gradient Descent with Fixed Step Size","text":"","category":"section"},{"location":"api_methods/","page":"Methods","title":"Methods","text":"FixedStepGD\n\nfixed_step_gd","category":"page"},{"location":"api_methods/#OptimizationMethods.FixedStepGD","page":"Methods","title":"OptimizationMethods.FixedStepGD","text":"FixedStepGD{T} <: AbstractOptimizerData{T}\n\nA structure for storing data about fixed step-size gradient descent, and the     progress of its application on an optimization problem.\n\nFields\n\nname::String, name of the solver for reference\nstep_size::T, the step-size selection for the optimization procedure\nthreshold::T, the threshold on the norm of the gradient to induce stopping\nmax_iterations::Int, the maximum allowed iterations\niter_hist::Vector{Vector{T}}, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration 0). The k+1 entry   corresponds to the iterate at iteration k.\ngrad_val_hist::Vector{T}, a vector for storing max_iterations+1 gradient   norm values. The first entry corresponds to iteration 0. The k+1 entry   correpsonds to the gradient norm at iteration k\nstop_iteration, iteration number that the algorithm stopped at.   Iterate number stop_iteration is produced. \n\nConstructors\n\nFixedStepGD(::Type{T}; x0::Vector{T}, step_size::T, threshold::T, \n    max_iterations::Int) where {T}\n\nConstructs the struct for the optimizer.\n\nArguments\n\nT::DataType, specific data type for the calculations\n\nKeyword Arguments\n\nx0::Vector{T}, the initial iterate for the optimizers\nstep_size::T, the step size of the optimizer \nthreshold::T, the threshold on the norm of the gradient to induce stopping\nmax_iterations::Int, the maximum number of iterations allowed  \n\n\n\n\n\n","category":"type"},{"location":"api_methods/#OptimizationMethods.fixed_step_gd","page":"Methods","title":"OptimizationMethods.fixed_step_gd","text":"fixed_step_gd(optData::FixedStepGD{T}, progData<:AbstractNLPModel{T,S})\n    where {T,S}\n\nImplements fixed step-size gradient descent for the desired optimization problem     specified by progData.\n\nMethod\n\nThe iterates are updated according to the procedure\n\nx_k+1 = x_k - alpha f(x_k)\n\nwhere alpha is the step size, f is the objective function, and f is the      gradient function of f. \n\nArguments\n\noptData::FixedStepGD{T}, the specification for the optimization method.\nprogData<:AbstractNLPModel{T,S}, the specification for the optimization   problem. \n\nwarning: Warning\nprogData must have an initialize function that returns subtypes of AbstractPrecompute and AbstractProblemAllocate, where the latter has a grad argument. \n\n\n\n\n\n","category":"function"},{"location":"api_methods/#Lipschitz-Approximation-(Malitsky-and-Mishchenko)","page":"Methods","title":"Lipschitz Approximation (Malitsky & Mishchenko)","text":"","category":"section"},{"location":"api_methods/","page":"Methods","title":"Methods","text":"LipschitzApproxGD\n\nlipschitz_approximation_gd","category":"page"},{"location":"api_methods/#OptimizationMethods.LipschitzApproxGD","page":"Methods","title":"OptimizationMethods.LipschitzApproxGD","text":"LipschitzApproxGD{T} <: AbstractOptimizerData{T}\n\nA structure for storing data about adaptive gradient descent     using a Lipschitz Approximation scheme (AdGD), and the progress      of its application on an optimization problem.\n\nFields\n\nname::String, name of the solver for reference\ninit_stepsize::T, the initial step size for the method\nprev_stepsize::T, step size used at iter - 1 when iter > 1.\ntheta::T, element used in the computation of the step size. See the    referenced paper for more information.\nlipschitz_approximation::T, help the lipschitz approximation used in the   computation of the step size. See the referenced paper for more information.\nthreshold::T, the threshold on the norm of the gradient to induce stopping\nmax_iterations::Int64, the maximum allowed iterations\niter_diff::Vector{T}, a buffer for storing differences between subsequent   iterate values that are used for computing the step size\ngrad_diff::Vector{T}, a buffer for storing differences between gradient    values at adjacent iterates, which is used to compute the step size\niter_hist::Vector{Vector{T}}, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration 0). The k+1 entry   corresponds to the iterate at iteration k.\ngrad_val_hist::Vector{T}, a vector for storing max_iterations+1 gradient   norm values. The first entry corresponds to iteration 0. The k+1 entry   corresponds to the gradient norm at iteration k\nstop_iteration::Int64, iteration number that the algorithm stopped at.   Iterate number stop_iteration is produced. \n\nConstructors\n\nLipschitzApproxGD(::Type{T}; x0::Vector{T}, init_stepsize::T, threshold::T, \n    max_iterations::Int) where {T}\n\nConstructs the struct for the optimizer.\n\nArguments\n\nT::DataType, specific data type for the calculations\n\nKeyword Arguments\n\nx0::Vector{T}, the initial iterate for the optimizers\ninit_stepsize::T, the initial step size for the method\nthreshold::T, the threshold on the norm of the gradient to induce stopping\nmax_iterations::Int, the maximum number of iterations allowed  \n\n\n\n\n\n","category":"type"},{"location":"api_methods/#OptimizationMethods.lipschitz_approximation_gd","page":"Methods","title":"OptimizationMethods.lipschitz_approximation_gd","text":"lipschitz_approximation_gd(optData::FixedStepGD{T}, progData::P where P \n    <: AbstractNLPModel{T, S}) where {T, S}\n\nImplements gradient descent with adaptive step sizes formed through a lipschitz      approximation for the desired optimization problem specified by progData.\n\nwarning: Warning\nThis method is designed for convex optimization problems.\n\nReferences(s)\n\nMalitsky, Y. and Mishchenko, K. (2020). \"Adaptive Gradient Descent without      Descent.\"      Proceedings of the 37th International Conference on Machine Learning,      in Proceedings of Machine Learning Research 119:6702-6712.     Available from https://proceedings.mlr.press/v119/malitsky20a.html.\n\nMethod\n\nThe iterates are updated according to the procedure,\n\nx_k+1 = x_k - alpha_k nabla f(x_k)\n\nwhere alpha_k is the step size and nabla f is the gradient function      of the objective function f.\n\nThe step size is computed depending on k.      When k = 0, alpha_k = optDatainit_stepsize.      When k  0, \n\nalpha_k = minleftlbrace sqrt1 + theta_k-1alpha_k-1 \n    fracVert x_k - x_k-1 VertVert nabla f(x_k) - \n    nabla f(x_k-1)Vert rightrbrace\n\nwhere theta_0 = inf and theta_k = alpha_k  alpha_k-1.\n\nArguments\n\noptData::LipschitzApproxGD{T}, the specification for the optimization method.\nprogData<:AbstractNLPModel{T,S}, the specification for the optimization   problem. \n\nwarning: Warning\nprogData must have an initialize function that returns subtypes of AbstractPrecompute and AbstractProblemAllocate, where the latter has a grad argument. \n\n\n\n\n\n","category":"function"},{"location":"api_methods/#Index","page":"Methods","title":"Index","text":"","category":"section"},{"location":"api_methods/","page":"Methods","title":"Methods","text":"","category":"page"},{"location":"problems/quasilikelihood_estimation/#Quasi-likelihood-Estimation","page":"Quasi-likelihood Estimation","title":"Quasi-likelihood Estimation","text":"","category":"section"},{"location":"#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"OptimizationMethods is a  Julia library for implementing and comparing optimization methods with a focus on problems arising in data science. The library is primarily designed to serve those researching optimization  methods for data science applications. Accordingly, the library is not implementing highly efficient versions of these methods, even though we do our best to make preliminary efficiency optimizations to the code.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"There are two primary components to this library.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Problems, which are implementations of optimization problems primarily   arising in data science. At the moment, problems follow the guidelines   provided by    NLPModels. \nMethods, which are implementations of important optimization methods   that appear in the literature. ","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"The library is still in its infancy and will continue to evolve rapidly. To understand how to use the library, we recommend looking in the examples directory to see how different problems are instantiated and how optimization methods can be applied to them. We also recommend looking at the docstring for specific problems and methods for additional details.","category":"page"},{"location":"#Manual","page":"Overview","title":"Manual","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"The manual section includes descriptions of problems and methods that require a bit more explanation than what is appropriate for in a docstring.","category":"page"},{"location":"#API","page":"Overview","title":"API","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"The API section contains explanations for all problems and methods available in  the library. This is a super set of what is contained in the manual. ","category":"page"}]
}
