var documenterSearchIndex = {"docs":
[{"location":"api_problems/#Contents","page":"Problems","title":"Contents","text":"","category":"section"},{"location":"api_problems/","page":"Problems","title":"Problems","text":"Pages=[\"api_problems.md\"]","category":"page"},{"location":"api_problems/#Regression-Problems","page":"Problems","title":"Regression Problems","text":"","category":"section"},{"location":"api_problems/#Generalized-Linear-Models","page":"Problems","title":"Generalized Linear Models","text":"","category":"section"},{"location":"api_problems/#OptimizationMethods.LeastSquares","page":"Problems","title":"OptimizationMethods.LeastSquares","text":"LeastSquares{T,S} <: AbstractNLSModel{T,S}\n\nImplements the data structure for defining a least squares problem.\n\nObjective Function\n\nmin_x 05Vert F(x) Vert_2^2\n\nwhere\n\nF(x) = A * x - b\n\nA is the coefficient matrix. b is the constant vector.\n\nFields\n\nmeta::NLPModelMeta{T, S}, data structure for nonlinear programming models\nnls_meta::NLSMeta{T, S}, data structure for nonlinear least squares models\ncounters::NLSCounters, counters for nonlinear least squares models\ncoef::Matrix{T}, coefficient matrix, A, for least squares problem \ncons::Vector{T}, constant vector, b, for least squares problem\n\nConstructors\n\nLeastSquares(::Type{T}; nequ=1000, nvar=50) where {T}\n\nConstructs a least squares problems with 1000 equations and 50 unknowns,     where the entries of the matrix and constant vector are independent     standard Gaussian variables.\n\nArguments\n\nT::DataType, specific data type of the optimization parameter\n\nOptional Keyword Arguments\n\nnequ::Int64=1000, the number of equations in the system \nnvar::Int64=50, the number of parameters in the system \n\nLeastSquares(design::Matrix{T}, response::Vector{T}; \n    x0 = ones(T, size(design, 2))) where {T}\n\nConstructs a least squares problem using the design as the coef matrix and the response as the cons vector. \n\nArguments\n\ndesign::Matrix{T}, coefficient matrix for least squares.\nresponse::Vector{T}, constant vector for least squares.\n\nOptional Keyword Arguments\n\nx0::Vector{T}=ones(T, size(design, 2)), default starting point for    optimization algorithms.\n\n\n\n\n\n","category":"type"},{"location":"api_problems/#OptimizationMethods.LogisticRegression","page":"Problems","title":"OptimizationMethods.LogisticRegression","text":"LogisticRegression{T,S} <: AbstractNLPModel{T,S}\n\nImplements logistic regression problem with canonical link function. If     the covariate (i.e., design) matrix and response vector are not supplied,     then these are simulated. \n\nObjective Function\n\nLet A be the covariate matrix and b denote the response vector. The     rows of A and corresponding entry of b correspond to      the predictors and response for the same experimental unit.      Let A_i be the vector that is row i of A,     and let b_i be the i entry of b. Note, b_i is either 0 or      1.\n\nLet mu(x) denote a vector-valued function whose ith entry is \n\nmu_i(x) = frac11 + exp(-A_i^intercal x)\n\nIf A has n rows (i.e., n is the number of observations),  then the objective function is negative log-likelihood function given by\n\nF(x) = -sum_i=1^n b_i log( mu_i(x) ) + (1 - b_i) log(1 - mu_i(x))\n\nFields\n\nmeta::NLPModelMeta{T,S}, data structure for nonlinear programming models\ncounters::Counters, counters for a nonlinear programming model\ndesign::Matrix{T}, the design matrix, A, of the logistic regression   problem\nresponse::Vector{Bool}, the response vector, b, of the logistic   regression problem\n\nConstructors\n\nLogisticRegression(::Type{T}; nobs::Int64 = 1000, nvar::Int64 = 50) where T\n\nConstructs a simulated problem where the number of observations is nobs and     the dimension of the parameter is nvar. The generated design matrix's     first column is all 1s. The remaining columns are independent random     normal entries such that each row (excluding the first entry) has unit     variance. The design matrix is stored as type Matrix{T}.\n\nLogisticRegression(design::Matrix{T}, response::Vector{Bool};\n    x0::Vector{T} = ones(T, size(design, 2)) ./ sqrt(size(design, 2))\n    ) where T\n\nConstructs a LogisticRegression problem with design matrix design and      response vector response. The default initial iterate, x0 is      a scaling of the vector of ones. x0 is optional. \n\n\n\n\n\n","category":"type"},{"location":"api_problems/#OptimizationMethods.PoissonRegression","page":"Problems","title":"OptimizationMethods.PoissonRegression","text":"PoissonRegression{T, S} <: AbstractNLPModel{T, S}\n\nImplements Poisson regression with the canonical link function. If the design     matrix (i.e., the covariates) and responses are not supplied, they are      randomly generated. \n\nObjective Function\n\nLet A be the design matrix, and b be the responses. Each row of A     and corresponding entry in b are the predictor and observation from one      unit. The entries in b must be integer valued and non-negative.\n\nLet A_i be row i of A and b_i entry i of b. Let\n\nmu_i(x) = exp(A_i^intercal x)\n\nLet n be the number of rows of A (i.e., number of observations), then     the negative log-likelihood of the model is \n\nsum_i=1^n mu_i(x) - b_i (A_i^intercal x) + C(b)\n\nwhere C(b) is a constant depending on the data. We implement the objective       function to be the negative log-likelihood up to the constant term C(b).      That is,\n\nF(x) = sum_i=1^n mu_i(x) - b_i (A_i^intercal x)\n\nwarn: Warn\nBecause the additive term C(b) is not included in the objective function, the objective function can take on negative values.\n\nFields\n\nmeta::NLPModelMeta{T, S}, NLPModel struct for storing meta information for    the problem\ncounters::Counters, NLPModel Counter struct that provides evaluations    tracking.\ndesign::Matrix{T}, covariate matrix for the problem/experiment (A).\nresponse::Vector{T}, observations for the problem/experiment (b).\n\nConstructors\n\nPoissonRegression(::Type{T}; nobs::Int64 = 1000, nvar::Int64 = 50) where {T}\n\nConstruct the struct for Poisson Regression when simulated data is needed.      The design matrix (A) and response vector b are randomly generated      as follows.      For the design matrix, the first column is all ones, and the rest are     generated according to a normal distribution where each row has been      scaled to have unit variance (excluding the first column).      For the response vector, let beta be the \"true\" relationship between      the covariates and response vector for the poisson regression model,      then the ith entry of the response vector is generated from a Poisson      Distribution with rate parameter exp(A_i^intercal beta).\n\nPoissonRegression(design::Matrix{T}, response::Vector{T}; \n    x0::Vector{T} = zeros(T, size(design)[2])) where {T}\n\nConstructs the struct for Poisson Regression when the design matrix and response      vector are known. The initial guess, x0 is a keyword argument that is set      to all zeros by default. \n\n!!! Remark     When using this constructor, the number of rows of design must be equal to      the size of response. When providing x0, the number of entries must be the      same as the number of columns in design.\n\n\n\n\n\n","category":"type"},{"location":"api_problems/#Quasi-likelihood-Objectives","page":"Problems","title":"Quasi-likelihood Objectives","text":"","category":"section"},{"location":"api_problems/#OptimizationMethods.QLLogisticSin","page":"Problems","title":"OptimizationMethods.QLLogisticSin","text":"QLLogisticSin{T, S} <: AbstractDefaultQL{T, S}\n\nImplements a Quasi-likelihood objective with a logistic link function and     linear_plus_sin variance funtion. If the design matrix and the     responses are not supplied, they are randomly generated.\n\nObjective Function\n\nLet A be the design matrix, and b be the responses. Each row of A     and corresponding entry in b are the predictor and observations from one     unit. The statistical model for this objective function assums b     are between 0 and 1.\n\nLet A_i be row i of A and b_i entry i of b. Let\n\n    mu_i(x) = mathrmlogistic(A_i^intercal x)\n\nand\n\n    v_i(mu) = 1 + mu + sin(2 pi mu) \n\nLet n be the number of rows in A, then the quasi-likelihood objective is\n\n    F(x) = -sum_i=1^n int_0^mu_i(x) fracb_i - mv_i(m) dm\n\nwarn: Warn\nF(x) does not have an easily expressible closed form, so a numerical integration scheme is used to evaluate the objective. The gradient and hessian have closed form solutions however.\n\nFields\n\nmeta::NLPModelMeta{T, S}, NLPModel struct for storing meta information for    the problem\ncounters::Counters, NLPModel Counter struct that provides evaluations    tracking.\ndesign::Matrix{T}, covariate matrix for the problem/experiment (A).\nresponse::Vector{T}, observations for the problem/experiment (b).\nmean::Function, component-wise function of the linear predictor Ax    to predict expected value of the response.\nmean_first_derivative::Function, function for the first derivative of the    mean function.\nmean_second_derivative::Function, function for the second derivative of the   mean funciton.\nvariance::Function, variance function that estimates the variance of each   response.\nvariance_first_derivative::Function, function that returns the first    derivative of the variance function.\nweighted_residual::Function, computes the weighted residual of the model. \n\nConstructors\n\nInner Constructors\n\nQLLogisticSin{T, S}(meta::NLPModelMeta{T, S}, counters::Counters,\n    design::Matrix{T}, response::Vector{T})\n\nInitializes the data structure for a quasi-likelihood estimation problem with      a logistic link function and      a linear plus sine variance function.\n\nOuter Constructors\n\nQLLogisticSin(::Type{T}; nobs::Int64 = 1000,\n    nvar::Int64 = 50) where {T}\n\nConstruct a quasi-likelihood estimation problem with      a logistic link function;      a linear plus sine variance function;     and a randomly generated design matrix and response vector that is consistent      with the quasi-likelihood model.      The design matrix has a column of all ones and the rest generated from a      normal distribution with its entries divided by sqrt(nvar - 1).      The response vector has entries generated by      mean.(design * x) + variance.(mean.(design * x)) .^ (.5) * ϵ, where ϵ is a noise     vector generated from the Arcsine distribution with default parameters.\n\nQLLogisticSin(design::Matrix{T}, response::Vector{T}; \n    x0::Vector{T} = zeros(T, size(design)[2])) where {T}\n\nConstructs a quasi-likelihood estimation problem with      a logistic link function;      a linear plus sine variance function;     and user-supplied design matrix and response vector.\n\n\n\n\n\n","category":"type"},{"location":"api_problems/#OptimizationMethods.QLLogisticCenteredExp","page":"Problems","title":"OptimizationMethods.QLLogisticCenteredExp","text":"QLLogisticCenteredExp{T, S} <: AbstractDefaultQL{T, S}\n\nImplements a Quasi-likelihood objective with a logistic link function and     centered_exp variance funtion. If the design matrix and the     responses are not supplied, they are randomly generated.\n\nObjective Function\n\nLet A be the design matrix, and b be the responses. Each row of A     and corresponding entry in b are the predictor and observations from one     unit. The statistical model for this objective function assums b     are between 0 and 1.\n\nLet A_i be row i of A and b_i entry i of b. Let\n\n    mu_i(x) = mathrmlogistic(A_i^intercal x)\n\nand\n\n    v_i(mu) = expleft( -mu - c^2p right) \n\nwhere p in mathbbR and c in mathbbR.\n\nnote: Note\nFor the variance to be differentiable everywhere, p  5.\n\nLet n be the number of rows in A, then the quasi-likelihood objective is\n\n    F(x) = -sum_i=1^n int_0^mu_i(x) fracb_i - mv_i(m) dm\n\nwarn: Warn\nF(x) does not have an easily expressible closed form, so a numerical integration scheme is used to evaluate the objective. The gradient and hessian have closed form solutions however.\n\nFields\n\nmeta::NLPModelMeta{T, S}, NLPModel struct for storing meta information for    the problem\ncounters::Counters, NLPModel Counter struct that provides evaluations    tracking.\ndesign::Matrix{T}, covariate matrix for the problem/experiment (A).\nresponse::Vector{T}, observations for the problem/experiment (b).\nmean::Function, component-wise function of the linear predictor Ax    to predict expected value of the response.\nmean_first_derivative::Function, function for the first derivative of the    mean function.\nmean_second_derivative::Function, function for the second derivative of the   mean funciton.\nvariance::Function, variance function that estimates the variance of each   response.\nvariance_first_derivative::Function, function that returns the first    derivative of the variance function.\nweighted_residual::Function, computes the weighted residual of the model. \np::T, parameter of the variance function saved for testing\nc::T, parameter of the variance function saved for testing\n\nConstructors\n\nInner Constructors\n\nQLLogisticCenteredExp{T, S}(meta::NLPModelMeta{T, S}, counters::Counters,\n    design::Matrix{T}, response::Vector{T}, p::T, c::T)\n\nInitializes the data structure for a quasi-likelihood estimation problem with      a logistic link function and      a centered exponential variance function.\n\nOuter Constructors\n\nQLLogisticCenteredExp(::Type{T}; nobs::Int64 = 1000, nvar::Int64 = 50,\n    p::T = T(1), c::T = T(1)) where {T}\n\nConstruct a quasi-likelihood estimation problem with      a logistic link function;      a centered exponential variance function;     and a randomly generated design matrix and response vector that is consistent      with the quasi-likelihood model.      The design matrix has a column of all ones and the rest generated from a      normal distribution with its entries divided by sqrt(nvar - 1).      The response vector has entries generated by      mean.(design * x) + variance.(mean.(design * x)) .^ (.5) * ϵ, where ϵ is a noise     vector generated from the Arcsine distribution with default parameters.\n\nQLLogisticCenteredExp(design::Matrix{T}, response::Vector{T};\n    x0::Vector{T} = zeros(T, size(design)[2]), p::T = T(1), c::T = T(1)\n    ) where {T}\n\nConstructs a quasi-likelihood estimation problem with      a logistic link function;      a centered exponential variance function;     and user-supplied design matrix and response vector.\n\n\n\n\n\n","category":"type"},{"location":"api_problems/#OptimizationMethods.QLLogisticCenteredLog","page":"Problems","title":"OptimizationMethods.QLLogisticCenteredLog","text":"QLLogisticCenteredLog{T, S} <: AbstractDefaultQL{T, S}\n\nImplements a Quasi-likelihood objective with a logistic link function and     centered_shifted_log variance funtion. If the design matrix and the     responses are not supplied, they are randomly generated.\n\nObjective Function\n\nLet A be the design matrix, and b be the responses. Each row of A     and corresponding entry in b are the predictor and observations from one     unit. The statistical model for this objective function assums b     are between 0 and 1.\n\nLet A_i be row i of A and b_i entry i of b. Let\n\n    mu_i(x) = mathrmlogistic(A_i^intercal x)\n\nand\n\n    v_i(mu) = log(mu-c^2p + 1) + d\n\nwhere p in mathbbR, c in mathbbR, d in mathbbR. \n\nnote: Note\nThis function is only differentiable everywhere when p  5. For the function to be positive everywhere (a requirement to be a valid variance function), d  0.\n\nLet n be the number of rows in A, then the quasi-likelihood objective is\n\n    F(x) = -sum_i=1^n int_0^mu_i(x) fracb_i - mv_i(m) dm\n\nwarn: Warn\nF(x) does not have an easily expressible closed form, so a numerical integration scheme is used to evaluate the objective. The gradient and hessian have closed form solutions however.\n\nFields\n\nmeta::NLPModelMeta{T, S}, NLPModel struct for storing meta information for    the problem\ncounters::Counters, NLPModel Counter struct that provides evaluations    tracking.\ndesign::Matrix{T}, covariate matrix for the problem/experiment (A).\nresponse::Vector{T}, observations for the problem/experiment (b).\nmean::Function, component-wise function of the linear predictor Ax    to predict expected value of the response.\nmean_first_derivative::Function, function for the first derivative of the    mean function.\nmean_second_derivative::Function, function for the second derivative of the   mean funciton.\nvariance::Function, variance function that estimates the variance of each   response.\nvariance_first_derivative::Function, function that returns the first    derivative of the variance function.\nweighted_residual::Function, computes the weighted residual of the model. \np::T, parameter for the variance function for testing purposes\nc::T, parameter for the variance function for testing purposes\nd::T, parameter for the variance function for testing purposes\n\nConstructors\n\nInner Constructors\n\nQLLogisticCenteredLog{T, S}(meta::NLPModelMeta{T, S}, counters::Counters,\n    design::Matrix{T}, response::Vector{T}, p::T, c::T, d::T)\n\nInitializes the data structure for a quasi-likelihood estimation problem with      a logistic link function and      a centered shifted log variance function.\n\nOuter Constructors\n\nQLLogisticCenteredLog(::Type{T}; nobs::Int64 = 1000,\n    nvar::Int64 = 50, p::T = T(1), c::T = T(1), d::T = T(1)) where {T}\n\nConstruct a quasi-likelihood estimation problem with      a logistic link function;      a centered shifted log variance function;     and a randomly generated design matrix and response vector that is consistent      with the quasi-likelihood model.      The design matrix has a column of all ones and the rest generated from a      normal distribution with its entries divided by sqrt(nvar - 1).      The response vector has entries generated by      mean.(design * x) + variance.(mean.(design * x)).^(.5) * ϵ, where ϵ is a noise     vector generated from the Arcsine distribution with default parameters.\n\nQLLogisticCenteredLog(design::Matrix{T}, response::Vector{T}, \n    x0::Vector{T} = zeros(T, size(design)[2]), p::T = T(1),\n    c::T = T(1), d::T = T(1)) where {T}\n\nConstructs a quasi-likelihood estimation problem with      a logistic link function;      a centered shifted log variance function;     and user-supplied design matrix and response vector.\n\n\n\n\n\n","category":"type"},{"location":"api_problems/#OptimizationMethods.QLLogisticMonomial","page":"Problems","title":"OptimizationMethods.QLLogisticMonomial","text":"QLLogisticMonomial{T, S} <: AbstractDefaultQL{T, S}\n\nImplements a quasi-likelihood objective with a logistic link function and the     variance function monomial_plus_constant. If the design matrix and the     responses are not supplied, they are randomly generated.\n\nObjective Function\n\nLet A be the design matrix, and b be the responses. Each row of A     and corresponding entry in b are the predictor and observations from one     unit. The statistical model for this objective function assums b     are between 0 and 1.\n\nLet A_i be row i of A and b_i entry i of b. Let\n\n    mu_i(x) = mathrmlogistic(A_i^intercal x)\n\nand\n\n    v_i(mu) = mu^2p + c \n\nfor p in mathbbR and c in mathbbR.\n\nnote: Note\nFor this to be a valid variance function that is differentiable everywhere, p  5 and c  0.\n\nLet n be the number of rows in A, then the quasi-likelihood objective is\n\n    F(x) = -sum_i=1^n int_0^mu_i(x) fracb_i - mv_i(m) dm\n\nwarn: Warn\nF(x) does not have an easily expressible closed form, so a numerical integration scheme is used to evaluate the objective. The gradient and hessian have closed form solutions however.\n\nFields\n\nmeta::NLPModelMeta{T, S}, NLPModel struct for storing meta information for    the problem\ncounters::Counters, NLPModel Counter struct that provides evaluations    tracking.\ndesign::Matrix{T}, covariate matrix for the problem/experiment (A).\nresponse::Vector{T}, observations for the problem/experiment (b).\nmean::Function, component-wise function of the linear predictor Ax    to predict expected value of the response.\nmean_first_derivative::Function, function for the first derivative of the    mean function.\nmean_second_derivative::Function, function for the second derivative of the   mean funciton.\nvariance::Function, variance function that estimates the variance of each   response.\nvariance_first_derivative::Function, function that returns the first    derivative of the variance function.\nweighted_residual::Function, computes the weighted residual of the model.\np::T, parameter of the variance function for testing purposes.\nc::T, parameter of the variance function for testing purposes.\n\nConstructors\n\nInner Constructors\n\nQLLogisticMonomial{T, S}(meta::NLPModelMeta{T, S}, counters::Counters,\n    design::Matrix{T}, response::Vector{T}, p::T, c::T) where {T, S}\n\nInitialize the data structure for a quasi-likelihood estimation problem with     a logistic link function and     a monomial plus constant variance function.\n\nOuter Constructors\n\nQLLogisticMonomial(::Type{T}; nobs::Int64 = 1000, nvar::Int64 = 50,\n    p::T = T(1), c::T = T(1)) where {T}\n\nConstruct a quasi-likelihood estimation problem with      a logistic link function;      a monomial plus constant variance function;     and a randomly generated design matrix and response vector that is consistent      with the quasi-likelihood model.      The design matrix has a column of all ones and the rest generated from a      normal distribution with its entries divided by sqrt(nvar - 1).      The response vector has entries generated by      mean.(design * x) + variance.(mean.(design * x))^(.5) * ϵ, where ϵ is a noise     vector generated from the Arcsine distribution with default parameters.\n\nQLLogisticMonomial(design::Matrix{T}, response::Vector{T};\n    x0::Vector{T} = zeros(T, size(design)[2]), p::T = T(1),\n    c::T = T(1)) where {T}\n\nConstructs a quasi-likelihood estimation problem with      a logistic link function;      a monomial plus constant variance function;     and user-supplied design matrix and response vector.\n\n\n\n\n\n","category":"type"},{"location":"api_problems/#Problem-Utility","page":"Problems","title":"Problem Utility","text":"","category":"section"},{"location":"api_problems/#Regression-Link-Functions","page":"Problems","title":"Regression Link Functions","text":"","category":"section"},{"location":"api_problems/#OptimizationMethods.logistic","page":"Problems","title":"OptimizationMethods.logistic","text":"logistic(η::T} where {T}\n\nImplements\n\n mathrmlogistic(eta) = frac11 + exp(-eta)\n\nwhere T is a scalar type.\n\nArguments\n\nη::T, scalar. In the regression context this is the linear effect.\n\n\n\n\n\n","category":"function"},{"location":"api_problems/#OptimizationMethods.inverse_complimentary_log_log","page":"Problems","title":"OptimizationMethods.inverse_complimentary_log_log","text":"inverse_complimentary_log_log(η::T) where {T}\n\nImplements the link function\n\n    mathrmCLogLog^-1(eta) = 1 - exp(-exp(eta))\n\nArguments\n\nη::T, scalar. In the regression context this is the linear effect.\n\n\n\n\n\n","category":"function"},{"location":"api_problems/#OptimizationMethods.inverse_probit","page":"Problems","title":"OptimizationMethods.inverse_probit","text":"inverse_probit(η::T) where {T}\n\nImplements the link function\n\n    Phi(eta) = frac12pi exp(-5eta^2)\n\nArguments\n\nη::T, scalar. In the regression context this is the linear effect.\n\n\n\n\n\n","category":"function"},{"location":"api_problems/#Regression-Link-Function-Derivatives","page":"Problems","title":"Regression Link Function Derivatives","text":"","category":"section"},{"location":"api_problems/#OptimizationMethods.dlogistic","page":"Problems","title":"OptimizationMethods.dlogistic","text":"dlogistic(η::T) where {T}\n\nFirst derivative of the logistic function. Implements\n\n    fracddeta  mathrmlogistic(eta) = fracexp(-eta)(1+exp(-eta))^2\n\nwhere T is a scalar type.\n\nArguments\n\nη::T, scalar. In the regression context this is the linear effect.\n\n\n\n\n\n","category":"function"},{"location":"api_problems/#OptimizationMethods.ddlogistic","page":"Problems","title":"OptimizationMethods.ddlogistic","text":"ddlogistic(η::T) where {T}\n\nDouble derivative of the logistic function. Implements\n\n    fracdd^2eta mathrmlogistic(eta) = \n    frac2exp(-2eta)(1+exp(-eta))^3 -\n    fracexp(-eta)(1+exp(-eta))^2\n\nwhere T is a scalar type.\n\nArguments\n\nη::T, scalar. In the regression context this is the linear effect.\n\n\n\n\n\n","category":"function"},{"location":"api_problems/#Regression-Variance-Functions","page":"Problems","title":"Regression Variance Functions","text":"","category":"section"},{"location":"api_problems/#OptimizationMethods.monomial_plus_constant","page":"Problems","title":"OptimizationMethods.monomial_plus_constant","text":"monomial_plus_constant(μ::T, p::T, c::T) where {T}\n\nImplements the variance function\n\n    V(mu) = (mu^2)^p + c\n\nSee Quasi-likelihood Estimation for details.\n\nArguments\n\nμ::T, scalar. In the regression context, this is the estimated mean of a    datapoint.\np::T, scalar. Power applied to μ^2.\nc::T, scalar. Translate the effect on variance by c.\n\nnote: Note\nFor a negative c this might not be a valid variance function. For p smaller than 5, the variance function is not continuously  differentiable at 0.\n\n\n\n\n\n","category":"function"},{"location":"api_problems/#OptimizationMethods.linear_plus_sin","page":"Problems","title":"OptimizationMethods.linear_plus_sin","text":"linear_plus_sin(μ::T) where {T}\n\nImplements the variance function \n\n    V(mu) = 1 + mu + mathrmsin(2pimu)\n\nSee Quasi-likelihood Estimation for details.\n\nArguments\n\nμ::T, scalar. In the regression context, this is the estimated mean of a    datapoint.\n\n\n\n\n\n","category":"function"},{"location":"api_problems/#OptimizationMethods.centered_exp","page":"Problems","title":"OptimizationMethods.centered_exp","text":"centered_exp(μ::T, p::T, c::T) where {T}\n\nImplements the variance function\n\n    V(mu) = expleft( -mu - c^2p right)\n\nSee Quasi-likelihood Estimation for details.\n\nArguments\n\nμ::T, scalar. In the regression context, this is the estimated mean of a    datapoint.\np::T, scalar. Power applied to (μ-c)^2.\nc::T, scalar. Center where noise level is highest.\n\nnote: Note\nFor p smaller than 5, the variance function is not continuously  differentiable at c.\n\n\n\n\n\n","category":"function"},{"location":"api_problems/#OptimizationMethods.centered_shifted_log","page":"Problems","title":"OptimizationMethods.centered_shifted_log","text":"centered_shifted_log(μ::T, p::T, c::T, d::T) where {T}\n\nImplements the variance function\n\n    V(mu) = log(mu-c^2p + 1) + d\n\nSee Quasi-likelihood Estimation for details.\n\nArguments\n\nμ::T, scalar. In the regression context, this is the estimated mean of a    datapoint.\np::T, scalar. Power applied to |\\mu-c|^2.\nc::T, scalar. Center where noise level is lowest.\nd::T, scalar. Irreducible variance. Corresponds to the minimum variance.   Must be positive.\n\nnote: Note\nFor p smaller than or equal to 5, the variance function is not  continuously differentiable at c. When d <= 0, the variance function is not well-defined, that is it can take on negative and zero values.\n\n\n\n\n\n","category":"function"},{"location":"api_problems/#Regression-Variance-Function-Derivatives","page":"Problems","title":"Regression Variance Function Derivatives","text":"","category":"section"},{"location":"api_problems/#OptimizationMethods.dlinear_plus_sin","page":"Problems","title":"OptimizationMethods.dlinear_plus_sin","text":"dlinear_plus_sin(μ::T) where {T}\n\nCompute the following function\n\n    fracddmu (1 + mu + sin(2 pi mu)) = 1 + 2 pi cos(2pi mu)\n\nArguments\n\nμ::T, point at which to compute the derivative. In the context of    regression, this is the mean.\n\n\n\n\n\n","category":"function"},{"location":"api_problems/#OptimizationMethods.dcentered_exp","page":"Problems","title":"OptimizationMethods.dcentered_exp","text":"dcentered_exp(μ::T, p::T, c::T) where {T}\n\nCompute the following function\n\n    fracddmu expleft( -mu - c^2p right)\n\nwhere c in mathbbR and p in mathbbR. See Quasi-likelihood Estimation for details.\n\nwarning: Warning\nFor p smaller than 5, the derivative is not well-defined at c.\n\nArguments\n\nμ::T, scalar. In the regression context, this is the estimated mean of a    datapoint.\np::T, scalar. Power applied to (μ-c)^2.\nc::T, scalar. Center where noise level is highest.\n\n\n\n\n\n","category":"function"},{"location":"api_problems/#OptimizationMethods.dcentered_shifted_log","page":"Problems","title":"OptimizationMethods.dcentered_shifted_log","text":"dcentered_shifted_log(μ::T, p::T, c::T) where {T}\n\nCompute and returns the following function\n\n    fracddmu log(mu-c^2p + 1) =\n    sign(mu - c) frac2pmu - c^2p-1mu-c^2p + 1\n\nwhere sign(x) returns the sign of x. This equality is only  correct everywhere when p  5.\n\nwarning: Warning\nThe function does not check the correctness of p and is not guaranteed to return the correct derivative at c when p <= .5.\n\nArguments\n\nμ::T, point at which to compute the derivative. In the context of    regression, this is the mean.\np::T, scalar. Power applied to mu-c^2.\nc::T, scalar. Center where noise level is lowest.\n\n\n\n\n\n","category":"function"},{"location":"api_problems/#OptimizationMethods.dmonomial_plus_constant","page":"Problems","title":"OptimizationMethods.dmonomial_plus_constant","text":"dmonomial_plus_constant(μ::T, p::T) where {T}\n\nCompute the following function\n\n    fracddmu (mu^2p + c) = 2p mu^2p-1\n\nwarning: Warning\nThe derivative above is only correct everywhere when p is larger than  .5. We do not guarantee correctness if p is smaller than or equal to  .5. User beware.\n\n\n\n\n\n","category":"function"},{"location":"api_problems/#Index","page":"Problems","title":"Index","text":"","category":"section"},{"location":"api_problems/","page":"Problems","title":"Problems","text":"","category":"page"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"Armijo, L. (1966). Minimization of functions having lipschitz continuous first partial derivatives. Pacific Journal of Mathematics.\n\n\n\nBarzilai, J. and Borwein, J. M. (1988). Two-Point Step Size Gradient Methods. IMA Journal of Numerical Analysis 8.\n\n\n\nBertsekas, D. (2016). Nonlinear Optimization (Athena Scientific).\n\n\n\nGrippo, L.; Lampariello, F. and Lucidi, S. (1986). A Nonmonotone Line Search Technique for Newton's Method. Society for Industrial and Applied Mathematics.\n\n\n\nLanteri, A.; Leorato, S.; Lopez-Fidalgo, J. and Tommasi, C. (2023-02-24). Designing to Detect Heteroscedasticity in a Regression Model. Journal of the Royal Statistical Society 85, 315–326.\n\n\n\nLi, H.; Qian, J.; Tian, Y.; Rakhlin, A. and Jadbabaie, A. (2023-06). Convex and Non-convex Optimization Under Generalized Smoothness. Advances in Neural Information Processing Systems 36.\n\n\n\nMalitsky, Y. and Mishchenko, K. (2020-11-21). Adaptive Gradient Descent without Descent. In: Proceedings of the 37th International Conference on Machine Learning (PMLR); pp. 6702–6712.\n\n\n\nNesterov, Y. (1983). A Method for Solving the Convex Programming Problem with Convergence Rate O(1/K2). Proceedings of the USSR Academy of Sciences 269, 543–547.\n\n\n\nNocedal, J. and Wright, S. (2006). Numerical Optimization. 2 Edition, Springer Series in Operations Research and Financial Engineering (Springer New York, NY).\n\n\n\nPatel, V. and Berahas, A. (2024). Gradient Descent in the Absence of Global Lipschitz Continuity of the Gradients. SIAM 6, 579–846.\n\n\n\nWedderburn, R. W. (1974). Quasi-Likelihood Functions, Generalized Linear Models, and the Gauss—Newton Method. Biometrika 61, 439–447.\n\n\n\n","category":"page"},{"location":"api_methods/#Contents","page":"Methods","title":"Contents","text":"","category":"section"},{"location":"api_methods/","page":"Methods","title":"Methods","text":"Pages=[\"api_methods.md\"]","category":"page"},{"location":"api_methods/#Barzilai-Borwein-Method","page":"Methods","title":"Barzilai Borwein Method","text":"","category":"section"},{"location":"api_methods/#OptimizationMethods.barzilai_borwein_gd","page":"Methods","title":"OptimizationMethods.barzilai_borwein_gd","text":"barzilai_borwein_gd(optData::BarzilaiBorweinGD{T}, progData::P \n    where P <: AbstractNLPModel{T, S}) where {T,S}\n\nImplements gradient descent with Barzilai-Borwein step size and applies the      method to the optimization problem specified by progData. \n\nReference(s)\n\nBarzilai and Borwein. \"Two-Point Step Size Gradient Methods\". IMA Journal of      Numerical Analysis.\n\nMethod\n\nGiven iterates lbrace x_0ldotsx_krbrace, the iterate x_k+1     is equal to x_k - alpha_k nabla f(x_k), where alpha_k is     one of two versions.\n\nLong Step Size Version (if optData.long_stepsize==true)\n\nIf k=0, then alpha_0 is set to optData.init_stepsize. For k0,\n\nalpha_k = frac Vert x_k - x_k-1 Vert_2^2(x_k - x_k-1)^intercal \n    (nabla f(x_k) - nabla f(x_k-1))\n\nShort Step Size Version (if optData.long_stepsize==false)\n\nIf k=0, then alpha_0 is set to optData.init_stepsize. For k0,\n\nalpha_k = frac(x_k - x_k-1)^intercal (nabla f(x_k) - \n    nabla f(x_k-1))Vert nabla f(x_k) - nabla f(x_k-1)Vert_2^2\n\nArguments\n\noptData::BarzilaiBorweinGD{T}, the specification for the optimization method.\nprogData<:AbstractNLPModel{T,S}, the specification for the optimization   problem. \n\nwarning: Warning\nprogData must have an initialize function that returns subtypes of AbstractPrecompute and AbstractProblemAllocate, where the latter has a grad argument.\n\n\n\n\n\n","category":"function"},{"location":"api_methods/#OptimizationMethods.BarzilaiBorweinGD","page":"Methods","title":"OptimizationMethods.BarzilaiBorweinGD","text":"BarzilaiBorweinGD{T} <: AbstractOptimizerData{T}\n\nA structure for storing data about gradient descent using the Barzilai-Borwein      step size, and the progress of its application on an optimization problem.\n\nFields\n\nname:String, name of the solver for reference.\ninit_stepsize::T, initial step size to start the method. \nlong_stepsize::Bool, flag for step size; if true, use the long version of    Barzilai-Borwein. If false, use the short version. \nthreshold::T, gradient threshold. If the norm gradient is below this, then    iteration stops.\nmax_iterations::Int64, max number of iterations (gradient steps) taken by    the solver.\niter_diff::Vector{T}, a buffer for storing differences between subsequent   iterate values that are used for computing the step size\ngrad_diff::Vector{T}, a buffer for storing differences between gradient    values at adjacent iterates, which is used to compute the step size\niter_hist::Vector{Vector{T}}, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration 0). The k+1 entry   corresponds to the iterate at iteration k.\ngrad_val_hist::Vector{T}, a vector for storing max_iterations+1 gradient   norm values. The first entry corresponds to iteration 0. The k+1 entry   correpsonds to the gradient norm at iteration k.\nstop_iteration::Int64, the iteration number that the solver stopped on.   The terminal iterate is saved at iter_hist[stop_iteration+1].\n\nConstructors\n\nBarzilaiBorweinGD(::Type{T}; x0::Vector{T}, init_stepsize::T, \n    long_stepsize::Bool, threshold::T, max_iterations::Int) where {T}\n\nConstructs the struct for the Barzilai-Borwein optimization method\n\nArguments\n\nT::DataType, specific data type used for calculations.\n\nKeyword Arguments\n\nx0::Vector{T}, initial point to start the solver at.\ninit_stepsize::T, initial step size used for the first iteration. \nlong_stepsize::Bool, flag for step size; if true, use the long version of   Barzilai-Borwein, if false, use the short version. \nthreshold::T, gradient threshold. If the norm gradient is below this,    then iteration is terminated. \nmax_iterations::Int, max number of iterations (gradient steps) taken by    the solver.\n\n\n\n\n\n","category":"type"},{"location":"api_methods/#Gradient-Descent-with-Fixed-Step-Size","page":"Methods","title":"Gradient Descent with Fixed Step Size","text":"","category":"section"},{"location":"api_methods/#OptimizationMethods.fixed_step_gd","page":"Methods","title":"OptimizationMethods.fixed_step_gd","text":"fixed_step_gd(optData::FixedStepGD{T}, progData<:AbstractNLPModel{T,S})\n    where {T,S}\n\nImplements fixed step-size gradient descent for the desired optimization problem     specified by progData.\n\nMethod\n\nThe iterates are updated according to the procedure\n\nx_k+1 = x_k - alpha f(x_k)\n\nwhere alpha is the step size, f is the objective function, and f is the      gradient function of f. \n\nArguments\n\noptData::FixedStepGD{T}, the specification for the optimization method.\nprogData<:AbstractNLPModel{T,S}, the specification for the optimization   problem. \n\nwarning: Warning\nprogData must have an initialize function that returns subtypes of AbstractPrecompute and AbstractProblemAllocate, where the latter has a grad argument. \n\n\n\n\n\n","category":"function"},{"location":"api_methods/#OptimizationMethods.FixedStepGD","page":"Methods","title":"OptimizationMethods.FixedStepGD","text":"FixedStepGD{T} <: AbstractOptimizerData{T}\n\nA structure for storing data about fixed step-size gradient descent, and the     progress of its application on an optimization problem.\n\nFields\n\nname::String, name of the solver for reference\nstep_size::T, the step-size selection for the optimization procedure\nthreshold::T, the threshold on the norm of the gradient to induce stopping\nmax_iterations::Int, the maximum allowed iterations\niter_hist::Vector{Vector{T}}, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration 0). The k+1 entry   corresponds to the iterate at iteration k.\ngrad_val_hist::Vector{T}, a vector for storing max_iterations+1 gradient   norm values. The first entry corresponds to iteration 0. The k+1 entry   correpsonds to the gradient norm at iteration k\nstop_iteration, iteration number that the algorithm stopped at.   Iterate number stop_iteration is produced. \n\nConstructors\n\nFixedStepGD(::Type{T}; x0::Vector{T}, step_size::T, threshold::T, \n    max_iterations::Int) where {T}\n\nConstructs the struct for the optimizer.\n\nArguments\n\nT::DataType, specific data type for the calculations\n\nKeyword Arguments\n\nx0::Vector{T}, the initial iterate for the optimizers\nstep_size::T, the step size of the optimizer \nthreshold::T, the threshold on the norm of the gradient to induce stopping\nmax_iterations::Int, the maximum number of iterations allowed  \n\n\n\n\n\n","category":"type"},{"location":"api_methods/#Lipschitz-Approximation-(Malitsky-and-Mishchenko)","page":"Methods","title":"Lipschitz Approximation (Malitsky & Mishchenko)","text":"","category":"section"},{"location":"api_methods/#OptimizationMethods.lipschitz_approximation_gd","page":"Methods","title":"OptimizationMethods.lipschitz_approximation_gd","text":"lipschitz_approximation_gd(optData::FixedStepGD{T}, progData::P where P \n    <: AbstractNLPModel{T, S}) where {T, S}\n\nImplements gradient descent with adaptive step sizes formed through a lipschitz      approximation for the desired optimization problem specified by progData.\n\nwarning: Warning\nThis method is designed for convex optimization problems.\n\nReferences(s)\n\nMalitsky, Y. and Mishchenko, K. (2020). \"Adaptive Gradient Descent without      Descent.\"      Proceedings of the 37th International Conference on Machine Learning,      in Proceedings of Machine Learning Research 119:6702-6712.     \n\nMethod\n\nThe iterates are updated according to the procedure,\n\nx_k+1 = x_k - alpha_k nabla f(x_k)\n\nwhere alpha_k is the step size and nabla f is the gradient function      of the objective function f.\n\nThe step size is computed depending on k.      When k = 0, the step size is set to optData.init_stepsize.      When k  0, \n\nalpha_k = minleftlbrace sqrt1 + theta_k-1alpha_k-1 \n    fracVert x_k - x_k-1 VertVert nabla f(x_k) - \n    nabla f(x_k-1)Vert rightrbrace\n\nwhere theta_0 = inf and theta_k = alpha_k  alpha_k-1.\n\nArguments\n\noptData::LipschitzApproxGD{T}, the specification for the optimization method.\nprogData<:AbstractNLPModel{T,S}, the specification for the optimization   problem. \n\nwarning: Warning\nprogData must have an initialize function that returns subtypes of AbstractPrecompute and AbstractProblemAllocate, where the latter has a grad argument. \n\n\n\n\n\n","category":"function"},{"location":"api_methods/#OptimizationMethods.LipschitzApproxGD","page":"Methods","title":"OptimizationMethods.LipschitzApproxGD","text":"LipschitzApproxGD{T} <: AbstractOptimizerData{T}\n\nA structure for storing data about adaptive gradient descent     using a Lipschitz Approximation scheme (AdGD), and the progress      of its application on an optimization problem.\n\nFields\n\nname::String, name of the solver for reference\ninit_stepsize::T, the initial step size for the method\nprev_stepsize::T, step size used at iter - 1 when iter > 1.\ntheta::T, element used in the computation of the step size. See the    referenced paper for more information.\nlipschitz_approximation::T, help the lipschitz approximation used in the   computation of the step size. See the referenced paper for more information.\nthreshold::T, the threshold on the norm of the gradient to induce stopping\nmax_iterations::Int64, the maximum allowed iterations\niter_diff::Vector{T}, a buffer for storing differences between subsequent   iterate values that are used for computing the step size\ngrad_diff::Vector{T}, a buffer for storing differences between gradient    values at adjacent iterates, which is used to compute the step size\niter_hist::Vector{Vector{T}}, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration 0). The k+1 entry   corresponds to the iterate at iteration k.\ngrad_val_hist::Vector{T}, a vector for storing max_iterations+1 gradient   norm values. The first entry corresponds to iteration 0. The k+1 entry   corresponds to the gradient norm at iteration k\nstop_iteration::Int64, iteration number that the algorithm stopped at.   Iterate number stop_iteration is produced. \n\nConstructors\n\nLipschitzApproxGD(::Type{T}; x0::Vector{T}, init_stepsize::T, threshold::T, \n    max_iterations::Int) where {T}\n\nConstructs the struct for the optimizer.\n\nArguments\n\nT::DataType, specific data type for the calculations\n\nKeyword Arguments\n\nx0::Vector{T}, the initial iterate for the optimizers\ninit_stepsize::T, the initial step size for the method\nthreshold::T, the threshold on the norm of the gradient to induce stopping\nmax_iterations::Int, the maximum number of iterations allowed  \n\n\n\n\n\n","category":"type"},{"location":"api_methods/#Weighted-Norm-Damping-Gradient-Method-(WNGrad)","page":"Methods","title":"Weighted Norm Damping Gradient Method (WNGrad)","text":"","category":"section"},{"location":"api_methods/#OptimizationMethods.weighted_norm_damping_gd","page":"Methods","title":"OptimizationMethods.weighted_norm_damping_gd","text":"weighted_norm_damping_gd(optData::WeightedNormDampingGD{T}, \n    progData::P where P <: AbstractNLPModel{T, S}) where {T, S}\n\nMethod that implements gradient descent with weighted norm damping step size      using the specifications in optData on the problem specified by progData.\n\nReference\n\nWu, Xiaoxia et. al. \"WNGrad: Learn the Learning Rate in Gradient Descent\". arxiv,      https://arxiv.org/abs/1803.02865\n\nMethod\n\nLet theta_k be the k^th iterate, and alpha_k be the k^th      step size. The optimization method generate iterates following\n\ntheta_k + 1 = theta_k - alpha_k nabla f(theta_k)\n\nwhere nabla f is the gradient of the objective function f.\n\nThe step size depends on the iteration number k. For k = 0, the step      size alpha_0 is the reciprocal of optData.init_norm_damping_factor.      For k  0, the step size is iteratively updated as\n\nalpha_k = left\nfrac1alpha_k-1 + Vertdot F(theta_k)Vert_2^2 alpha_k-1\nright^-1\n\nwarning: Warning\nWhen alpha_0  Vertdot F(theta_0)Vert_2^-1 and a globally Lipschitz smooth objective function is used, then the method is guaranteed to find an epsilon-stationary point. It is recommended then that  optData.init_norm_damping_factor exceed Vertdot F(theta_0)Vert_2.\n\nArguments\n\noptData::WeightedNormDampingGD{T}, specification for the optimization algorithm.\nprogData::P where P <: AbstractNLPModel{T, S}, specification for the problem.\n\nwarning: Warning\nprogData must have an initialize function that returns subtypes of AbstractPrecompute and AbstractProblemAllocate, where the latter has a grad argument. \n\n\n\n\n\n","category":"function"},{"location":"api_methods/#OptimizationMethods.WeightedNormDampingGD","page":"Methods","title":"OptimizationMethods.WeightedNormDampingGD","text":"WeightedNormDampingGD{T} <: AbstractOptimizerData{T}\n\nA mutable struct that represents gradient descent using the weighted-norm      damping step size. It stores the specification for the method and records      values during iteration.\n\nFields\n\nname::String, name of the optimizer for recording purposes\ninit_norm_damping_factor::T, initial damping factor. This value's reciprocal    will be the initial step size.\nthreshold::T, norm gradient tolerance condition. Induces stopping when norm    is at most threshold.\nmax_iterations::Int64, max number of iterates that are produced, not    including the initial iterate.\niter_hist::Vector{Vector{T}}, store the iterate sequence as the algorithm    progresses. The initial iterate is stored in the first position.\ngrad_val_hist::Vector{T}, stores the norm gradient values at each iterate.    The norm of the gradient evaluated at the initial iterate is stored in the    first position.\nstop_iteration::Int64, the iteration number the algorithm stopped on. The    iterate that induced stopping is saved at iter_hist[stop_iteration + 1].\n\nConstructors\n\nWeightedNormDampingGD(::Type{T}; x0::Vector{T}, init_norm_damping_factor::T, \n    threshold::T, max_iterations::Int64) where {T}\n\nConstructs an instance of type WeightedNormDampingGD{T}.\n\nArguments\n\nT::DataType, type for data and computation\n\nKeyword Arguments\n\nx0::Vector{T}, initial point to start the optimization routine. Saved in   iter_hist[1].\ninit_norm_damping_factor::T, initial damping factor, which will correspond   to the reciprocoal of the initial step size. \nthreshold::T, norm gradient tolerance condition. Induces stopping when norm    at most threshold.\nmax_iterations::Int64, max number of iterates that are produced, not    including the initial iterate.\n\n\n\n\n\n","category":"type"},{"location":"api_methods/#Nesterov's-Accelerated-Gradient-Descent","page":"Methods","title":"Nesterov's Accelerated Gradient Descent","text":"","category":"section"},{"location":"api_methods/#OptimizationMethods.nesterov_accelerated_gd","page":"Methods","title":"OptimizationMethods.nesterov_accelerated_gd","text":"nesterov_accelerated_gd(optData::NesterovAcceleratedGD{T}, progData::P\n    where P <: AbstractNLPModel{T, S}) where {T, S}\n\nImplements Nesterov's Accelerated Gradient Descent as specified by optData on the problem specified by progData.\n\nwarning: Warning\nThis algorithm is designed for convex problems.\n\nReference(s)\n\nSee Algorithm 1 of Li et. al. \"Convex and Non-convex Optimization Under    Generalized Smoothness\". arxiv, https://arxiv.org/abs/2306.01264.\nSee line-search based approach in Nesterov, Yurii. 1983. “A Method for    Solving the Convex Programming Problem    with Convergence Rate O(1/K^2).” Proceedings of the USSR Academy of Sciences    269:543–47.\n\nMethod\n\nLet the objective function be denoted by F(theta) and nabla F(     theta) denote its gradient function. Given theta_0 and  a step      size alpha (equal to optData.step_size), the method produces five      sequences. At k=0,\n\nbegincases\n    B_0 = 0 \n    z_0 = theta_0 \n    Delta_0  = 1 \n    y_0 = theta_0\nendcases\n\nand, for kinmathbbN,\n\nbegincases\n    B_k = B_k-1 + Delta_k-1 \n    theta_k = y_k-1 - alpha nabla F(y_k-1) \n    z_k = z_k-1 - alphaDelta_k-1nabla F(y_k-1) \n    Delta_k = frac12left( 1 + sqrt4 B_k + 1  right) \n    y_k = theta_k + fracDelta_kB_k + Delta_k + alpha^-1 \n    (z_k - theta_k)\nendcases\n\nThe iterate sequence of interest is lbrace theta_k rbrace.\n\nArguments\n\noptData::NesterovAcceleratedGD{T}, the specification for the optimization    method.\nprogData<:AbstractNLPModel{T,S}, the specification for the optimization   problem.\n\nwarning: Warning\nprogData must have an initialize function that returns subtypes of AbstractPrecompute and AbstractProblemAllocate, where the latter has a grad argument.\n\n\n\n\n\n","category":"function"},{"location":"api_methods/#OptimizationMethods.NesterovAcceleratedGD","page":"Methods","title":"OptimizationMethods.NesterovAcceleratedGD","text":"NesterovAcceleratedGD{T} <: AbstractOptimizerData\n\nA structure that represents Nesterov Accelerated Gradient Descent. Stores variables related to the method, and tracks quantities as the algorithm progresses.\n\nFields\n\nname:String, name of the solver for reference.\nstep_size::T, step size used in the method. \nz::Vector{T}, buffer array for auxiliary iterate sequence\ny::Vector{T}, buffer array for convex combination of iterate and auxiliary   sequence \nB::T, auxiliary quadratic scaling term for computing acceleration weights    and step size.\nthreshold::T, gradient threshold. If the norm gradient is below this, then    iteration stops.\nmax_iterations::Int64, max number of iterations (gradient steps) taken by    the solver.\niter_hist::Vector{Vector{T}}, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration 0). The k+1 entry   corresponds to the iterate at iteration k.\ngrad_val_hist::Vector{T}, a vector for storing max_iterations+1 gradient   norm values. The first entry corresponds to iteration 0. The k+1 entry   corresponds to the gradient norm at iteration k.\nstop_iteration::Int64, the iteration number that the solver stopped on.   The terminal iterate is saved at iter_hist[stop_iteration+1].\n\nConstructors\n\n    NesterovAcceleratedGD(::Type{T}; x0::Vector{T}, step_size::T,\n        threshold::T, max_iterations::Int64) where {T}\n\nArguments\n\nT::DataType, specific data type used for calculations.\n\nKeyword Arguments\n\nx0::Vector{T}, initial point to start the solver at.\nstep_size::T, step size used in the method. \nthreshold::T, gradient threshold. If the norm gradient is below this,    then iteration is terminated. \nmax_iterations::Int, max number of iterations (gradient steps) taken by    the solver.\n\n\n\n\n\n","category":"type"},{"location":"api_methods/#Gradient-Descent-with-Diminishing-Step-Size","page":"Methods","title":"Gradient Descent with Diminishing Step Size","text":"","category":"section"},{"location":"api_methods/#OptimizationMethods.DiminishingStepGD","page":"Methods","title":"OptimizationMethods.DiminishingStepGD","text":"DiminishingStepGD{T} <: AbstractOptimizerData{T}\n\nA structure for storing data about gradient descent using diminishing step sizes,     and recording the progress of its application on an optimization problem.\n\nFields\n\nname::String, name of the solver for reference.\nstep_size_function::Function, step size function. Should take the iteration    number and return the step size for that iteration.\nstep_size_scaling::T, factor that is multipled with the amount of the step    size function.\nthreshold::T, gradient threshold. If the norm gradient is below this, then    iteration stops.\nmax_iterations::Int64, max number of iterations (gradient steps) taken by    the solver.\niter_hist::Vector{Vector{T}}, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration 0). The k+1 entry   corresponds to the iterate at iteration k.\ngrad_val_hist::Vector{T}, a vector for storing max_iterations+1 gradient   norm values. The first entry corresponds to iteration 0. The k+1 entry   correpsonds to the gradient norm at iteration k.\nstop_iteration::Int64, the iteration number that the solver stopped on.   The terminal iterate is saved at iter_hist[stop_iteration+1].\n\nwarning: Warning\nstep_size_function should take in two arguments, the data type for computation T and the iteration number. For example, calling step_size_function(Float64, 1) should return the step size as a Float64 for the iteration 1.\n\nConstructors\n\nDiminishingStepGD(::Type{T}; x0::Vector{T}, step_size_function::Function,\n    step_size_scaling::T, threshold::T, max_iterations::Int)\n\nConstructs the struct for the diminishing step size gradient descent method.\n\nArguments\n\nT::DataType, specific data type used for calculations.\n\nKeyword Arguments\n\nx0::Vector{T}, initial point to start the solver at.\nstep_size_function::Function, step size function. Should take the iteration    number and return the step size for that iteration.\nstep_size_scaling::T, factor that is multipled with the amount of the step    size function.\nthreshold::T, gradient threshold. If the norm gradient is below this,    then iteration is terminated. \nmax_iterations::Int, max number of iterations (gradient steps) taken by    the solver.\n\n\n\n\n\n","category":"type"},{"location":"api_methods/#OptimizationMethods.diminishing_step_gd","page":"Methods","title":"OptimizationMethods.diminishing_step_gd","text":"diminishing_step_gd(optData::DiminishingStepGD{T}, progData::P\n    where P <: AbstractNLPModel{T, S}) where {T, S}\n\nImplements gradient descent with diminishing step sizes and applies the method     to the optimization problem specified by progData.\n\nReference(s)\n\nPatel, Vivak, and Albert Berahas. “Gradient Descent in the Absence of Global      Lipschitz Continuity of the Gradients.” SIAM 6 (3): 579–846.      https://doi.org/10.1137/22M1527210.\n\nBertsekas, Dimitri. \"Nonlinear Programming\". 3rd Edition, Athena Scientific,      Chapter 1.\n\nMethod\n\nGiven iterates lbrace x_0ldotsx_krbrace, the iterate x_k + 1     is equal to x_k - alpha_k nabla f(x_k), where alpha_k is equal     to optData.step_size_scaling * optData.step_size_function(T, k).\n\ninfo: Step Size Function\nThe step size function should satisfy several conditions. First, alpha_k  0 for all k. Second, lim_k to infty alpha_k = 0 Finally, sum_k=0^infty alpha_k = infty See Patel and Berahas (2024) for details.\n\nArguments\n\noptData::DiminishingStepGD{T}, the specification for the optimization method.\nprogData<:AbstractNLPModel{T,S}, the specification for the optimization   problem. \n\nwarning: Warning\nprogData must have an initialize function that returns subtypes of AbstractPrecompute and AbstractProblemAllocate, where the latter has a grad argument.\n\n\n\n\n\n","category":"function"},{"location":"api_methods/","page":"Methods","title":"Methods","text":"Below is a list of step size functions that are in the library. The step size sequence generated by these functions, lbrace alpha_k rbrace  satisfies alpha_k  0, lim_k toinfty alpha_k = 0 and  sum_k=0^infty alpha_k = infty.","category":"page"},{"location":"api_methods/#OptimizationMethods.inverse_k_step_size","page":"Methods","title":"OptimizationMethods.inverse_k_step_size","text":"inverse_k_step_size(::Type{T}, k::Int64)\n\nReturn the inverse of k.\n\nMethod\n\nThe step size sequence generated for k in mathbbN is\n\n    alpha_k = frac1k+1\n\nwhen using this method.\n\nArguments\n\nT::Type, data type that the computations are done in.\nk::Int64, index of the step size needed. \n\nReturns\n\nReturns a number of type T.\n\n\n\n\n\n","category":"function"},{"location":"api_methods/#OptimizationMethods.inverse_log2k_step_size","page":"Methods","title":"OptimizationMethods.inverse_log2k_step_size","text":"inverse_log2k_step_size(::Type{T}, k::Int64) where {T}\n\nMethod\n\nThe step size sequence generated when using this method is\n\n    alpha_k = frac1lfloor log_2(k+1) + 1 rfloor\n\nfor k in mathbbN.\n\nArguments\n\nT::Type, data type that the computation are done in.\nk::Int64, index of the step size needed.\n\nReturns\n\nReturns a number of type T.\n\n\n\n\n\n","category":"function"},{"location":"api_methods/#OptimizationMethods.stepdown_100_step_size","page":"Methods","title":"OptimizationMethods.stepdown_100_step_size","text":"stepdown_100_step_size(::Type{T}, k::Int64) where {T}\n\nMethod\n\nThe step size sequence generated is\n\n    alpha_k = frac12^i\n\nfor k in  (2^i-1)100 (2^i+1-1)100).\n\nArguments\n\nT::Type, data type that the computation are done in.\nk::Int64, index of the step size needed.\n\nReturns\n\nReturns a number of type T.\n\n\n\n\n\n","category":"function"},{"location":"api_methods/#Gradient-Descent-with-Backtracking","page":"Methods","title":"Gradient Descent with Backtracking","text":"","category":"section"},{"location":"api_methods/#OptimizationMethods.backtracking_gd","page":"Methods","title":"OptimizationMethods.backtracking_gd","text":"backtracking_gd(optData::BacktrackingGD{T},\n    progData::P where P <: AbstractNLPModel{T, S}) where {T, S}\n\nReference(s)\n\nArmijo, Larry. “Minimization of Functions Having Lipschitz Continuous      First Partial Derivatives.” Pacific Journal of Mathematics 16.1      (1966): 1–3. Pacific Journal of Mathematics. Web.\n\nNocedal and Wright. \"Numerical Optimization\".      Springer New York, NY.\n\nMethod\n\nLet theta_k-1 be the current iterate, and let  alpha in mathbbR_0, delta in (0 1), and rho in (0 1). The k^th iterate is generated as theta_k = theta_k-1 - delta^talpha dot F(theta_k-1)  where t + 1 in mathbbN is the smallest such number satisfying\n\n    F(theta_k) leq F(theta_k-1) - rhodelta^talpha\n    dot F(theta_k-1)_2^2\n\nwhere cdot_2 is the L2-norm. \n\nnote: Note\nTheoretically, there exists such a t, but it can be made arbitrarily large. Therefore, the line search procedure stops searching after optData.line_search_max_iteration. The current implementation terminates the procedure if the backtracking condition is not satisfied, and returns the previous iterate.\n\nArguments\n\noptData::BarzilaiBorweinGD{T}, the specification for the optimization method.\nprogData<:AbstractNLPModel{T,S}, the specification for the optimization   problem. \n\nwarning: Warning\nprogData must have an initialize function that returns subtypes of AbstractPrecompute and AbstractProblemAllocate, where the latter has a grad argument.\n\n\n\n\n\n","category":"function"},{"location":"api_methods/#OptimizationMethods.BacktrackingGD","page":"Methods","title":"OptimizationMethods.BacktrackingGD","text":"BacktrackingGD{T} <: AbstractOptimizerData{T}\n\nMutable sturcture representing gradient descent using backtracking.     It also stores and keeps track of values during the optimization     routine.\n\nFields\n\nname:String, name of the solver for reference.\nα::T, initial step size used for backtracking.\nδ::T, backtracking decreasing factor applied to α when line   search criterion is not satisfied.\nρ::T, factor involved in the acceptance criterion in the line search   procedure. Larger values correspond to stricter descent conditions, and   smaller values correspond to looser descent conditions.\nline_search_max_iteration::Int64, maximum allowable iterations for the   backtracking procedure.\nthreshold::T, gradient threshold. If the norm gradient is below this, then    iteration stops.\nmax_iterations::Int64, max number of iterations (gradient steps) taken by    the solver.\niter_hist::Vector{Vector{T}}, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration 0). The k+1 entry   corresponds to the iterate at iteration k.\ngrad_val_hist::Vector{T}, a vector for storing max_iterations+1 gradient   norm values. The first entry corresponds to iteration 0. The k+1 entry   correpsonds to the gradient norm at iteration k.\nstop_iteration::Int64, the iteration number that the solver stopped on.   The terminal iterate is saved at iter_hist[stop_iteration+1].\n\nConstructors\n\nBacktrackingGD(::Type{T}; x0::Vector{T}, α::T, δ::T, ρ::T,\n    line_search_max_iteration::Int64, threshold::T, max_iteration::Int64)\n    where {T}\n\nReturns a struct of type BacktrackingGD{T} with all the field initialized to     either initialized to the values given, or their default values.\n\nArguments\n\nT::DataType, specific data type used for calculations.\n\nKeyword Arguments\n\nx0::Vector{T}, initial point to start the solver at.\nα::T, initial step size used for backtracking.\nδ::T, backtracking decreasing factor applied to α when line   search criterion not satisfied.\nρ::T, factor involved in the acceptance criterion in the line search   procedure. Larger values correspond to stricter descent conditions, and   smaller values correspond to looser descent conditions.\nline_search_max_iteration::Int64, maximum allowable iterations for the   backtracking procedure.\nthreshold::T, gradient threshold. If the norm gradient is below this,    then iteration is terminated. \nmax_iterations::Int, max number of iterations (gradient steps) taken by    the solver.\n\n\n\n\n\n","category":"type"},{"location":"api_methods/#Gradient-Descent-with-Non-monotone-Line-Search","page":"Methods","title":"Gradient Descent with Non-monotone Line Search","text":"","category":"section"},{"location":"api_methods/#Fixed-Step","page":"Methods","title":"Fixed Step","text":"","category":"section"},{"location":"api_methods/#OptimizationMethods.fixed_step_nls_maxval_gd","page":"Methods","title":"OptimizationMethods.fixed_step_nls_maxval_gd","text":"fixed_step_nls_maxval_gd(optData::FixedStepNonmonLSMaxValG{T},\n    progData::P where P <: AbstractNLPModel{T, S}) where {T, S}\n\nImplementation of gradient descent with non-monotone line search using the maximum value of a fixed number of previous objective values on an optimization problem specified by progData. This implementation  initializes the line search procedure with optData.α every iteration.\n\nReference(s)\n\nGrippo, L., et al. \"A Nonmonotone Line Search Technique for Newton's Method.\"     SIAM Journal on Numerical Analysis, vol. 23, no. 4, 1886, pp. 707-16.     JSTOR, http://www.jstor.org/stable/2157617.\n\nMethod\n\nLet theta_k-1 be the current iterate, and let  alpha in mathbbR_0, delta in (0 1), and rho in (0 1). The k^th iterate is generated as theta_k = theta_k-1 - delta^talpha dot F(theta_k-1)  where t + 1 in mathbbN is the smallest such number satisfying\n\n    F(theta_k) leq max_max(0 k-M) leq j  k F(theta_k-1) - \n    rhodelta^talphadot F(theta_k-1)_2^2\n\nwhere cdot_2 is the L2-norm, and M in mathbbN_0.\n\nArguments\n\noptData::FixedStepNonmonLSMaxValG{T}, the specification for the optimization    method.\nprogData<:AbstractNLPModel{T,S}, the specification for the optimization   problem. \n\nwarning: Warning\nprogData must have an initialize function that returns subtypes of AbstractPrecompute and AbstractProblemAllocate, where the latter has a grad argument.\n\n\n\n\n\n","category":"function"},{"location":"api_methods/#OptimizationMethods.FixedStepNonmonLSMaxValG","page":"Methods","title":"OptimizationMethods.FixedStepNonmonLSMaxValG","text":"FixedStepNonmonLSMaxValGD{T} <: AbstractOptimizerData{T}\n\nMutable struct that represents gradient descent using non-monotone line search where the initial step size for line search is fixed. This struct also keeps track of values during the optimization procedure implemented in  fixed_step_nls_maxval_gd.\n\nFields\n\nname::String, name of the solver for reference.\nα::T, the initial step size for line search.     \nδ::T, backtracking decreasing factor applied to α when the line search   criterion is not satisfied\nρ::T, factor involved in the acceptance criterion in the line search   procedure. Larger values correspond to stricter descent conditions, and   smaller values correspond to looser descent conditions.\nwindow_size::Int64, number of previous objective values that are used   to construct the reference objective value for the line search criterion.\nline_search_max_iteration::Int64, maximum number of iterations for   line search.\nobjective_hist::CircularVector{T, Vector{T}}, buffer array of size    window_size that stores window_size previous objective values.\nmax_value::T, maximum value of objective_hist. This is the reference    objective value used in the line search procedure.\nmax_index::Int64, index of the maximum value that corresponds to the    reference objective value.\nthreshold::T, gradient threshold. If the norm gradient is below this, then    iteration stops.\nmax_iterations::Int64, max number of iterations (gradient steps) taken by    the solver.\niter_hist::Vector{Vector{T}}, a history of the iterates. The first entry   corresponds to the initial iterate (i.e., at iteration 0). The k+1 entry   corresponds to the iterate at iteration k.\ngrad_val_hist::Vector{T}, a vector for storing max_iterations+1 gradient   norm values. The first entry corresponds to iteration 0. The k+1 entry   correpsonds to the gradient norm at iteration k.\nstop_iteration::Int64, the iteration number that the solver stopped on.   The terminal iterate is saved at iter_hist[stop_iteration+1].\n\nConstructors\n\nFixedStepNonmonLSMaxValG(::Type{T}; x0::Vector{T}, α::T, δ::T, ρ::T,\n    window_size::Int64, line_search_max_iteration::Int64,\n    threshold::T, max_iterations::Int64) where {T}\n\nArguments\n\nT::DataType, specific data type used for calculations.\n\nKeyword Arguments\n\nα::T, the initial step size for line search.     \nδ::T, backtracking decreasing factor applied to α when the line search   criterion is not satisfied\nρ::T, factor involved in the acceptance criterion in the line search   procedure. Larger values correpsond to stricter descetn conditions, and   smaller values correspond to looser descent conditions.\nwindow_size::Int64, number of previous objective values that are used   to construct the reference value for the line search criterion.\nline_search_max_iteration::Int64, maximum number of iterations for   line search.\nthreshold::T, gradient threshold. If the norm gradient is below this, then    iteration stops.\nmax_iterations::Int64, max number of iterations (gradient steps) taken by    the solver.\n\n\n\n\n\n","category":"type"},{"location":"api_methods/#Gradient-Descent-with-Non-sequential-Armijo-Line-Search","page":"Methods","title":"Gradient Descent with Non-sequential Armijo Line Search","text":"","category":"section"},{"location":"api_methods/#OptimizationMethods.NonsequentialArmijoGD","page":"Methods","title":"OptimizationMethods.NonsequentialArmijoGD","text":"NonsequentialArmijoGD{T} <: AbstractOptimizerData{T}\n\nA mutable struct that represents gradient descent with non-sequential armijo     line search and triggering events. It stores the specification for the     method and records values during iteration.\n\nFields\n\nname::String, name of the optimizer for reference.\n∇F_θk::Vector{T}, buffer array for the gradient of the initial inner   loop iterate.\nnorm_∇F_ψ::T, norm of the gradient of the current inner loop iterate.\nprev_∇F_ψ::Vector{T}, buffer array for the previous gradient in the inner    loop. Necessary for updating the local Lipschitz approximation.\nprev_norm_step::T, norm of the step between inner loop iterates.    Used for updating the local Lipschitz approximation.\nα0k::T, first step size used in the inner loop. \nδk::T, scaling factor used to condition the step size.\nδ_upper::T, upper limit imposed on the scaling factor when updating.\nρ::T, parameter used in the non-sequential Armijo condition. Larger   numbers indicate stricter descent conditions. Smaller numbers indicate   less strict descent conditions.\nτ_lower::T, lower bound on the gradient interval triggering event.\nτ_upper::T, upper bound on the gradient interval triggering event.\nlocal_lipschitz_estimate::T, local Lipshitz approximation.\nthreshold::T, norm gradient tolerance condition. Induces stopping when norm    is at most threshold.\nmax_iterations::Int64, max number of iterates that are produced, not    including the initial iterate.\niter_hist::Vector{Vector{T}}, store the iterate sequence as the algorithm    progresses. The initial iterate is stored in the first position.\ngrad_val_hist::Vector{T}, stores the norm gradient values at each iterate.    The norm of the gradient evaluated at the initial iterate is stored in the    first position.\nstop_iteration::Int64, the iteration number the algorithm stopped on. The    iterate that induced stopping is saved at iter_hist[stop_iteration + 1].\n\nConstructors\n\nNonsequentialArmijoGD(::Type{T}; x0::Vector{T}, δ0::T, δ_upper::T, ρ::T,\n    threshold::T, max_iterations::Int64) where {T}\n\nConstructs an instance of type NonsequentialArmijoGD{T}.\n\nArguments\n\nT::DataType, type for data and computation.\n\nKeyword Arguments\n\nx0::Vector{T}, initial point to start the optimization routine. Saved in   iter_hist[1].\nδ0::T, starting scaling factor.\nδ_upper::T, upper limit imposed on the scaling factor when updating.\nρ::T, parameter used in the non-sequential Armijo condition. Larger   numbers indicate stricter descent conditions. Smaller numbers indicate   less strict descent conditions.\nthreshold::T, norm gradient tolerance condition. Induces stopping when norm    at most threshold.\nmax_iterations::Int64, max number of iterates that are produced, not    including the initial iterate.\n\n\n\n\n\n","category":"type"},{"location":"api_methods/#OptimizationMethods.nonsequential_armijo_gd","page":"Methods","title":"OptimizationMethods.nonsequential_armijo_gd","text":"nonsequential_armijo_gd(optData::NonsequentialArmijoGD{T},\n    progData::P where P <: AbstractNLPModel{T, S}) where {T, S}\n\nImplementation of gradient descent with non-sequential armijo and triggering     events. The optimization algorithm is specified through optData, and     applied to the problem progData.\n\nReference(s)\n\nMethod\n\nIn what follows, we let cdot_2 denote the L2-norm.  Let theta_k for k + 1 inmathbbN be the k^th iterate of the optimization algorithm. Let delta_k  tau_mathrmgradmathrmlower^k tau_mathrmgradmathrmupper^k be the k^th parameters for the optimization method.  The k+1^th iterate and parameters are produced by the following procedure. \n\nLet psi_0^k = theta_k, and recursively define\n\n    psi_j^k = psi_0^k - sum_i = 0^j-1 delta_k alpha_i^k dot F(psi_i^k)\n\nLet j_k in mathbbN be the smallest iteration for which at least one of the conditions are satisfied: \n\npsi_j_k^k - theta_k_2  10, \ndot F(psi_j_k^k)_2 notin (tau_mathrmgradmathrmlower^k  tau_mathrmgradmathrmupper^k), \nj_k == 100.\n\nThe next iterate and algorithmic parameters in optData are updated based on  the result of the non-sequential Armijo condition\n\n    F(psi_j_k^k) leq \n    F(theta_k) - rhodelta_kalpha_0^kdot F(theta_k)_2\n\nWhen this condition is not satisfied, the following quantities are updated.\n\nThe iterate psi_j_k^k is rejected, and theta_k+1 = theta_k\nThe scaling factor delta_k+1 = 5delta_k\n\nWhen this condition is satisfied, the following quantities are updated.\n\nThe iterate psi_j_k^k is accepted, and theta_k+1 = psi_j_k^k.\nThe scaling factor is updated as delta_k+1 = min(15*delta_k bardelta)  when dot F(psi_j_k^k)_2  tau_mathrmgradmathrmlower^k,  otherwise delta_k+1 = delta_k.\nIf dot F(psi_j_k^k)_2 notin (tau_mathrmgradmathrmlower^k  tau_mathrmgradmathrmupper^k), then   tau_mathrmgradmathrmlower^k+1 =   dot F(psi_j_k^k)_2sqrt2 and   tau_mathrmgradmathrmupper^k+1 =   sqrt10dot F(psi_j_k^k)_2. Otherwise, this parameters are held  constant.\n\nArguments\n\noptData::NesterovAcceleratedGD{T}, the specification for the optimization    method.\nprogData<:AbstractNLPModel{T,S}, the specification for the optimization   problem.\n\nwarning: Warning\nprogData must have an initialize function that returns subtypes of AbstractPrecompute and AbstractProblemAllocate, where the latter has a grad argument.\n\nReturn\n\nx::S, final iterate of the optimization algorithm.\n\n\n\n\n\n","category":"function"},{"location":"api_methods/","page":"Methods","title":"Methods","text":"The method above requires several utility functions. These are listed below.","category":"page"},{"location":"api_methods/#OptimizationMethods.update_local_lipschitz_approximation","page":"Methods","title":"OptimizationMethods.update_local_lipschitz_approximation","text":"update_local_lipschitz_approximation(j::Int64, k::Int64, djk::S, curr_grad::S,\n    prev_grad::S, prev_approximation::T, prev_acceptance::Bool) where {T, S}\n\nGiven the previous approximation of the local Lipschitz constant,     prev_approximation::T, update the current estimate.      That is, return the Lipschitz approximation for inner loop iteration j      and outer loop iteration k.\n\nMethod\n\nThe local Lipschitz approximation method is conducted as follows. Let j     be the current inner loop iteration counter, and let k be the current     outer loop iteration counter. Let psi_j^k be the j^th iterate     of the k^th outer loop, and let hat L_j^k be the j^th estimate     of the k^th outer loop of the local Lipschitz constant. The local     estimate is updated according the following five cases.\n\nWhen j == 1 and k == 1, this is the first iteration of the first   inner loop, and as there is no information available we set it to 1.0.\nWhen j == 1 and k  1, this is the first iteration of the k^th  inner loop, and we return L_j_k-1^k-1 which is the local Lipschitz  estimates formed using information at the terminal iteration of the k-1^th  inner loop (i.e., this is the latest estimate).\nWhen j  1 and k == 1, this is an inner loop iteration where we have  possible taken multiple steps, so we return the most 'local' estimate  of the local Lipschitz constant which is  frac      dot F(psi_j^k) - dot F(psi_j-1^k)_2       psi_j^k - psi_j-1^k_2\nWhen j  1 and k  1 and psi_j_k-1^k-1 satisfied the  descent condition, then we return   frac  dot F(psi_j^k) - dot F(psi_j-1^k)_2      psi_j^k - psi_j-1^k_2\nWhen j  1 and k  1 and psi_j_k-1^k-1 did not satisfy the  descent condition, then we return   max  left( fracdot F(psi_j^k) - dot F(psi_j-1^k)_2      psi_j^k - psi_j-1^k_2 hat L_j-1^k right)\n\nArguments\n\nj::Int64, inner loop iteration.\nk::Int6, outer loop iteration.\nnorm_djk::T, norm of difference between \\psi_j^k and psi_j-1^k.   On the first iteration of the inner loop this will not matter.\ncurr_grad::S, gradient at psi_j^k (i.e., the current iterate).\nprev_grad::S, gradient at psi_j-1^k (i.e., the previous iterate).\nprev_approximation::T, the local Lipschitz approximation from the previous    iteration\nprev_acceptance::Bool, whether or not the previous inner loop resulted in   an accepted iterate.\n\nReturn\n\nestimate::T, estimate of the local Lipschitz constant.\n\n\n\n\n\n","category":"function"},{"location":"api_methods/#OptimizationMethods.compute_step_size","page":"Methods","title":"OptimizationMethods.compute_step_size","text":"compute_step_size(τ_lower::T, norm_grad::T, local_lipschitz_estimate::T\n    ) where {T}\n\nComputes the step size for the inner loop iterates.\n\nMethod\n\nThe inner loop iterates are generated according to the formula\n\n    psi_j+1^k = psi_j^k - delta_kalpha_j^k dot F(psi_j^k)\n\nThe step size, alpha_j^k is computed as\n\n    alpha_j^k = \n    min left( \n    (tau_mathrmgrad mathrmlower^k)^2  C_j1^k \n    1  C_j2^k \n    right)\n\nwhere \n\n    C_j1^k = dot F(psi_j^k)_2^3 + 5 hatL_j^k dot F(psi_j^k)_2^2 + 1e-16\n\nand\n\n    C_j2^k = dot F(psi_j^k)_2 + 5 hatL_j^k + 1e-16\n\nArguments\n\nτ_lower::T, lower bound on the gradient. \nnorm_grad::T, norm of the gradient at psi_j^k.\nlocal_lipschitz_estimate::T, local lipschitz approximation at inner loop   iteration j and outer loop iteration k.\n\nReturn\n\nαjk::T, the step size.\n\n\n\n\n\n","category":"function"},{"location":"api_methods/#OptimizationMethods.inner_loop!","page":"Methods","title":"OptimizationMethods.inner_loop!","text":"inner_loop!(ψjk::S, θk::S, optData::NonsequentialArmijoGD{T}, \n    progData::P1 where P1 <: AbstractNLPModel{T, S}, \n    precomp::P2 where P2 <: AbstractPrecompute{T}, \n    store::P3 where P3 <: AbstractProblemAllocate{T}, \n    past_acceptance::Bool, k::Int64; max_iteration = 100) where {T, S}\n\nConduct the inner loop iteration, modifying ψjk, optData, and store in place. ψjk gets updated to be the terminal iterate of the inner loop; the fields local_lipschitz_estimate, norm_∇F_ψ, and prev_∇F_ψ are updated in optData; the fields grad in store gets updated to be the gradient at ψjk.\n\nArguments\n\nψjk::S, buffer array for the inner loop iterates.\nθk::S, starting iterate.\noptData::NonsequentialArmijoGD{T}, struct that specifies the optimization   algorithm. Fields are modified during the inner loop.\nprogData::P1 where P1 <: AbstractNLPModel{T, S}, struct that specifies the   optimization problem. Fields are modified during the inner loop.\nprecomp::P2 where P2 <: AbstractPrecompute{T}, struct that has precomputed   values. Required to take advantage of this during the gradient computation.\nstore::P3 where P3 <: AbstractProblemAllocate{T}, struct that contains   buffer arrays for computation.\npast_acceptance::Bool, flag indicating if the previous inner loop resulted   in a success (i.e., F(theta_k)  F(theta_k-1)).\nk::Int64, outer loop iteration for computation of the local Lipschitz   approximation scheme.\n\nOptional Keyword Arguments\n\nmax_iteration = 100, maximum number of allowable iteration of the inner loop.   Should be kept at 100 as that is what is specified in the paper, but   is useful to change for testing.\n\nReturns\n\nj::Int64, the iteration for which a triggering event evaluated to true.\n\n\n\n\n\n","category":"function"},{"location":"api_methods/#OptimizationMethods.update_algorithm_parameters!","page":"Methods","title":"OptimizationMethods.update_algorithm_parameters!","text":"update_algorithm_parameters!(θkp1::S, optData::NonsequentialArmijoGD{T},\n    achieved_descent::Bool, iter::Int64) where {T, S}\n\nGiven that the non-sequential Armijo condition is checked, update the parameters     the optimization method. The method updates the following variables in place.\n\nθkp1 is updated to be the next outer loop iterate.\noptData has (potentially) the following fields updated: δk, τ_lower,   τ_upper.\n\nArguments\n\nθkp1::S, buffer array for the storage of the next iterate.\noptData::NonsequentialArmijoGD{T}, struct that specifies the optimization   algorithm. \nachieved_descent::Bool, boolean flag indicating whether or not the   descent condition was achieved.\niter::Int64, the current iteration of the method. The outer loop iteration.   This is requried as it is used to overwrite θkp1 with the previous iterate.\n\nReturns\n\nA boolean flag equal to achieved_descent to indicate whether θkp1 is    modified in-place.\n\n\n\n\n\n","category":"function"},{"location":"api_methods/#Line-search-Helper-Functions","page":"Methods","title":"Line search Helper Functions","text":"","category":"section"},{"location":"api_methods/#Backtracking","page":"Methods","title":"Backtracking","text":"","category":"section"},{"location":"api_methods/#OptimizationMethods.backtracking!","page":"Methods","title":"OptimizationMethods.backtracking!","text":"backtracking!(θk::S, θkm1::S, F::Function, gkm1::S, step_direction::S,\n    reference_value::T, α::T, δ::T, ρ::T; max_iteration::Int64 = 100\n    ) where {T, S}\n\nImplementation of backtracking which modifies θk in place. This method     should be used for general step directions. If gkm1 is the step direction     use the other backtracking!(...) method.\n\nReference(s)\n\nNocedal and Wright. \"Numerical Optimization\".      Springer New York, NY.\n\nMethod\n\nLet theta_k-1 be the current iterate, and let  alpha in mathbbR_0, delta in (0 1), and rho in (0 1). Let d_k be the step direction, then theta_k = theta_k-1 - delta^talpha d_k where  t + 1 in mathbbN is the smallest such number satisfying\n\n    F(theta_k) leq O_k-1 - rhodelta^talpha\n    dot F(theta_k-1)^intercal d_k\n\nwhere O_k-1 is some reference value. \n\nArguments\n\nθk::S, buffer array for the next iterate.\nθkm1::S, current iterate the optimization algorithm.\nF::Function, objective function. Should take in   a single argument and return the value of the    objective at the input value.\ngkm1::S, gradient value at θkm1.\nstep_direction::S, direction to move θkm1 to form θk.\nreference_value::T, value to check the objective function   at θk against.\nα::T, initial step size.\nδ::T, backtracking decrease factor.\nρ::T, factor involved in the acceptance criterion in θk.   Larger values correspond to stricter descent conditions, and   smaller values correspond to looser descent conditions on θk.\n\nOptional Keyword Arguments\n\nmax_iteration::Int64 = 100, the maximum allowable iterations   for the line search procedure. In the language above, when   t is equal to max_iteration the algorithm will terminate.\n\nReturn\n\nbacktracking_condition_satisfied::Bool, whether the backtracking condition   is satisfied before the max iteration limit.\n\n\n\n\n\nbacktracking!(θk::S, θkm1::S, F::Function, gkm1::S, norm_gkm1_squared::T,\n    reference_value::T, α::T, δ::T, ρ::T; max_iteration::Int64 = 100)\n    where {T, S}\n\nImplementation of backtracking which modifies θk in place. This method      assumes that the step direction is -gkm1.\n\nReference(s)\n\nArmijo, Larry. “Minimization of Functions Having Lipschitz Continuous      First Partial Derivatives.” Pacific Journal of Mathematics 16.1      (1966): 1–3. Pacific Journal of Mathematics. Web.\n\nNocedal and Wright. \"Numerical Optimization\".      Springer New York, NY.\n\nMethod\n\nLet theta_k-1 be the current iterate, and let  alpha in mathbbR_0, delta in (0 1), and rho in (0 1). Let dot F(theta_k-1)  be the step direction, then  theta_k = theta_k-1 - delta^talpha dot F(theta_k-1) where  t + 1 in mathbbN is the smallest such number satisfying\n\n    F(theta_k) leq O_k-1 - rhodelta^talpha\n    dot F(theta_k-1)_2^2\n\nwhere O_k-1 is some reference value, and cdot_2 is the L2-norm. \n\nArguments\n\nθk::S, buffer array for the next iterate.\nθkm1::S, current iterate the optimization algorithm.\nF::Function, objective function. Should take in   a single argument and return the value of the    objective at the input value.\ngkm1::S, gradient value at θkm1.\nnorm_gkm1_squared::T, norm squared for the value of gkm1.\nreference_value::T, value to check the objective function   at θk against.\nα::T, initial step size.\nδ::T, backtracking decrease factor.\nρ::T, factor involved in the acceptance criterion in θk.   Larger values correspond to stricter descent conditions, and   smaller values correspond to looser descent conditions on θk.\n\nOptional Keyword Arguments\n\nmax_iteration::Int64 = 100, the maximum allowable iterations   for the line search procedure. In the language above, when   t is equal to max_iteration the algorithm will terminate.\n\nReturn\n\nbacktracking_condition_satisfied::Bool, whether the backtracking condition   is satisfied before the max iteration limit.\n\n\n\n\n\n","category":"function"},{"location":"api_methods/#Non-sequential-Armijo","page":"Methods","title":"Non-sequential Armijo","text":"","category":"section"},{"location":"api_methods/#OptimizationMethods.non_sequential_armijo_condition","page":"Methods","title":"OptimizationMethods.non_sequential_armijo_condition","text":"non_sequential_armijo_condition(F_ψjk::T, reference_value::T,\n    norm_grad_θk::T, ρ::T, δk::T, α0k::T) where {T}\n\nCheck if F_ψjk satisfies the non-sequential armijo condition with respect to reference_value and the remaining parameters. Returns a boolean value indicating if the descent condition is satisfied or not.\n\nMethod\n\nLet F(theta)  mathbbR^n to mathbbR be the objective function. Suppose that theta_k in mathbbR^n is an iterate of an optimization routine. Let psi_0^k = theta_k and for j in lbrace 1T rbrace for some T in mathbbN, recursively define\n\n    psi_j^k = psi_0^k - sum_t = 0^j-1 delta_k alpha_t^k dot F(psi_t^k)\n\nThen, the (monotone) non-sequential armijo condition requires that\n\n    F(psi_j^k)  F(psi_0^k) - rho delta_k alpha_0^k dot F(psi_0^k)_2^2\n\nwhere cdot_2 is the L2-norm and rho in (0 1).\n\nThis function implements checking the inequality, where F_ψjk corresponds to     F(psi_j^k), reference_value corresponds to F(psi_0^k),     norm_grad_θk to dot F(psi_0^k)_2, ρ to rho,      δk to delta_k, and α0k to alpha_0^k. To see more      about how this method is used read the documentation for      gradient descent with non-sequential armijo\n\nArguments\n\nF_ψjk::T, numeric value on the LHS of the inequality. In optimization context,   this is the objective value of a trial iterate to check if sufficient descent   is achieved.\nreference_value::T, numeric value on the RHS of the inequality. In the   optimization context, the value of the current iterate F_ψjk must be   smaller than this to guarantee a sufficient descent criterion.\nnorm_grad_θk::T, numeric value forming the amount of descent that needs   to be achieved. This value is usually the norm of the gradient of a    previous iterate.\nρ::T, parameter in the line search criterion dictating how much descent   should be required. Should be positive. Larger values indicate stricter   conditions, and lower value indicate looser conditions.\nδk::T, numeric value that corresponds to a scaling factor for the step size.\nα0k::T, numeric value. In the context of    non-sequential armijo gradient descent   this is the first step size used in an inner loop.\n\nReturn\n\nflag::Bool, true if the descent condition is satisfied, and false   if the descent condition is not satisfied.\n\n\n\n\n\n","category":"function"},{"location":"api_methods/#Index","page":"Methods","title":"Index","text":"","category":"section"},{"location":"api_methods/","page":"Methods","title":"Methods","text":"","category":"page"},{"location":"problems/quasilikelihood_estimation/#Quasi-likelihood-Estimation","page":"Quasi-likelihood Estimation","title":"Quasi-likelihood Estimation","text":"","category":"section"},{"location":"problems/quasilikelihood_estimation/","page":"Quasi-likelihood Estimation","title":"Quasi-likelihood Estimation","text":"Quasi-likelihood estimation was first introduced by  Wedderburn (1974) as a way of estimating regression coefficients when the underlying probability model generating the data is hard to identify,  or when the data is not explained well by common approaches, such as generalized linear models.  We now briefly provide background information on the method, followed by some examples that are implemented in our package.","category":"page"},{"location":"problems/quasilikelihood_estimation/#Quasi-likelihood-Setting-and-Estimation-Methodology","page":"Quasi-likelihood Estimation","title":"Quasi-likelihood Setting and Estimation Methodology","text":"","category":"section"},{"location":"problems/quasilikelihood_estimation/","page":"Quasi-likelihood Estimation","title":"Quasi-likelihood Estimation","text":"Quasi-likelihood estimation, following Wedderburn's description, assumes that the response variables, y_i in mathbbR, for i = 1n, being either discrete or continuous, are independently collected from a distribution that is only partially known. Specifically, for covariate vectors, x_i in mathbbR^p, i = 1n, and a vector theta^star in mathbbR^p the model assumes the following relationship between y_i x_i and theta^star.","category":"page"},{"location":"problems/quasilikelihood_estimation/","page":"Quasi-likelihood Estimation","title":"Quasi-likelihood Estimation","text":"(Mean Relationship) The expected value of each observation, mathbbEy_i  x_i theta^star = mu_i, satisfies mu_i = mu(x_i^intercal theta^star) for a known function mu  mathbbR to mathbbR. Typically, g is selected to be invertible.\n(Variance Relationship) The variance of each data point satisfies mathbbVy_i  x_i theta^star = V(mu(x_i^intercal theta^star)) for a known non-negative function V  mathbbR to mathbbR.","category":"page"},{"location":"problems/quasilikelihood_estimation/","page":"Quasi-likelihood Estimation","title":"Quasi-likelihood Estimation","text":"Estimation of theta^star proceeds by combining these two components to form the quasi-likelihood objective,","category":"page"},{"location":"problems/quasilikelihood_estimation/","page":"Quasi-likelihood Estimation","title":"Quasi-likelihood Estimation","text":"min_theta F(theta) = min_theta -sum_i=1^n int_c_i^mu(x_i^intercal theta) fracy_i - mV(m)dm","category":"page"},{"location":"problems/quasilikelihood_estimation/","page":"Quasi-likelihood Estimation","title":"Quasi-likelihood Estimation","text":"note: Note\nSince the mean and variance function can be arbitrarily selected, F(theta) might not correspond to any known likelihood function, hence the name quasi-likelihood. Furthermore, the objective function F(theta) might not have a closed form expression, requiring numerical integration techniques to evaluate.  Owing to the structure of the objective however, the fundamental theorem of calculus can be used to compute the gradient. This potentially makes the objective more expensive to compute than the gradient, an atypical scenario for optimization.","category":"page"},{"location":"problems/quasilikelihood_estimation/","page":"Quasi-likelihood Estimation","title":"Quasi-likelihood Estimation","text":"Having presented some background on quasi-likelihood estimation, we now present a use case of the framework for a special case in semi-parametric regression.","category":"page"},{"location":"problems/quasilikelihood_estimation/#Example:-Semi-parametric-Regression-with-Heteroscedasticity-Errors","page":"Quasi-likelihood Estimation","title":"Example: Semi-parametric Regression with Heteroscedasticity Errors","text":"","category":"section"},{"location":"problems/quasilikelihood_estimation/","page":"Quasi-likelihood Estimation","title":"Quasi-likelihood Estimation","text":"Suppose that observations, y_i in mathbbR, i = 1  n, are independent  and are associated with covariate vectors x_i in mathbbR^p, i = 1n.  Furthermore, suppose (x_i y_i) satisfy the following relationship","category":"page"},{"location":"problems/quasilikelihood_estimation/","page":"Quasi-likelihood Estimation","title":"Quasi-likelihood Estimation","text":"y_i = mu(x_i^intercal theta^star) + V( mu(x_i^intercal theta^star) )^12 epsilon_i","category":"page"},{"location":"problems/quasilikelihood_estimation/","page":"Quasi-likelihood Estimation","title":"Quasi-likelihood Estimation","text":"for a function mumathbbR to mathbbR, a non-negative function  V  mathbbR to mathbbR, and a vector theta^star in mathbbR^p. Here, epsilon_i are independent realization from a distribution with a mean and variance of 0 and 1, respectively, but whose exact form cannot be fully specified. ","category":"page"},{"location":"problems/quasilikelihood_estimation/","page":"Quasi-likelihood Estimation","title":"Quasi-likelihood Estimation","text":"This model is a special form of semi-parametric regression with heteroscedastic errors, also  satisfying the requirements of the quasi-likelihood estimation framework. Indeed, the mean and variance relationships for quasi-likelihood are satisfied by checking the expected value and variance of y_i using the statistical model above.","category":"page"},{"location":"problems/quasilikelihood_estimation/","page":"Quasi-likelihood Estimation","title":"Quasi-likelihood Estimation","text":"Below, we provide a list of variance functions that lead to the quasi-likelihood objective being hard to analytically integrate (if not impossible), some of which appear in literature.","category":"page"},{"location":"problems/quasilikelihood_estimation/","page":"Quasi-likelihood Estimation","title":"Quasi-likelihood Estimation","text":"Let V  mathbbR to mathbbR be defined as V(mu) = 1 + mu + sin(2pimu). See Section 4 of Lanteri et al. (2023-02-24).\nLet V  mathbbR to mathbbR be defined as V(mu) = (mu^2)^p + c for c in mathbbR_ 0 and p in mathbbR_5. See for example variance stabilization transformations.\nLet V  mathbbR to mathbbR be defined as V(mu) = exp(-((mu - c)^2)^p) for c in mathbbR and p in mathbbR_5. See Section 4 of Lanteri et al. (2023-02-24).\nLet V  mathbbR to mathbbR be defined as V(mu) = log( ((mu - c)^2)^p + 1) + d for c in mathbbR, p in mathbbR_5, and d in mathbbR_0.","category":"page"},{"location":"problems/quasilikelihood_estimation/#References","page":"Quasi-likelihood Estimation","title":"References","text":"","category":"section"},{"location":"problems/quasilikelihood_estimation/","page":"Quasi-likelihood Estimation","title":"Quasi-likelihood Estimation","text":"Lanteri, A.; Leorato, S.; Lopez-Fidalgo, J. and Tommasi, C. (2023-02-24). Designing to Detect Heteroscedasticity in a Regression Model. Journal of the Royal Statistical Society 85, 315–326.\n\n\n\nWedderburn, R. W. (1974). Quasi-Likelihood Functions, Generalized Linear Models, and the Gauss—Newton Method. Biometrika 61, 439–447.\n\n\n\n","category":"page"},{"location":"#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"OptimizationMethods is a  Julia library for implementing and comparing optimization methods with a focus on problems arising in data science. The library is primarily designed to serve those researching optimization  methods for data science applications. Accordingly, the library is not implementing highly efficient versions of these methods, even though we do our best to make preliminary efficiency optimizations to the code.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"There are two primary components to this library.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Problems, which are implementations of optimization problems primarily   arising in data science. At the moment, problems follow the guidelines   provided by    NLPModels. \nMethods, which are implementations of important optimization methods   that appear in the literature. ","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"The library is still in its infancy and will continue to evolve rapidly. To understand how to use the library, we recommend looking in the examples directory to see how different problems are instantiated and how optimization methods can be applied to them. We also recommend looking at the docstring for specific problems and methods for additional details.","category":"page"},{"location":"#Manual","page":"Overview","title":"Manual","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"The manual section includes descriptions of problems and methods that require a bit more explanation than what is appropriate for in a docstring.","category":"page"},{"location":"#API","page":"Overview","title":"API","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"The API section contains explanations for all problems and methods available in  the library. This is a super set of what is contained in the manual. ","category":"page"}]
}
